<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using Ollama for Note Generation Locally | Raghu Vijaykumar</title>
<meta name=keywords content="llm,data-science"><meta name=description content="Desc Text."><meta name=author content="Me"><link rel=canonical href=https://raghu-vijaykumar.github.io/blog/posts/data-science/ollama-note-generation/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.e7c811b1152f0ea0017b0724f2c040700cf8bf84343fd4fd5eca45af82337db9.css integrity="sha256-58gRsRUvDqABewck8sBAcAz4v4Q0P9T9XspFr4Izfbk=" rel="preload stylesheet" as=style><link rel=icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://raghu-vijaykumar.github.io/blog/posts/data-science/ollama-note-generation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Using Ollama for Note Generation Locally"><meta property="og:description" content="Desc Text."><meta property="og:type" content="article"><meta property="og:url" content="https://raghu-vijaykumar.github.io/blog/posts/data-science/ollama-note-generation/"><meta property="og:image" content="https://raghu-vijaykumar.github.io/blog/cover.jpeg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-27T16:55:14+00:00"><meta property="article:modified_time" content="2024-08-27T16:55:14+00:00"><meta property="og:site_name" content="Raghu Vijaykumar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raghu-vijaykumar.github.io/blog/cover.jpeg"><meta name=twitter:title content="Using Ollama for Note Generation Locally"><meta name=twitter:description content="Desc Text."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://raghu-vijaykumar.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Using Ollama for Note Generation Locally","item":"https://raghu-vijaykumar.github.io/blog/posts/data-science/ollama-note-generation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using Ollama for Note Generation Locally","name":"Using Ollama for Note Generation Locally","description":"Desc Text.","keywords":["llm","data-science"],"articleBody":" we will explore how to use the ollama library to run and connect to models locally for generating readable and easy-to-understand notes. We will walk through the process of setting up the environment, running the code, and comparing the performance and quality of different models like llama3:8b, phi3:14b, llava:34b, and llama3:70b.\nI generated notes from a transcript of a YouTube video in markdown format, with no changes in prompt, here I have included pdf versions.\nTranscript: https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.txt llama3:8b generated (ok): https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.llama3_8b.notes.pdf phi3:14b generated (better): https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.phi3_14b.notes.pdf llava:34b generated (bit descriptive, with additional info): https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.llava_34b.notes.pdf Step-by-Step Guide First, we need to install ollama on your system. Follow the instructions here https://ollama.com/download based on your operating system.\nCreate a virtual environment python -m venv venv source venv/bin/activate Install the dependencies pip install ollama Run the Model Next, download and run the phi3:14b model using the ollama library.\nollama run phi3:14b Generate Notes from Transcripts Now, we will use the provided code to generate notes from .raw.txt files. Here are the scripts you need:\napp.py is a Python script designed to execute various pipelines for generating notes from raw transcripts using the Ollama model. It utilizes the ThreadPoolExecutor for concurrent execution of tasks. The main functions include:\nprocess_raw_to_notes: Function to process raw transcript files into notes using the NotesGenerator class. run_raw_to_notes: Function to run the pipeline for converting raw transcript files to notes. main: Main function to parse command-line arguments and initiate the note generation process. from concurrent.futures import ThreadPoolExecutor import glob import os import logging import sys from notes_generator import NotesGenerator # Set up logging logging.basicConfig( level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(threadName)s - %(message)s\", handlers=[logging.StreamHandler()], ) def process_raw_to_notes(file_path, notes_generator): try: notes_generator.process_transcript(file_path) except Exception as e: logging.error(f\"Error processing file {file_path}: {e}\") def run_raw_to_notes(model, max_threads, folder): notes_generator = NotesGenerator(model=model, max_tokens=4096) # Get all transcript files from input directory raw_files = glob.glob(os.path.join(folder, \"**/*.raw.txt\"), recursive=True) # Initialize a list to store raw files without corresponding notes files filtered_raw_files = [] # Iterate over the raw files for raw_file in raw_files: # Get the name of the raw file without the extension raw_file_name = os.path.splitext(os.path.basename(raw_file))[0] # Check if a corresponding notes file exists model_name = model.replace(\":\", \"_\") notes_file = os.path.join( os.path.dirname(raw_file), raw_file_name + f\".{model_name}\" + \".notes.md\" ) if not os.path.exists(notes_file): # If notes file does not exist, add the raw file to the filtered list filtered_raw_files.append(raw_file) with ThreadPoolExecutor(max_workers=max_threads) as executor: futures = [ executor.submit(process_raw_to_notes, file, notes_generator) for file in filtered_raw_files ] for future in futures: try: future.result() except Exception as e: logging.error(f\"Error in transcript to notes thread: {e}\") def main(pipeline, model, max_threads, folder): if pipeline == \"raw_to_notes\": run_raw_to_notes(model, max_threads, folder) else: logging.error(f\"Unknown pipeline: {pipeline}\") sys.exit(1) if __name__ == \"__main__\": if len(sys.argv) != 5: print(\"Usage: python main.py \") print(\"pipeline: 'raw_to_notes'\") sys.exit(1) pipeline = sys.argv[1] max_threads = int(sys.argv[2]) model = sys.argv[3] folder = sys.argv[4] logging.info(\"Starting note generation process.\") main(pipeline, model, max_threads, folder) logging.info(\"Completed note generation process.\") notes_generator.py contains the NotesGenerator class responsible for generating notes from transcript files. Key functionalities of this class include:\ninit: Initializes the NotesGenerator with the specified model and maximum tokens. split_text: Splits the transcript into chunks based on the maximum token limit. query_gpt: Queries the Ollama model to generate notes based on provided prompts. process_transcript: Reads a transcript file, splits it into chunks, and generates notes for each chunk. import logging import os import time import ollama class NotesGenerator: def __init__(self, model, max_tokens=512): self.model = model self.max_tokens = max_tokens self.system = \"\"\"You are NotesGPT, When provided with a topic your task is - Taking detailed, precise, and easy-to-understand notes - Create advanced bullet-point notes summarizing the important parts of the reading or topic. - Include all essential information, use text highlighting with bold fonts for important key words. - Remove any extraneous language. - Strictly base your notes on the provided information. - Tabulate any comparisions in markdown syntax. - Numerical values in the context are important dont leave them out. - Includes code. - Use latex for any mathematical equations. - Avoid repetition. - The length of the summary should be appropriate for the length and complexity of the original text. - Dont include tasks or insructions or homework in the text. - Provide response in markdown for easy documentation. Content: \"\"\" @staticmethod def count_tokens(text): \"\"\"Counts the number of tokens in a text string.\"\"\" return len(text.split()) def split_text(self, text): \"\"\"Splits the text into chunks based on a specified maximum number of tokens.\"\"\" paragraphs = text.split(\"\\n\\n\") chunks = [] logging.info(\"Starting to split the transcript into chunks.\") for paragraph in paragraphs: words = paragraph.split() while words: if len(words) \u003c= self.max_tokens: # Add the entire paragraph as a chunk chunks.append(\" \".join(words).strip()) logging.info( f\"Created a chunk with {self.count_tokens(' '.join(words))} tokens.\" ) words = [] else: # Split the paragraph into a chunk and update remaining words split_point = self.max_tokens sub_chunk = words[:split_point] chunks.append(\" \".join(sub_chunk).strip()) logging.info( f\"Split a paragraph into a chunk with {self.count_tokens(' '.join(sub_chunk))} tokens.\" ) words = words[split_point:] logging.info(f\"Total chunks created: {len(chunks)}\") return chunks def query_gpt(self, messages): \"\"\"Generates notes for a given prompt using LLaMA3.\"\"\" start_time = time.time() response = ollama.chat(model=self.model, messages=messages) end_time = time.time() logging.info( f\"Received response of {len(response['message']['content'])} tokens for input {len(messages[-1]['content'])} tokens from the model in {end_time - start_time:.2f} seconds.\" ) return response[\"message\"] def process_transcript(self, file_path): \"\"\"Reads a transcript file, splits it, and generates notes.\"\"\" logging.info(f\"Reading transcript from {file_path}.\") with open(file_path, \"r\", encoding=\"utf-8\") as file: transcript = file.read() start_time = time.time() chunks = self.split_text(transcript) # Determine output path for notes model_name = self.model.replace(\":\", \"_\") output_path = os.path.splitext(file_path)[0] + f\".{model_name}\" + \".notes.md\" messages = [] with open(output_path, \"w\", encoding=\"utf-8\") as output_file: for i, chunk in enumerate(chunks): logging.info(f\"Processing chunk {i+1}/{len(chunks)}.\") messages.append({\"role\": \"user\", \"content\": f\"{self.system + chunk}\"}) message = self.query_gpt(messages) messages.append(message) output_file.write(message[\"content\"] + \"\\n\\n\") output_file.flush() # Ensure the note is written to disk immediately end_time = time.time() logging.info( f\"Finished processing all chunks in {end_time - start_time:.2f} seconds.\" ) Run the Script Run the script to generate notes:\npython app.py raw_to_notes 4 phi3_14b /path/to/your/folder Performance Comparison We compared the note generation performance between different models:\nllama3:8b: Takes approximately 830 seconds to process, with decent quality. phi3:14b: Takes approximately 4200 seconds (1.5 hours) but produces better quality notes. llava:34b: Takes approximately 20000 seconds (5-6 Hours) its a bit descriptive for my taste, and provides info on my prompts (unnecessary overhead for automated generation, could be handled with better prompts). You can also experiment with larger models like llama3:70b to see if the quality improves further and speeds depends on the GPU’s.\nConclusion In conclusion, the utilization of Ollama for generating notes from raw transcripts offers a powerful solution for automating the note-taking process. By leveraging advanced language models like LLaMA3 and Phi3:14b, users can obtain detailed and accurate notes in a fraction of the time it would take to manually transcribe and summarize content. The ability to customize prompts and iterate on generated output allows for fine-tuning and refinement to achieve desired results.\nFurthermore, the implementation of efficient chunking strategies in the note generation process enhances productivity and scalability. By breaking down large transcripts into manageable chunks, the system can process information more effectively and generate coherent notes without overwhelming computational resources.\nOverall, Ollama’s note generation capabilities, coupled with efficient chunking techniques, empower users to streamline their workflow, save time, and obtain high-quality notes effortlessly. However there are cases of spelling mistakes, passage repetitions, improper formatting even though specifically asked for markdown format and hallucinations, a manual review and revision is always needed.\nLet me know if there are any other changes you’d like to incorporate!\nSource Code\n","wordCount":"1242","inLanguage":"en","image":"https://raghu-vijaykumar.github.io/blog/cover.jpeg","datePublished":"2024-08-27T16:55:14Z","dateModified":"2024-08-27T16:55:14Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://raghu-vijaykumar.github.io/blog/posts/data-science/ollama-note-generation/"},"publisher":{"@type":"Organization","name":"Raghu Vijaykumar","logo":{"@type":"ImageObject","url":"https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://raghu-vijaykumar.github.io/blog/ accesskey=h title="Raghu Vijaykumar (Alt + H)"><img src=https://raghu-vijaykumar.github.io/apple-touch-icon.png alt aria-label=logo height=35>Raghu Vijaykumar</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://raghu-vijaykumar.github.io/blog/about/ title=about><span>about</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/docs/ title=docs><span>docs</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/sponsor/ title=sponsor><span>sponsor</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://raghu-vijaykumar.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://raghu-vijaykumar.github.io/blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Using Ollama for Note Generation Locally</h1><div class=post-description>Desc Text.</div><div class=post-meta><span title='2024-08-27 16:55:14.989597888 +0000 UTC'>August 27, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1242 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#step-by-step-guide>Step-by-Step Guide</a><ul><li><a href=#create-a-virtual-environment>Create a virtual environment</a></li><li><a href=#install-the-dependencies>Install the dependencies</a></li><li><a href=#run-the-model>Run the Model</a></li><li><a href=#generate-notes-from-transcripts>Generate Notes from Transcripts</a></li><li><a href=#run-the-script>Run the Script</a></li></ul></li><li><a href=#performance-comparison>Performance Comparison</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p><img loading=lazy src=./cover.png alt="Note Taking"></p><p>we will explore how to use the ollama library to run and connect to models locally for generating readable and easy-to-understand notes. We will walk through the process of setting up the environment, running the code, and comparing the performance and quality of different models like llama3:8b, phi3:14b, llava:34b, and llama3:70b.</p><p>I generated notes from a transcript of a YouTube video in markdown format, with no changes in prompt, here I have included pdf versions.</p><ul><li>Transcript: <a href=https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.txt>https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.txt</a></li><li>llama3:8b generated (ok): <a href=https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.llama3_8b.notes.pdf>https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.llama3_8b.notes.pdf</a></li><li>phi3:14b generated (better): <a href=https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.phi3_14b.notes.pdf>https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.phi3_14b.notes.pdf</a></li><li>llava:34b generated (bit descriptive, with additional info): <a href=https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.llava_34b.notes.pdf>https://github.com/raghu-vijaykumar/ollama-note-generation/blob/main/transcript/python_tutorial.raw.llava_34b.notes.pdf</a></li></ul><h2 id=step-by-step-guide>Step-by-Step Guide<a hidden class=anchor aria-hidden=true href=#step-by-step-guide>#</a></h2><p>First, we need to install ollama on your system. Follow the instructions here <a href=https://ollama.com/download>https://ollama.com/download</a> based on your operating system.</p><h3 id=create-a-virtual-environment>Create a virtual environment<a hidden class=anchor aria-hidden=true href=#create-a-virtual-environment>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python -m venv venv
</span></span><span class=line><span class=cl><span class=nb>source</span> venv/bin/activate
</span></span></code></pre></div><h3 id=install-the-dependencies>Install the dependencies<a hidden class=anchor aria-hidden=true href=#install-the-dependencies>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>pip install ollama
</span></span></code></pre></div><h3 id=run-the-model>Run the Model<a hidden class=anchor aria-hidden=true href=#run-the-model>#</a></h3><p>Next, download and run the phi3:14b model using the ollama library.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>ollama run phi3:14b
</span></span></code></pre></div><h3 id=generate-notes-from-transcripts>Generate Notes from Transcripts<a hidden class=anchor aria-hidden=true href=#generate-notes-from-transcripts>#</a></h3><p>Now, we will use the provided code to generate notes from .raw.txt files. Here are the scripts you need:</p><p>app.py is a Python script designed to execute various pipelines for generating notes from raw transcripts using the Ollama model. It utilizes the ThreadPoolExecutor for concurrent execution of tasks. The main functions include:</p><ul><li>process_raw_to_notes: Function to process raw transcript files into notes using the NotesGenerator class.</li><li>run_raw_to_notes: Function to run the pipeline for converting raw transcript files to notes.</li><li>main: Main function to parse command-line arguments and initiate the note generation process.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>concurrent.futures</span> <span class=kn>import</span> <span class=n>ThreadPoolExecutor</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>glob</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>notes_generator</span> <span class=kn>import</span> <span class=n>NotesGenerator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set up logging</span>
</span></span><span class=line><span class=cl><span class=n>logging</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>level</span><span class=o>=</span><span class=n>logging</span><span class=o>.</span><span class=n>INFO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nb>format</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>%(asctime)s</span><span class=s2> - </span><span class=si>%(levelname)s</span><span class=s2> - </span><span class=si>%(threadName)s</span><span class=s2> - </span><span class=si>%(message)s</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>handlers</span><span class=o>=</span><span class=p>[</span><span class=n>logging</span><span class=o>.</span><span class=n>StreamHandler</span><span class=p>()],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>process_raw_to_notes</span><span class=p>(</span><span class=n>file_path</span><span class=p>,</span> <span class=n>notes_generator</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>notes_generator</span><span class=o>.</span><span class=n>process_transcript</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error processing file </span><span class=si>{</span><span class=n>file_path</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>run_raw_to_notes</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>max_threads</span><span class=p>,</span> <span class=n>folder</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>notes_generator</span> <span class=o>=</span> <span class=n>NotesGenerator</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>4096</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Get all transcript files from input directory</span>
</span></span><span class=line><span class=cl>    <span class=n>raw_files</span> <span class=o>=</span> <span class=n>glob</span><span class=o>.</span><span class=n>glob</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>folder</span><span class=p>,</span> <span class=s2>&#34;**/*.raw.txt&#34;</span><span class=p>),</span> <span class=n>recursive</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Initialize a list to store raw files without corresponding notes files</span>
</span></span><span class=line><span class=cl>    <span class=n>filtered_raw_files</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Iterate over the raw files</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>raw_file</span> <span class=ow>in</span> <span class=n>raw_files</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Get the name of the raw file without the extension</span>
</span></span><span class=line><span class=cl>        <span class=n>raw_file_name</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>splitext</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>basename</span><span class=p>(</span><span class=n>raw_file</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=c1># Check if a corresponding notes file exists</span>
</span></span><span class=line><span class=cl>        <span class=n>model_name</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&#34;:&#34;</span><span class=p>,</span> <span class=s2>&#34;_&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>notes_file</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>dirname</span><span class=p>(</span><span class=n>raw_file</span><span class=p>),</span> <span class=n>raw_file_name</span> <span class=o>+</span> <span class=sa>f</span><span class=s2>&#34;.</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;.notes.md&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>exists</span><span class=p>(</span><span class=n>notes_file</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># If notes file does not exist, add the raw file to the filtered list</span>
</span></span><span class=line><span class=cl>            <span class=n>filtered_raw_files</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>raw_file</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>ThreadPoolExecutor</span><span class=p>(</span><span class=n>max_workers</span><span class=o>=</span><span class=n>max_threads</span><span class=p>)</span> <span class=k>as</span> <span class=n>executor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>futures</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>executor</span><span class=o>.</span><span class=n>submit</span><span class=p>(</span><span class=n>process_raw_to_notes</span><span class=p>,</span> <span class=n>file</span><span class=p>,</span> <span class=n>notes_generator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>file</span> <span class=ow>in</span> <span class=n>filtered_raw_files</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>future</span> <span class=ow>in</span> <span class=n>futures</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>future</span><span class=o>.</span><span class=n>result</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logging</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error in transcript to notes thread: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>max_threads</span><span class=p>,</span> <span class=n>folder</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>pipeline</span> <span class=o>==</span> <span class=s2>&#34;raw_to_notes&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>run_raw_to_notes</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>max_threads</span><span class=p>,</span> <span class=n>folder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Unknown pipeline: </span><span class=si>{</span><span class=n>pipeline</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>sys</span><span class=o>.</span><span class=n>exit</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>)</span> <span class=o>!=</span> <span class=mi>5</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Usage: python main.py &lt;pipeline&gt; &lt;max_threads&gt; &lt;model&gt; &lt;folder&gt;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;pipeline: &#39;raw_to_notes&#39;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>sys</span><span class=o>.</span><span class=n>exit</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>pipeline</span> <span class=o>=</span> <span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>max_threads</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>folder</span> <span class=o>=</span> <span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>4</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Starting note generation process.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>main</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>max_threads</span><span class=p>,</span> <span class=n>folder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Completed note generation process.&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>notes_generator.py contains the NotesGenerator class responsible for generating notes from transcript files. Key functionalities of this class include:</p><ul><li><strong>init</strong>: Initializes the NotesGenerator with the specified model and maximum tokens.</li><li>split_text: Splits the transcript into chunks based on the maximum token limit.</li><li>query_gpt: Queries the Ollama model to generate notes based on provided prompts.</li><li>process_transcript: Reads a transcript file, splits it into chunks, and generates notes for each chunk.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>ollama</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>NotesGenerator</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_tokens</span> <span class=o>=</span> <span class=n>max_tokens</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>system</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;You are NotesGPT, When provided with a topic your task is
</span></span></span><span class=line><span class=cl><span class=s2>        - Taking detailed, precise, and easy-to-understand notes
</span></span></span><span class=line><span class=cl><span class=s2>        - Create advanced bullet-point notes summarizing the important parts of the reading or topic.
</span></span></span><span class=line><span class=cl><span class=s2>        - Include all essential information, use text highlighting with bold fonts for important key words.
</span></span></span><span class=line><span class=cl><span class=s2>        - Remove any extraneous language.
</span></span></span><span class=line><span class=cl><span class=s2>        - Strictly base your notes on the provided information.
</span></span></span><span class=line><span class=cl><span class=s2>        - Tabulate any comparisions in markdown syntax.
</span></span></span><span class=line><span class=cl><span class=s2>        - Numerical values in the context are important dont leave them out.
</span></span></span><span class=line><span class=cl><span class=s2>        - Includes code.
</span></span></span><span class=line><span class=cl><span class=s2>        - Use latex for any mathematical equations.
</span></span></span><span class=line><span class=cl><span class=s2>        - Avoid repetition.
</span></span></span><span class=line><span class=cl><span class=s2>        - The length of the summary should be appropriate for the length and complexity of the original text.
</span></span></span><span class=line><span class=cl><span class=s2>        - Dont include tasks or insructions or homework in the text.
</span></span></span><span class=line><span class=cl><span class=s2>        - Provide response in markdown for easy documentation.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Content:
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>count_tokens</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Counts the number of tokens in a text string.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>split_text</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Splits the text into chunks based on a specified maximum number of tokens.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>paragraphs</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Starting to split the transcript into chunks.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>paragraph</span> <span class=ow>in</span> <span class=n>paragraphs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>words</span> <span class=o>=</span> <span class=n>paragraph</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>while</span> <span class=n>words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>words</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=c1># Add the entire paragraph as a chunk</span>
</span></span><span class=line><span class=cl>                    <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>words</span><span class=p>)</span><span class=o>.</span><span class=n>strip</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                    <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=sa>f</span><span class=s2>&#34;Created a chunk with </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>count_tokens</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>words</span><span class=p>))</span><span class=si>}</span><span class=s2> tokens.&#34;</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>words</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=c1># Split the paragraph into a chunk and update remaining words</span>
</span></span><span class=line><span class=cl>                    <span class=n>split_point</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_tokens</span>
</span></span><span class=line><span class=cl>                    <span class=n>sub_chunk</span> <span class=o>=</span> <span class=n>words</span><span class=p>[:</span><span class=n>split_point</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                    <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>sub_chunk</span><span class=p>)</span><span class=o>.</span><span class=n>strip</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                    <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=sa>f</span><span class=s2>&#34;Split a paragraph into a chunk with </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>count_tokens</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>sub_chunk</span><span class=p>))</span><span class=si>}</span><span class=s2> tokens.&#34;</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>words</span> <span class=o>=</span> <span class=n>words</span><span class=p>[</span><span class=n>split_point</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total chunks created: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>chunks</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>chunks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>query_gpt</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>messages</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Generates notes for a given prompt using LLaMA3.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>ollama</span><span class=o>.</span><span class=n>chat</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=n>messages</span><span class=o>=</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=sa>f</span><span class=s2>&#34;Received response of </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;message&#39;</span><span class=p>][</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=si>}</span><span class=s2> tokens for input </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>messages</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=si>}</span><span class=s2> tokens from the model in </span><span class=si>{</span><span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> seconds.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>response</span><span class=p>[</span><span class=s2>&#34;message&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>process_transcript</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>file_path</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Reads a transcript file, splits it, and generates notes.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Reading transcript from </span><span class=si>{</span><span class=n>file_path</span><span class=si>}</span><span class=s2>.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>file_path</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>file</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>transcript</span> <span class=o>=</span> <span class=n>file</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>chunks</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_text</span><span class=p>(</span><span class=n>transcript</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Determine output path for notes</span>
</span></span><span class=line><span class=cl>        <span class=n>model_name</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&#34;:&#34;</span><span class=p>,</span> <span class=s2>&#34;_&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>splitext</span><span class=p>(</span><span class=n>file_path</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=sa>f</span><span class=s2>&#34;.</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;.notes.md&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>messages</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>output_path</span><span class=p>,</span> <span class=s2>&#34;w&#34;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>output_file</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>chunks</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Processing chunk </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>chunks</span><span class=p>)</span><span class=si>}</span><span class=s2>.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>messages</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>system</span> <span class=o>+</span> <span class=n>chunk</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl>                <span class=n>message</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>query_gpt</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>messages</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>message</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>output_file</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>message</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>]</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>output_file</span><span class=o>.</span><span class=n>flush</span><span class=p>()</span>  <span class=c1># Ensure the note is written to disk immediately</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=sa>f</span><span class=s2>&#34;Finished processing all chunks in </span><span class=si>{</span><span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> seconds.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><h3 id=run-the-script>Run the Script<a hidden class=anchor aria-hidden=true href=#run-the-script>#</a></h3><p>Run the script to generate notes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python app.py raw_to_notes <span class=m>4</span> phi3_14b /path/to/your/folder
</span></span></code></pre></div><h2 id=performance-comparison>Performance Comparison<a hidden class=anchor aria-hidden=true href=#performance-comparison>#</a></h2><p>We compared the note generation performance between different models:</p><ul><li>llama3:8b: Takes approximately 830 seconds to process, with decent quality.</li><li>phi3:14b: Takes approximately 4200 seconds (1.5 hours) but produces better quality notes.</li><li>llava:34b: Takes approximately 20000 seconds (5-6 Hours) its a bit descriptive for my taste, and provides info on my prompts (unnecessary overhead for automated generation, could be handled with better prompts).</li></ul><p>You can also experiment with larger models like llama3:70b to see if the quality improves further and speeds depends on the GPU&rsquo;s.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In conclusion, the utilization of Ollama for generating notes from raw transcripts offers a powerful solution for automating the note-taking process. By leveraging advanced language models like LLaMA3 and Phi3:14b, users can obtain detailed and accurate notes in a fraction of the time it would take to manually transcribe and summarize content. The ability to customize prompts and iterate on generated output allows for fine-tuning and refinement to achieve desired results.</p><p>Furthermore, the implementation of efficient chunking strategies in the note generation process enhances productivity and scalability. By breaking down large transcripts into manageable chunks, the system can process information more effectively and generate coherent notes without overwhelming computational resources.</p><p>Overall, Ollama&rsquo;s note generation capabilities, coupled with efficient chunking techniques, empower users to streamline their workflow, save time, and obtain high-quality notes effortlessly. However there are cases of spelling mistakes, passage repetitions, improper formatting even though specifically asked for markdown format and hallucinations, a manual review and revision is always needed.</p><p>Let me know if there are any other changes you&rsquo;d like to incorporate!</p><p><a href=https://github.com/raghu-vijaykumar/ollama-note-generation/tree/main>Source Code</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://raghu-vijaykumar.github.io/blog/tags/llm/>Llm</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/data-science/>Data-Science</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://raghu-vijaykumar.github.io/blog/>Raghu Vijaykumar</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>