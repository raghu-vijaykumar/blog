<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building Batch Pipelines on Google Cloud Platform | Raghu Vijaykumar</title>
<meta name=keywords content="data-engineering,batch-pipelines,google-cloud,course-summary"><meta name=description content="Introduction to Building Batch Data Pipelines Batch Pipelines Overview Definition: Pipelines that process a fixed amount of data and terminate.
Example: Daily processing of financial transactions to balance books and store reconciled data. ETL vs ELT:
ELT (Extract, Load, Transform):
Load raw data into a target system and perform transformations on the fly as needed. Suitable when transformations depend on user needs or view requirements (e.g., raw data accessible through views)."><meta name=author content="Me"><link rel=canonical href=https://raghu-vijaykumar.github.io/blog/posts/courses/building-batch-data-pipelines-gcp/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.e7c811b1152f0ea0017b0724f2c040700cf8bf84343fd4fd5eca45af82337db9.css integrity="sha256-58gRsRUvDqABewck8sBAcAz4v4Q0P9T9XspFr4Izfbk=" rel="preload stylesheet" as=style><link rel=icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://raghu-vijaykumar.github.io/blog/posts/courses/building-batch-data-pipelines-gcp/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Building Batch Pipelines on Google Cloud Platform"><meta property="og:description" content="Introduction to Building Batch Data Pipelines Batch Pipelines Overview Definition: Pipelines that process a fixed amount of data and terminate.
Example: Daily processing of financial transactions to balance books and store reconciled data. ETL vs ELT:
ELT (Extract, Load, Transform):
Load raw data into a target system and perform transformations on the fly as needed. Suitable when transformations depend on user needs or view requirements (e.g., raw data accessible through views)."><meta property="og:type" content="article"><meta property="og:url" content="https://raghu-vijaykumar.github.io/blog/posts/courses/building-batch-data-pipelines-gcp/"><meta property="og:image" content="https://raghu-vijaykumar.github.io/blog/cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-23T17:12:35+00:00"><meta property="article:modified_time" content="2024-08-23T17:12:35+00:00"><meta property="og:site_name" content="Raghu Vijaykumar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raghu-vijaykumar.github.io/blog/cover.png"><meta name=twitter:title content="Building Batch Pipelines on Google Cloud Platform"><meta name=twitter:description content="Introduction to Building Batch Data Pipelines Batch Pipelines Overview Definition: Pipelines that process a fixed amount of data and terminate.
Example: Daily processing of financial transactions to balance books and store reconciled data. ETL vs ELT:
ELT (Extract, Load, Transform):
Load raw data into a target system and perform transformations on the fly as needed. Suitable when transformations depend on user needs or view requirements (e.g., raw data accessible through views)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://raghu-vijaykumar.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Building Batch Pipelines on Google Cloud Platform","item":"https://raghu-vijaykumar.github.io/blog/posts/courses/building-batch-data-pipelines-gcp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building Batch Pipelines on Google Cloud Platform","name":"Building Batch Pipelines on Google Cloud Platform","description":"Introduction to Building Batch Data Pipelines Batch Pipelines Overview Definition: Pipelines that process a fixed amount of data and terminate.\nExample: Daily processing of financial transactions to balance books and store reconciled data. ETL vs ELT:\nELT (Extract, Load, Transform):\nLoad raw data into a target system and perform transformations on the fly as needed. Suitable when transformations depend on user needs or view requirements (e.g., raw data accessible through views).","keywords":["data-engineering","batch-pipelines","google-cloud","course-summary"],"articleBody":" Introduction to Building Batch Data Pipelines Batch Pipelines Overview Definition: Pipelines that process a fixed amount of data and terminate.\nExample: Daily processing of financial transactions to balance books and store reconciled data. ETL vs ELT:\nELT (Extract, Load, Transform):\nLoad raw data into a target system and perform transformations on the fly as needed. Suitable when transformations depend on user needs or view requirements (e.g., raw data accessible through views). ETL (Extract, Transform, Load):\nExtract data, transform it in an intermediate system, then load into a target system (e.g., Dataflow to BigQuery). Preferred for complex transformations or data quality improvements before loading. Use Cases:\nEL (Extract and Load):\nUse when data is clean and ready for direct loading (e.g., log files from cloud storage). Suitable for batch loading and scheduled data loads. ELT (Extract, Load, Transform):\nStart with loading as EL, then transform data on-demand or based on evolving requirements. Useful when transformations are uncertain initially, like processing API JSON responses in BigQuery. Quality Considerations in Data Pipelines Data Quality Dimensions:\nValidity: Data conforms to business rules (e.g., transaction amount validation). Accuracy: Data reflects objective truth (e.g., precise financial calculations). Completeness: All required data is processed (e.g., no missing records). Consistency: Data consistency across different sources (e.g., uniform formats). Uniformity: Data values within the same column are consistent. Methods to Address Quality Issues:\nBigQuery Capabilities: Use views to filter out invalid data rows (e.g., negative quantities). Aggregate and filter using HAVING clause for data completeness checks. Handle nulls and blanks using COUNTIF and IF statements for accurate computations. Ensuring Data Integrity:\nVerification and Backfilling: Verify file integrity with checksums (e.g., MD5). Address consistency issues such as duplicates with COUNT and COUNT DISTINCT. Clean data using string functions to remove extra characters or format inconsistencies. Documentation and Best Practices:\nDocument SQL queries, transformations, and data units clearly to maintain uniformity. Use SQL functions like CAST and FORMAT to manage data type changes and indicate units clearly. Shortcomings of ELT and Use Cases for ETL Complex Transformations Beyond SQL:\nExamples include: Translating text (e.g., Spanish to English using external APIs). Analyzing complex customer action streams over time windows. SQL may not handle these transformations efficiently or feasibly. Continuous Data Quality Control and Transformation:\nUse ETL when: Raw data requires extensive quality control or enrichment before loading into BigQuery. Transformations are too complex or cannot be expressed in SQL. Continuous streaming data needs processing and integration (e.g., using Dataflow for real-time updates). Integration with CI/CD Systems:\nETL is suitable for integrating with continuous integration and continuous delivery (CI/CD) systems. Allows for unit testing of pipeline components and scheduled launches of Dataflow pipelines. Google Cloud ETL Tools Dataflow:\nFully managed, serverless data processing service based on Apache Beam. Supports both batch and streaming data processing pipelines. Ideal for transforming and enriching data before loading into BigQuery. Dataproc:\nManaged service for running Apache Hadoop and Apache Spark jobs. Cost-effective for complex batch processing, querying, and machine learning tasks. Offers features like autoscaling and integrates seamlessly with BigQuery. Data Fusion:\nCloud-native enterprise data integration service for building and managing data pipelines. Provides a visual interface for non-programmers to create pipelines. Supports transformation, cleanup, and ensuring data consistency across various sources. Considerations for Choosing ETL Specific Needs that ETL Addresses:\nLow Latency and High Throughput:\nBigQuery offers low latency (100 ms with BI Engine) and high throughput (up to 1 million rows per second). For even stricter latency and throughput requirements, consider Cloud Bigtable. Reusing Existing Spark Pipelines:\nIf your organization already uses Hadoop and Spark extensively, leveraging Dataproc may be more productive. Visual Pipeline Building:\nData Fusion provides a drag-and-drop interface for visual pipeline creation, suitable for non-technical users. Metadata Management and Data Governance Importance of Data Lineage and Metadata: Data Catalog provides: Metadata management for cataloging data assets across multiple systems. Access controls and integration with Cloud Data Loss Prevention API for data classification. Unified data discovery and tagging capabilities for organizing and governing data assets effectively. Executing Spark on Dataproc Dataproc: Google Cloud’s managed Hadoop service, focusing on Apache Spark.\nThe Hadoop Ecosystem Historical Context: Pre-2006: Big data relied on large databases; storage was cheap, processing was expensive. 2006 Onward: Distributed processing of big data became practical with Hadoop. Hadoop Components: HDFS (Hadoop Distributed File System): Stores data on cluster machines. MapReduce: Distributed data processing framework. Related Tools: Hive, Pig, Spark, Presto. Apache Hadoop: Open-source software for distributed processing across clusters. Main File System: HDFS. Apache Spark: High-performance analytics engine. In-memory processing: Up to 100 times faster than Hadoop jobs. Data Abstractions: Resilient Distributed Datasets (RDDs), DataFrames. Cloud vs. On-premises: On-premises Hadoop: Limited by physical resources. Cloud Dataproc: Overcomes tuning and utilization issues of OSS Hadoop. Dataproc Benefits: Managed hardware/configuration: No need for physical hardware management. Version management: Simplifies updating open-source tools. Flexible job configuration: Create multiple clusters for specific tasks. Running Hadoop on Dataproc Advantages of Dataproc: Low Cost: Priced at one cent per virtual CPU per cluster per hour. Speed: Clusters start, scale, and shut down in 90 seconds or less. Resizable Clusters: Quick scaling with various VM types and disk sizes. Open-source Ecosystem: Frequent updates to Spark, Hadoop, Pig, and Hive. Integration: Works seamlessly with Cloud Storage, BigQuery, Cloud Bigtable. Management: Easy interaction through cloud console, SDK, REST API. Versioning: Switch between different versions of Spark, Hadoop, and other tools. High Availability: Multiple primary nodes, job restart on failure. Developer Tools: Cloud console, SDK, RESTful APIs, SSH access. Customization: Pre-configured optional components, initialization actions. Dataproc Cluster Architecture Cluster Components: Primary Nodes: Run HDFS name node, node and job drivers. Worker Nodes: Can be part of a managed instance group. Preemptible Secondary Workers: Lower cost but can be preempted anytime. Storage Options: Persistent Disks: Standard storage method. Cloud Storage: Recommended over native HDFS for durability. Cloud Bigtable/BigQuery: Alternatives for HBase and analytical workloads. Cluster Lifecycle: Setup: Create clusters via cloud console, command line, YAML files, Terraform, or REST API. Configuration: Specify region, zone, primary node, worker nodes. Initialization: Customize with initialization scripts and metadata. Optimization: Use preemptible VMs and custom machine types to balance cost and performance. Utilization: Submit jobs through console, command line, REST API, or orchestration services. Monitoring: Cloud Monitoring for job and cluster metrics, alert policies for incidents. Key Features of Dataproc Low Cost: Pay-per-use with second-by-second billing. Fast Cluster Operations: 90 seconds or less to start, scale, or shut down. Flexible and Resizable: Variety of VM types, disk sizes, and networking options. Integration with Google Cloud: Seamless interaction with other Google Cloud services. Ease of Use: Minimal learning curve, supports existing Hadoop tools. High Availability and Reliability: Multiple primary nodes, restart able jobs. Developer-Friendly: Multiple ways to manage clusters, customizable via scripts and metadata. Using Google Cloud Storage Instead of HDFS Network Evolution and Data Proximity Original Network Speeds: Slow, necessitating data to be close to the processor. Modern Network Speeds: Petabit networking enables independent storage and compute. On-Premise Hadoop Clusters: Require local storage on disks as the same server handles compute and storage. Cloud Migration and HDFS Lift-and-Shift: Moving Hadoop workloads to Dataproc with HDFS requires no code changes but is a short-term solution. Long-Term Issues with HDFS in Cloud: Block Size: Performance is tied to server hardware, not elastic. Data Locality: Storing data on persistent disks limits flexibility. Replication: HDFS requires three copies for high availability, leading to inefficiency. Advantages of Cloud Storage over HDFS Network Efficiency: Google’s Jupyter network fabric offers over one petabit per second bandwidth. Elastic Storage: Cloud Storage scales independently of compute resources. Cost Efficiency: Pay only for what you use, unlike HDFS which requires overprovisioning. Integration: Cloud Storage integrates seamlessly with various Google Cloud services. Considerations for Using Cloud Storage Latency: High throughput but significant latency; suitable for large, bulk operations. Object Storage Nature: Renaming objects is expensive due to immutability. Cannot append to objects. Output Committers: New object store-oriented committers help mitigate directory rename issues. Data Transfer and Management DistCp Tool: Essential for moving data; push-based model preferred for known necessary data. Data Management Continuum: Pre-2006: Big databases for big data. 2006: Distributed processing with Hadoop. 2010: BigQuery launch. 2015: Google’s Dataproc for managed Hadoop and Spark clusters. Ephemeral Clusters Ephemeral Model: Utilize clusters as temporary resources, reducing costs by not paying for idle compute capacity. Workflow Example: Use Cloud Storage for initial and final data, with intermediate processing in local HDFS if needed. Cluster Optimization Data Locality: Ensure Cloud Storage bucket and Dataproc region are physically close. Network Configuration: Avoid bottlenecks by ensuring traffic isn’t funneled through limited gateways. Input File Management: Reduce the number of input files (\u003c10,000) and partitions (\u003c50,000) if possible. Adjust fs.gs.block.size for larger datasets. Persistent Disks and Virtual Machines Disk Size: Choose appropriate persistent disk size to avoid performance limitations. VM Allocation: Prototype and benchmark with real data to determine optimal VM size. Job-Scoped Clusters: Customize clusters for specific tasks to optimize resource use. Local HDFS vs. Cloud Storage Local HDFS: Better for jobs requiring frequent metadata operations, small file sizes, or heavy I/O workloads with low latency needs. Cloud Storage: Ideal for the initial and final data source in pipelines, reducing disk requirements and saving costs. Autoscaling and Templates Workflow Templates: Automate cluster creation, job submission, and cluster deletion. Autoscaling: Scale clusters based on YARN metrics, providing flexible capacity and improving resource utilization. Initial Workers: Set the number of initial workers to ensure basic capacity from the start. Monitoring and Logging Cloud Logging: Consolidates logs for easy error diagnosis and monitoring. Custom Dashboards: Use Cloud Monitoring to visualize CPU, disk, and network usage, and YARN resources. Application Logs: Retrieve Spark job logs from the driver output, Cloud Console, or Cloud Storage bucket. Serverless Data Processing with Dataflow Dataflow Overview:\nServerless: Dataflow is serverless, eliminating the need to manage clusters. Scaling: Scales step by step, fine-grained scaling unlike Data Proc. Unified Processing: Supports both batch and stream processing with the same code. Choosing Between Dataflow and DataProc:\nRecommendation: Use Dataflow for new pipelines due to serverless benefits. Existing Hadoop Pipelines: Consider Data Proc for migration and modernization. Decision Factors: Depends on existing technology stack and operational preferences. Unified Programming with Apache Beam:\nHistorical Context: Batch processing (1940s) vs. stream processing (1970s). Apache Beam: Unifies batch and stream processing concepts. Core Concepts: P transforms, P collections, pipelines, and pipeline runners. Immutable Data and Distributed Processing:\nP collections: Immutable distributed data abstractions in Dataflow. Serialization: All data types are serialized as byte strings for efficient network transfer. Processing Model: Each transform creates a new P collection, enabling distributed processing. Why Customers Value Dataflow Efficient Execution and Optimization:\nPipeline Execution: Dataflow manages job execution from data source to sink. Optimization: Optimizes job graph, fuses transforms, and dynamically rebalances work units. Resource Management: On-demand deployment of compute and storage resources per job. Reliability and Fault Tolerance:\nContinuous Optimization: Ongoing optimization and rebalancing of resources. Fault Tolerance: Handles late data arrivals, with automatic restarts and monitoring. Auto-Scaling: Scales resources dynamically during job execution, improving job efficiency. Operational Efficiency and Integration:\nManaged Service: Fully managed and auto-configured service, reducing operational overhead. Integration: Acts as a central component connecting various Google Cloud services. Use Cases: Supports diverse use cases from BigQuery to Cloud SQL seamlessly. Building Dataflow Pipelines in Code Constructing a Dataflow Pipeline Pipeline Construction:\nPython Syntax: Use pipe symbols (|) to chain P transforms from an input P collection. Java Syntax: Use .apply() method instead of pipe symbols. Branching in Pipelines:\nMultiple Transform Paths: Apply the same input P collection to different transforms. Named Outputs: Store results in separate P collection variables (Pcollection_out1, Pcollection_out2). Pipeline Initialization and Termination:\nPipeline Start: Begin with a source to fetch the initial P collection (e.g., readFromText). Pipeline End: Terminate with a sink operation (e.g., writing to a text file in cloud storage). Context Management: Use Python with clause to manage pipeline execution context. Key Considerations in Pipeline Design Data Sources and Sinks:\nReading Data: Utilize beam.io to read from various sources like text files, Pub/Sub, or BigQuery. Writing Data: Specify sinks such as BigQuery tables using beam.io.WriteToBigQuery. Transforming Data with PTransforms:\nMapping Data: Use Map to apply a function to each element in the P collection. Flat Mapping: Apply FlatMap to handle one-to-many relationships, flattening results. ParDo Transformation: Custom processing on each element, with serialization and thread safety ensured. DoFn Class: Define distributed processing functions, ensuring they are serializable and idempotent. Handling Multiple Outputs:\nBranching and Outputs: Output different types or formats from a single P collection using ParDo. Aggregate with GroupByKey and Combine Dataflow Model: Utilizes a shuffle phase after the map phase to group together like keys.\nOperates on a PCollection of key-value pairs (tuples). Groups by common key, returning a key-value pair where the value is a group of values. Example: Finding zip codes associated with a city.\nKey-Value Pair Creation: Create pairs and group by key. Data Skew Issue: Small Scale: Manageable with 1 million items. Large Scale: 1 billion items can cause performance issues. High Cardinality: Performance concerns in queries with billions of records. Performance Concern:\nGrouping by key causes unbalanced workload among workers. Example: 1 million X values on one worker. 1,000 Y values on another worker. Inefficiency: Idle worker waiting for the other to complete. Dataflow Optimization:\nDesign applications to divide work into aggregation steps. Push grouping towards the end of the processing pipeline. CoGroupByKey: Similar to GroupByKey but groups results across multiple PCollections by key.\nCombine: Used to aggregate values in PCollections.\nCombine Variants: Combine.globally(): Reduces a PCollection to a single value. Combine.perKey(): Similar to GroupByKey but combines values using a specified function. Prebuilt Combine Functions: For common operations like sum, min, and max. Custom Combine Function: Create a subclass of CombineFn for complex operations. Four Operations: Create Accumulator: New local accumulator. Add Input: Add input to the accumulator. Merge Accumulators: Merge multiple accumulators. Extract Output: Produce final result from the accumulator. Efficiency: Combine is faster than GroupByKey due to parallelization. Custom Combine Class: For operations with commutative and associative properties. Flatten: Merges multiple PCollection objects into a single PCollection, similar to SQL UNION.\nPartition: Splits a single PCollection into smaller collections.\nUse Case: Different processing for different quartiles. Side Inputs and Windows of Data Side Inputs: Additional inputs to a ParDo transform. Use Case: Inject additional data at runtime. Example: Compute average word length. Use as side input to determine if a word is longer or shorter than average. Dataflow Windows: Global Window: Default, not useful for unbounded PCollections (streaming data). Time-based Windows: Useful for processing streaming data. Example: Sliding Windows - beam.WindowInto(beam.window.SlidingWindows(60, 30)). Capture 60 seconds of data, start new window every 30 seconds. Fixed Windows: Example - group sales by day. Creating and Re-using Pipeline Templates Dataflow Templates: Enable non-developers to execute dataflow jobs. Workflow: Developer creates pipeline with Dataflow SDK (Java/Python). Separate development from execution activities. Benefits: Simplifies scheduling batch jobs. Allows deployment from environments like App Engine, Cloud Functions. Custom Templates: Value Providers: Parse command-line or optional arguments. Runtime Parameters: Convert compile-time parameters for user customization. Metadata File: Describes template parameters and functions for downstream users. Manage Data Pipelines with Cloud Data Fusion and Cloud Composer Cloud Data Fusion provides a graphical user interface and APIs to build, deploy, and manage data integration pipelines efficiently. Users: Developers, Data Scientists, Business Analysts Developers: Cleanse, match, remove duplicates, blend, transform, partition, transfer, standardize, automate, and monitor data. Data Scientists: Visually build integration pipelines, test, debug, and deploy applications. Business Analysts: Run at scale, operationalize pipelines, inspect rich integration metadata. Benefits: Integration: Connects with a variety of systems (legacy, modern, relational databases, file systems, cloud services, NoSQL, etc.). Productivity: Centralizes data from various sources (e.g., BigQuery, Cloud Spanner). Reduced Complexity: Visual interface for building pipelines, code-free transformations, reusable templates. Flexibility: Supports on-prem and cloud environments, interoperable with open-source software (CDAP). Extensibility: Template pipelines, create conditional triggers, manage and create plugins, custom compute profiles. Components of Cloud Data Fusion User Interface Components: Wrangler UI: Explore datasets visually, build pipelines with no code. Data Pipeline UI: Draw pipelines on a canvas. Control Center: Manage applications, artifacts, and datasets. Pipeline Section: Developer studio, preview, export, schedule jobs, connector, function palette, navigation. Integration Metadata: Search, add tags and properties, see data lineage. Hub: Available plugins, sample use cases, prebuilt pipelines. Administration: Management (services, metrics) and configuration (namespace, compute profiles, preferences, system artifacts, REST client). Building a Pipeline Pipeline Representation: Visual series of stages in a Directed Acyclic Graph (DAG). Nodes: Different types (e.g., data from cloud storage, parse CSV, join data, sync data). Canvas: Area for creating and chaining nodes. Mini Map: Navigate large pipelines. Pipeline Actions Tool Bar: Save, run, and manage pipelines. Templates and Plugins: Use pre-existing resources to avoid starting from scratch. Preview Mode: Test and debug pipelines before deployment. Monitoring: Track health, data throughput, processing time, and anomalies. Tags Feature: Organize pipelines for quick access. Concurrency: Set maximum number of concurrent runs to optimize processing. Exploring Data Using Wrangler Wrangler UI: For exploring and analyzing new datasets visually. Connections: Add and manage connections to data sources (e.g., Google Cloud Storage, BigQuery). Data Exploration: Inspect rows and columns, view sample insights. Data Transformation: Create calculated fields, drop columns, filter rows using directives to form a transformation recipe. Pipeline Creation: Convert transformations into pipelines for regular execution. Example Pipeline Twitter Data Ingestion: Ingest data from Twitter and Google Cloud. Parse tweets. Load into various data sinks. Health Monitoring: View start time, duration, and summary of pipeline runs. Data throughput at each node. Inputs, outputs, and errors per node. Streaming Data Pipelines: Future modules will cover streaming data pipelines. Key Metrics and Features Metrics: Records out per second, average processing time, max processing time. Automation: Set pipelines to run automatically at intervals. Field Lineage Tracking: Track transformations applied to data fields. Example: Campaign Field Lineage Track every transformation before and after the field. View the lineage of operations between datasets. Identify time of last change and involved input fields. Summary Cloud Data Fusion: Efficient, flexible, and extensible tool for building and managing data pipelines. User Interfaces: Wrangler UI for visual exploration, Data Pipeline UI for pipeline creation. Building Pipelines: Use DAGs, nodes, canvas, templates, and plugins for streamlined pipeline creation. Wrangler for Exploration: Visual data exploration and transformation using directives. Monitoring and Automation: Track metrics, automate runs, and manage concurrency for optimized processing. Field Lineage Tracking: Detailed tracking of data transformations for comprehensive data management. Orchestrating Work Between Google Cloud Services with Cloud Composer Overview Orchestration: Managing multiple Google Cloud services (e.g., data fusion pipelines, ML models) in a specified order. Cloud Composer: Serverless environment running Apache Airflow for workflow orchestration. Apache Airflow Environment Launching Cloud Composer: Can be done via command line or Google Cloud web UI. Environment Variables: Edited at Apache Airflow instance level, not Cloud Composer level. Components: Airflow Web Server: Access via Cloud Composer to monitor and interact with workflows. DAGs Folder: Cloud storage bucket for storing Python DAG files. DAGs and Operators DAGs (Directed Acyclic Graphs): Represent workflows in Airflow. Consist of tasks invoking predefined operators. Operators: BigQuery Operators: Used for data querying and related tasks. Vertex AI Operators: Used for retraining and deploying ML models. Common Operators: Atomic tasks, often one operator per task. Sample DAG File from airflow import DAG from airflow.operators.dummy import DummyOperator from airflow.operators.python import PythonOperator from datetime import datetime def process_data(): ## Your data processing code here pass default_args = { 'start_date': datetime(2023, 1, 1), } dag = DAG( 'example_dag', default_args=default_args, schedule_interval='@daily', ) start = DummyOperator(task_id='start', dag=dag) process = PythonOperator(task_id='process', python_callable=process_data, dag=dag) end = DummyOperator(task_id='end', dag=dag) start \u003e\u003e process \u003e\u003e end Workflow Scheduling Periodic Scheduling Description: Set schedule (e.g., daily at 6 a.m.). Event-Driven Scheduling Description: Triggered by events (e.g., new CSV files in cloud storage). Event-Driven Example Cloud Function: Watches for new files in a Cloud Storage bucket and triggers a DAG. const { google } = require(\"googleapis\"); const dataflow = google.dataflow(\"v1b3\"); const PROJECT_ID = \"your-project-id\"; const LOCATION = \"your-location\"; const TEMPLATE_PATH = \"gs://your-template-path\"; const BUCKET_NAME = \"your-bucket-name\"; exports.triggerDAG = async (event, context) =\u003e { const file = event.name; const metadata = { projectId: PROJECT_ID, location: LOCATION, templatePath: TEMPLATE_PATH, gcsPath: `gs://${BUCKET_NAME}/${file}`, }; const request = { projectId: metadata.projectId, location: metadata.location, gcsPath: metadata.templatePath, requestBody: { parameters: { inputFile: metadata.gcsPath, }, }, }; try { const response = await dataflow.projects.locations.templates.launch( request ); console.log(`Job launched successfully: ${response.data.job.id}`); } catch (err) { console.error(`Error launching job: ${err}`); } }; Monitoring and Logging Monitoring DAG Runs Check status: Monitor DAGs for success, running, or failure states and troubleshoot accordingly. Logs: Available for each task and overall workflow. Google Cloud Logs Usage: Diagnose errors with Cloud Functions and other services. Troubleshooting Example Error Detection: Use logs to identify and correct issues (e.g., missing output bucket). Case Sensitivity: Ensure correct naming in Cloud Functions. ","wordCount":"3452","inLanguage":"en","image":"https://raghu-vijaykumar.github.io/blog/cover.png","datePublished":"2024-08-23T17:12:35Z","dateModified":"2024-08-23T17:12:35Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://raghu-vijaykumar.github.io/blog/posts/courses/building-batch-data-pipelines-gcp/"},"publisher":{"@type":"Organization","name":"Raghu Vijaykumar","logo":{"@type":"ImageObject","url":"https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://raghu-vijaykumar.github.io/blog/ accesskey=h title="Raghu Vijaykumar (Alt + H)"><img src=https://raghu-vijaykumar.github.io/apple-touch-icon.png alt aria-label=logo height=35>Raghu Vijaykumar</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://raghu-vijaykumar.github.io/blog/about/ title=about><span>about</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/docs/ title=docs><span>docs</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/sponsor/ title=sponsor><span>sponsor</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://raghu-vijaykumar.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://raghu-vijaykumar.github.io/blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building Batch Pipelines on Google Cloud Platform</h1><div class=post-meta><span title='2024-08-23 17:12:35.311939435 +0000 UTC'>August 23, 2024</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;3452 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction-to-building-batch-data-pipelines>Introduction to Building Batch Data Pipelines</a><ul><li><a href=#batch-pipelines-overview>Batch Pipelines Overview</a></li><li><a href=#quality-considerations-in-data-pipelines>Quality Considerations in Data Pipelines</a></li><li><a href=#shortcomings-of-elt-and-use-cases-for-etl>Shortcomings of ELT and Use Cases for ETL</a></li></ul></li><li><a href=#google-cloud-etl-tools>Google Cloud ETL Tools</a><ul><li><a href=#considerations-for-choosing-etl>Considerations for Choosing ETL</a></li><li><a href=#metadata-management-and-data-governance>Metadata Management and Data Governance</a></li></ul></li><li><a href=#executing-spark-on-dataproc>Executing Spark on Dataproc</a><ul><li><a href=#the-hadoop-ecosystem>The Hadoop Ecosystem</a></li><li><a href=#running-hadoop-on-dataproc>Running Hadoop on Dataproc</a></li><li><a href=#dataproc-cluster-architecture>Dataproc Cluster Architecture</a></li><li><a href=#key-features-of-dataproc>Key Features of Dataproc</a></li><li><a href=#using-google-cloud-storage-instead-of-hdfs>Using Google Cloud Storage Instead of HDFS</a></li><li><a href=#ephemeral-clusters>Ephemeral Clusters</a></li><li><a href=#autoscaling-and-templates>Autoscaling and Templates</a></li><li><a href=#monitoring-and-logging>Monitoring and Logging</a></li></ul></li><li><a href=#serverless-data-processing-with-dataflow>Serverless Data Processing with Dataflow</a><ul><li><a href=#why-customers-value-dataflow>Why Customers Value Dataflow</a></li><li><a href=#building-dataflow-pipelines-in-code>Building Dataflow Pipelines in Code</a></li><li><a href=#constructing-a-dataflow-pipeline>Constructing a Dataflow Pipeline</a></li><li><a href=#key-considerations-in-pipeline-design>Key Considerations in Pipeline Design</a></li><li><a href=#aggregate-with-groupbykey-and-combine>Aggregate with GroupByKey and Combine</a></li><li><a href=#side-inputs-and-windows-of-data>Side Inputs and Windows of Data</a></li><li><a href=#creating-and-re-using-pipeline-templates>Creating and Re-using Pipeline Templates</a></li></ul></li><li><a href=#manage-data-pipelines-with-cloud-data-fusion-and-cloud-composer>Manage Data Pipelines with Cloud Data Fusion and Cloud Composer</a><ul><li><a href=#components-of-cloud-data-fusion>Components of Cloud Data Fusion</a></li><li><a href=#building-a-pipeline>Building a Pipeline</a></li><li><a href=#exploring-data-using-wrangler>Exploring Data Using Wrangler</a></li><li><a href=#example-pipeline>Example Pipeline</a></li><li><a href=#key-metrics-and-features>Key Metrics and Features</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#orchestrating-work-between-google-cloud-services-with-cloud-composer>Orchestrating Work Between Google Cloud Services with Cloud Composer</a><ul><li><a href=#overview>Overview</a></li><li><a href=#apache-airflow-environment>Apache Airflow Environment</a></li><li><a href=#dags-and-operators>DAGs and Operators</a></li><li><a href=#workflow-scheduling>Workflow Scheduling</a></li><li><a href=#monitoring-and-logging-1>Monitoring and Logging</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><img loading=lazy src=./cover.png alt=cover></p><h2 id=introduction-to-building-batch-data-pipelines>Introduction to Building Batch Data Pipelines<a hidden class=anchor aria-hidden=true href=#introduction-to-building-batch-data-pipelines>#</a></h2><h3 id=batch-pipelines-overview>Batch Pipelines Overview<a hidden class=anchor aria-hidden=true href=#batch-pipelines-overview>#</a></h3><ul><li><p><strong>Definition:</strong> Pipelines that process a fixed amount of data and terminate.</p><ul><li>Example: Daily processing of financial transactions to balance books and store reconciled data.</li></ul></li><li><p><strong>ETL vs ELT:</strong></p><ul><li><p><strong>ELT (Extract, Load, Transform):</strong></p><ul><li>Load raw data into a target system and perform transformations on the fly as needed.</li><li>Suitable when transformations depend on user needs or view requirements (e.g., raw data accessible through views).</li></ul></li><li><p><strong>ETL (Extract, Transform, Load):</strong></p><ul><li>Extract data, transform it in an intermediate system, then load into a target system (e.g., Dataflow to BigQuery).</li><li>Preferred for complex transformations or data quality improvements before loading.</li></ul></li></ul></li><li><p><strong>Use Cases:</strong></p><ul><li><p><strong>EL (Extract and Load):</strong></p><ul><li>Use when data is clean and ready for direct loading (e.g., log files from cloud storage).</li><li>Suitable for batch loading and scheduled data loads.</li></ul></li><li><p><strong>ELT (Extract, Load, Transform):</strong></p><ul><li>Start with loading as EL, then transform data on-demand or based on evolving requirements.</li><li>Useful when transformations are uncertain initially, like processing API JSON responses in BigQuery.</li></ul></li></ul></li></ul><h3 id=quality-considerations-in-data-pipelines>Quality Considerations in Data Pipelines<a hidden class=anchor aria-hidden=true href=#quality-considerations-in-data-pipelines>#</a></h3><ul><li><p><strong>Data Quality Dimensions:</strong></p><ul><li><strong>Validity:</strong> Data conforms to business rules (e.g., transaction amount validation).</li><li><strong>Accuracy:</strong> Data reflects objective truth (e.g., precise financial calculations).</li><li><strong>Completeness:</strong> All required data is processed (e.g., no missing records).</li><li><strong>Consistency:</strong> Data consistency across different sources (e.g., uniform formats).</li><li><strong>Uniformity:</strong> Data values within the same column are consistent.</li></ul></li><li><p><strong>Methods to Address Quality Issues:</strong></p><ul><li><strong>BigQuery Capabilities:</strong><ul><li>Use views to filter out invalid data rows (e.g., negative quantities).</li><li>Aggregate and filter using <code>HAVING</code> clause for data completeness checks.</li><li>Handle nulls and blanks using <code>COUNTIF</code> and <code>IF</code> statements for accurate computations.</li></ul></li></ul></li><li><p><strong>Ensuring Data Integrity:</strong></p><ul><li><strong>Verification and Backfilling:</strong><ul><li>Verify file integrity with checksums (e.g., MD5).</li><li>Address consistency issues such as duplicates with <code>COUNT</code> and <code>COUNT DISTINCT</code>.</li><li>Clean data using string functions to remove extra characters or format inconsistencies.</li></ul></li></ul></li><li><p><strong>Documentation and Best Practices:</strong></p><ul><li>Document SQL queries, transformations, and data units clearly to maintain uniformity.</li><li>Use SQL functions like <code>CAST</code> and <code>FORMAT</code> to manage data type changes and indicate units clearly.</li></ul></li></ul><h3 id=shortcomings-of-elt-and-use-cases-for-etl>Shortcomings of ELT and Use Cases for ETL<a hidden class=anchor aria-hidden=true href=#shortcomings-of-elt-and-use-cases-for-etl>#</a></h3><ul><li><p><strong>Complex Transformations Beyond SQL:</strong></p><ul><li>Examples include:<ul><li>Translating text (e.g., Spanish to English using external APIs).</li><li>Analyzing complex customer action streams over time windows.</li></ul></li><li>SQL may not handle these transformations efficiently or feasibly.</li></ul></li><li><p><strong>Continuous Data Quality Control and Transformation:</strong></p><ul><li>Use ETL when:<ul><li>Raw data requires extensive quality control or enrichment before loading into BigQuery.</li><li>Transformations are too complex or cannot be expressed in SQL.</li><li>Continuous streaming data needs processing and integration (e.g., using Dataflow for real-time updates).</li></ul></li></ul></li><li><p><strong>Integration with CI/CD Systems:</strong></p><ul><li>ETL is suitable for integrating with continuous integration and continuous delivery (CI/CD) systems.<ul><li>Allows for unit testing of pipeline components and scheduled launches of Dataflow pipelines.</li></ul></li></ul></li></ul><h2 id=google-cloud-etl-tools>Google Cloud ETL Tools<a hidden class=anchor aria-hidden=true href=#google-cloud-etl-tools>#</a></h2><ul><li><p><strong>Dataflow:</strong></p><ul><li>Fully managed, serverless data processing service based on Apache Beam.</li><li>Supports both batch and streaming data processing pipelines.</li><li>Ideal for transforming and enriching data before loading into BigQuery.</li></ul></li><li><p><strong>Dataproc:</strong></p><ul><li>Managed service for running Apache Hadoop and Apache Spark jobs.</li><li>Cost-effective for complex batch processing, querying, and machine learning tasks.</li><li>Offers features like autoscaling and integrates seamlessly with BigQuery.</li></ul></li><li><p><strong>Data Fusion:</strong></p><ul><li>Cloud-native enterprise data integration service for building and managing data pipelines.</li><li>Provides a visual interface for non-programmers to create pipelines.</li><li>Supports transformation, cleanup, and ensuring data consistency across various sources.</li></ul></li></ul><h3 id=considerations-for-choosing-etl>Considerations for Choosing ETL<a hidden class=anchor aria-hidden=true href=#considerations-for-choosing-etl>#</a></h3><ul><li><p><strong>Specific Needs that ETL Addresses:</strong></p><ul><li><p><strong>Low Latency and High Throughput:</strong></p><ul><li>BigQuery offers low latency (100 ms with BI Engine) and high throughput (up to 1 million rows per second).</li><li>For even stricter latency and throughput requirements, consider Cloud Bigtable.</li></ul></li><li><p><strong>Reusing Existing Spark Pipelines:</strong></p><ul><li>If your organization already uses Hadoop and Spark extensively, leveraging Dataproc may be more productive.</li></ul></li><li><p><strong>Visual Pipeline Building:</strong></p><ul><li>Data Fusion provides a drag-and-drop interface for visual pipeline creation, suitable for non-technical users.</li></ul></li></ul></li></ul><h3 id=metadata-management-and-data-governance>Metadata Management and Data Governance<a hidden class=anchor aria-hidden=true href=#metadata-management-and-data-governance>#</a></h3><ul><li><strong>Importance of Data Lineage and Metadata:</strong><ul><li>Data Catalog provides:<ul><li>Metadata management for cataloging data assets across multiple systems.</li><li>Access controls and integration with Cloud Data Loss Prevention API for data classification.</li><li>Unified data discovery and tagging capabilities for organizing and governing data assets effectively.</li></ul></li></ul></li></ul><h2 id=executing-spark-on-dataproc>Executing Spark on Dataproc<a hidden class=anchor aria-hidden=true href=#executing-spark-on-dataproc>#</a></h2><p><strong>Dataproc</strong>: Google Cloud&rsquo;s managed Hadoop service, focusing on Apache Spark.</p><h3 id=the-hadoop-ecosystem>The Hadoop Ecosystem<a hidden class=anchor aria-hidden=true href=#the-hadoop-ecosystem>#</a></h3><ul><li><strong>Historical Context</strong>:<ul><li>Pre-2006: Big data relied on large databases; storage was cheap, processing was expensive.</li><li>2006 Onward: Distributed processing of big data became practical with Hadoop.</li></ul></li><li><strong>Hadoop Components</strong>:<ul><li><strong>HDFS (Hadoop Distributed File System)</strong>: Stores data on cluster machines.</li><li><strong>MapReduce</strong>: Distributed data processing framework.</li><li><strong>Related Tools</strong>: Hive, Pig, Spark, Presto.</li></ul></li><li><strong>Apache Hadoop</strong>:<ul><li><strong>Open-source</strong> software for distributed processing across clusters.</li><li><strong>Main File System</strong>: HDFS.</li></ul></li><li><strong>Apache Spark</strong>:<ul><li><strong>High-performance</strong> analytics engine.</li><li><strong>In-memory processing</strong>: Up to 100 times faster than Hadoop jobs.</li><li><strong>Data Abstractions</strong>: Resilient Distributed Datasets (RDDs), DataFrames.</li></ul></li><li><strong>Cloud vs. On-premises</strong>:<ul><li><strong>On-premises Hadoop</strong>: Limited by physical resources.</li><li><strong>Cloud Dataproc</strong>: Overcomes tuning and utilization issues of OSS Hadoop.</li></ul></li><li><strong>Dataproc Benefits</strong>:<ul><li><strong>Managed hardware/configuration</strong>: No need for physical hardware management.</li><li><strong>Version management</strong>: Simplifies updating open-source tools.</li><li><strong>Flexible job configuration</strong>: Create multiple clusters for specific tasks.</li></ul></li></ul><h3 id=running-hadoop-on-dataproc>Running Hadoop on Dataproc<a hidden class=anchor aria-hidden=true href=#running-hadoop-on-dataproc>#</a></h3><ul><li><strong>Advantages of Dataproc</strong>:<ul><li><strong>Low Cost</strong>: Priced at one cent per virtual CPU per cluster per hour.</li><li><strong>Speed</strong>: Clusters start, scale, and shut down in 90 seconds or less.</li><li><strong>Resizable Clusters</strong>: Quick scaling with various VM types and disk sizes.</li><li><strong>Open-source Ecosystem</strong>: Frequent updates to Spark, Hadoop, Pig, and Hive.</li><li><strong>Integration</strong>: Works seamlessly with Cloud Storage, BigQuery, Cloud Bigtable.</li><li><strong>Management</strong>: Easy interaction through cloud console, SDK, REST API.</li><li><strong>Versioning</strong>: Switch between different versions of Spark, Hadoop, and other tools.</li><li><strong>High Availability</strong>: Multiple primary nodes, job restart on failure.</li><li><strong>Developer Tools</strong>: Cloud console, SDK, RESTful APIs, SSH access.</li><li><strong>Customization</strong>: Pre-configured optional components, initialization actions.</li></ul></li></ul><h3 id=dataproc-cluster-architecture>Dataproc Cluster Architecture<a hidden class=anchor aria-hidden=true href=#dataproc-cluster-architecture>#</a></h3><ul><li><strong>Cluster Components</strong>:<ul><li><strong>Primary Nodes</strong>: Run HDFS name node, node and job drivers.</li><li><strong>Worker Nodes</strong>: Can be part of a managed instance group.</li><li><strong>Preemptible Secondary Workers</strong>: Lower cost but can be preempted anytime.</li></ul></li><li><strong>Storage Options</strong>:<ul><li><strong>Persistent Disks</strong>: Standard storage method.</li><li><strong>Cloud Storage</strong>: Recommended over native HDFS for durability.</li><li><strong>Cloud Bigtable/BigQuery</strong>: Alternatives for HBase and analytical workloads.</li></ul></li><li><strong>Cluster Lifecycle</strong>:<ul><li><strong>Setup</strong>: Create clusters via cloud console, command line, YAML files, Terraform, or REST API.</li><li><strong>Configuration</strong>: Specify region, zone, primary node, worker nodes.</li><li><strong>Initialization</strong>: Customize with initialization scripts and metadata.</li><li><strong>Optimization</strong>: Use preemptible VMs and custom machine types to balance cost and performance.</li><li><strong>Utilization</strong>: Submit jobs through console, command line, REST API, or orchestration services.</li><li><strong>Monitoring</strong>: Cloud Monitoring for job and cluster metrics, alert policies for incidents.</li></ul></li></ul><h3 id=key-features-of-dataproc>Key Features of Dataproc<a hidden class=anchor aria-hidden=true href=#key-features-of-dataproc>#</a></h3><ul><li><strong>Low Cost</strong>: Pay-per-use with second-by-second billing.</li><li><strong>Fast Cluster Operations</strong>: 90 seconds or less to start, scale, or shut down.</li><li><strong>Flexible and Resizable</strong>: Variety of VM types, disk sizes, and networking options.</li><li><strong>Integration with Google Cloud</strong>: Seamless interaction with other Google Cloud services.</li><li><strong>Ease of Use</strong>: Minimal learning curve, supports existing Hadoop tools.</li><li><strong>High Availability and Reliability</strong>: Multiple primary nodes, restart able jobs.</li><li><strong>Developer-Friendly</strong>: Multiple ways to manage clusters, customizable via scripts and metadata.</li></ul><h3 id=using-google-cloud-storage-instead-of-hdfs>Using Google Cloud Storage Instead of HDFS<a hidden class=anchor aria-hidden=true href=#using-google-cloud-storage-instead-of-hdfs>#</a></h3><h4 id=network-evolution-and-data-proximity>Network Evolution and Data Proximity<a hidden class=anchor aria-hidden=true href=#network-evolution-and-data-proximity>#</a></h4><ul><li><strong>Original Network Speeds</strong>: Slow, necessitating data to be close to the processor.</li><li><strong>Modern Network Speeds</strong>: Petabit networking enables independent storage and compute.</li><li><strong>On-Premise Hadoop Clusters</strong>: Require local storage on disks as the same server handles compute and storage.</li></ul><h4 id=cloud-migration-and-hdfs>Cloud Migration and HDFS<a hidden class=anchor aria-hidden=true href=#cloud-migration-and-hdfs>#</a></h4><ul><li><strong>Lift-and-Shift</strong>: Moving Hadoop workloads to Dataproc with HDFS requires no code changes but is a short-term solution.</li><li><strong>Long-Term Issues with HDFS in Cloud</strong>:<ul><li><strong>Block Size</strong>: Performance is tied to server hardware, not elastic.</li><li><strong>Data Locality</strong>: Storing data on persistent disks limits flexibility.</li><li><strong>Replication</strong>: HDFS requires three copies for high availability, leading to inefficiency.</li></ul></li></ul><h4 id=advantages-of-cloud-storage-over-hdfs>Advantages of Cloud Storage over HDFS<a hidden class=anchor aria-hidden=true href=#advantages-of-cloud-storage-over-hdfs>#</a></h4><ul><li><strong>Network Efficiency</strong>: Google’s Jupyter network fabric offers over one petabit per second bandwidth.</li><li><strong>Elastic Storage</strong>: Cloud Storage scales independently of compute resources.</li><li><strong>Cost Efficiency</strong>: Pay only for what you use, unlike HDFS which requires overprovisioning.</li><li><strong>Integration</strong>: Cloud Storage integrates seamlessly with various Google Cloud services.</li></ul><h4 id=considerations-for-using-cloud-storage>Considerations for Using Cloud Storage<a hidden class=anchor aria-hidden=true href=#considerations-for-using-cloud-storage>#</a></h4><ul><li><strong>Latency</strong>: High throughput but significant latency; suitable for large, bulk operations.</li><li><strong>Object Storage Nature</strong>:<ul><li>Renaming objects is expensive due to immutability.</li><li>Cannot append to objects.</li></ul></li><li><strong>Output Committers</strong>: New object store-oriented committers help mitigate directory rename issues.</li></ul><h4 id=data-transfer-and-management>Data Transfer and Management<a hidden class=anchor aria-hidden=true href=#data-transfer-and-management>#</a></h4><ul><li><strong>DistCp Tool</strong>: Essential for moving data; push-based model preferred for known necessary data.</li><li><strong>Data Management Continuum</strong>:<ul><li>Pre-2006: Big databases for big data.</li><li>2006: Distributed processing with Hadoop.</li><li>2010: BigQuery launch.</li><li>2015: Google’s Dataproc for managed Hadoop and Spark clusters.</li></ul></li></ul><h3 id=ephemeral-clusters>Ephemeral Clusters<a hidden class=anchor aria-hidden=true href=#ephemeral-clusters>#</a></h3><ul><li><strong>Ephemeral Model</strong>: Utilize clusters as temporary resources, reducing costs by not paying for idle compute capacity.</li><li><strong>Workflow Example</strong>: Use Cloud Storage for initial and final data, with intermediate processing in local HDFS if needed.</li></ul><h4 id=cluster-optimization>Cluster Optimization<a hidden class=anchor aria-hidden=true href=#cluster-optimization>#</a></h4><ul><li><strong>Data Locality</strong>: Ensure Cloud Storage bucket and Dataproc region are physically close.</li><li><strong>Network Configuration</strong>: Avoid bottlenecks by ensuring traffic isn’t funneled through limited gateways.</li><li><strong>Input File Management</strong>:<ul><li>Reduce the number of input files (&lt;10,000) and partitions (&lt;50,000) if possible.</li><li>Adjust <code>fs.gs.block.size</code> for larger datasets.</li></ul></li></ul><h4 id=persistent-disks-and-virtual-machines>Persistent Disks and Virtual Machines<a hidden class=anchor aria-hidden=true href=#persistent-disks-and-virtual-machines>#</a></h4><ul><li><strong>Disk Size</strong>: Choose appropriate persistent disk size to avoid performance limitations.</li><li><strong>VM Allocation</strong>: Prototype and benchmark with real data to determine optimal VM size.</li><li><strong>Job-Scoped Clusters</strong>: Customize clusters for specific tasks to optimize resource use.</li></ul><h4 id=local-hdfs-vs-cloud-storage>Local HDFS vs. Cloud Storage<a hidden class=anchor aria-hidden=true href=#local-hdfs-vs-cloud-storage>#</a></h4><ul><li><strong>Local HDFS</strong>: Better for jobs requiring frequent metadata operations, small file sizes, or heavy I/O workloads with low latency needs.</li><li><strong>Cloud Storage</strong>: Ideal for the initial and final data source in pipelines, reducing disk requirements and saving costs.</li></ul><h3 id=autoscaling-and-templates>Autoscaling and Templates<a hidden class=anchor aria-hidden=true href=#autoscaling-and-templates>#</a></h3><ul><li><strong>Workflow Templates</strong>: Automate cluster creation, job submission, and cluster deletion.</li><li><strong>Autoscaling</strong>: Scale clusters based on YARN metrics, providing flexible capacity and improving resource utilization.</li><li><strong>Initial Workers</strong>: Set the number of initial workers to ensure basic capacity from the start.</li></ul><h3 id=monitoring-and-logging>Monitoring and Logging<a hidden class=anchor aria-hidden=true href=#monitoring-and-logging>#</a></h3><ul><li><strong>Cloud Logging</strong>: Consolidates logs for easy error diagnosis and monitoring.</li><li><strong>Custom Dashboards</strong>: Use Cloud Monitoring to visualize CPU, disk, and network usage, and YARN resources.</li><li><strong>Application Logs</strong>: Retrieve Spark job logs from the driver output, Cloud Console, or Cloud Storage bucket.</li></ul><h2 id=serverless-data-processing-with-dataflow>Serverless Data Processing with Dataflow<a hidden class=anchor aria-hidden=true href=#serverless-data-processing-with-dataflow>#</a></h2><ul><li><p><strong>Dataflow Overview:</strong></p><ul><li><strong>Serverless:</strong> Dataflow is serverless, eliminating the need to manage clusters.</li><li><strong>Scaling:</strong> Scales step by step, fine-grained scaling unlike Data Proc.</li><li><strong>Unified Processing:</strong> Supports both batch and stream processing with the same code.</li></ul></li><li><p><strong>Choosing Between Dataflow and DataProc:</strong></p><ul><li><strong>Recommendation:</strong> Use Dataflow for new pipelines due to serverless benefits.</li><li><strong>Existing Hadoop Pipelines:</strong> Consider Data Proc for migration and modernization.</li><li><strong>Decision Factors:</strong> Depends on existing technology stack and operational preferences.</li></ul></li><li><p><strong>Unified Programming with Apache Beam:</strong></p><ul><li><strong>Historical Context:</strong> Batch processing (1940s) vs. stream processing (1970s).</li><li><strong>Apache Beam:</strong> Unifies batch and stream processing concepts.</li><li><strong>Core Concepts:</strong> P transforms, P collections, pipelines, and pipeline runners.</li></ul></li><li><p><strong>Immutable Data and Distributed Processing:</strong></p><ul><li><strong>P collections:</strong> Immutable distributed data abstractions in Dataflow.</li><li><strong>Serialization:</strong> All data types are serialized as byte strings for efficient network transfer.</li><li><strong>Processing Model:</strong> Each transform creates a new P collection, enabling distributed processing.</li></ul></li></ul><h3 id=why-customers-value-dataflow>Why Customers Value Dataflow<a hidden class=anchor aria-hidden=true href=#why-customers-value-dataflow>#</a></h3><ul><li><p><strong>Efficient Execution and Optimization:</strong></p><ul><li><strong>Pipeline Execution:</strong> Dataflow manages job execution from data source to sink.</li><li><strong>Optimization:</strong> Optimizes job graph, fuses transforms, and dynamically rebalances work units.</li><li><strong>Resource Management:</strong> On-demand deployment of compute and storage resources per job.</li></ul></li><li><p><strong>Reliability and Fault Tolerance:</strong></p><ul><li><strong>Continuous Optimization:</strong> Ongoing optimization and rebalancing of resources.</li><li><strong>Fault Tolerance:</strong> Handles late data arrivals, with automatic restarts and monitoring.</li><li><strong>Auto-Scaling:</strong> Scales resources dynamically during job execution, improving job efficiency.</li></ul></li><li><p><strong>Operational Efficiency and Integration:</strong></p><ul><li><strong>Managed Service:</strong> Fully managed and auto-configured service, reducing operational overhead.</li><li><strong>Integration:</strong> Acts as a central component connecting various Google Cloud services.</li><li><strong>Use Cases:</strong> Supports diverse use cases from BigQuery to Cloud SQL seamlessly.</li></ul></li></ul><h3 id=building-dataflow-pipelines-in-code>Building Dataflow Pipelines in Code<a hidden class=anchor aria-hidden=true href=#building-dataflow-pipelines-in-code>#</a></h3><h3 id=constructing-a-dataflow-pipeline>Constructing a Dataflow Pipeline<a hidden class=anchor aria-hidden=true href=#constructing-a-dataflow-pipeline>#</a></h3><ul><li><p><strong>Pipeline Construction:</strong></p><ul><li><strong>Python Syntax:</strong> Use pipe symbols (<code>|</code>) to chain P transforms from an input P collection.</li><li><strong>Java Syntax:</strong> Use <code>.apply()</code> method instead of pipe symbols.</li></ul></li><li><p><strong>Branching in Pipelines:</strong></p><ul><li><strong>Multiple Transform Paths:</strong> Apply the same input P collection to different transforms.</li><li><strong>Named Outputs:</strong> Store results in separate P collection variables (<code>Pcollection_out1</code>, <code>Pcollection_out2</code>).</li></ul></li><li><p><strong>Pipeline Initialization and Termination:</strong></p><ul><li><strong>Pipeline Start:</strong> Begin with a source to fetch the initial P collection (e.g., <code>readFromText</code>).</li><li><strong>Pipeline End:</strong> Terminate with a sink operation (e.g., writing to a text file in cloud storage).</li><li><strong>Context Management:</strong> Use Python <code>with</code> clause to manage pipeline execution context.</li></ul></li></ul><h3 id=key-considerations-in-pipeline-design>Key Considerations in Pipeline Design<a hidden class=anchor aria-hidden=true href=#key-considerations-in-pipeline-design>#</a></h3><ul><li><p><strong>Data Sources and Sinks:</strong></p><ul><li><strong>Reading Data:</strong> Utilize <code>beam.io</code> to read from various sources like text files, Pub/Sub, or BigQuery.</li><li><strong>Writing Data:</strong> Specify sinks such as BigQuery tables using <code>beam.io.WriteToBigQuery</code>.</li></ul></li><li><p><strong>Transforming Data with PTransforms:</strong></p><ul><li><strong>Mapping Data:</strong> Use <code>Map</code> to apply a function to each element in the P collection.</li><li><strong>Flat Mapping:</strong> Apply <code>FlatMap</code> to handle one-to-many relationships, flattening results.</li><li><strong>ParDo Transformation:</strong> Custom processing on each element, with serialization and thread safety ensured.</li><li><strong>DoFn Class:</strong> Define distributed processing functions, ensuring they are serializable and idempotent.</li></ul></li><li><p><strong>Handling Multiple Outputs:</strong></p><ul><li><strong>Branching and Outputs:</strong> Output different types or formats from a single P collection using <code>ParDo</code>.</li></ul></li></ul><h3 id=aggregate-with-groupbykey-and-combine>Aggregate with GroupByKey and Combine<a hidden class=anchor aria-hidden=true href=#aggregate-with-groupbykey-and-combine>#</a></h3><ul><li><p><strong>Dataflow Model</strong>: Utilizes a shuffle phase after the map phase to group together like keys.</p><ul><li>Operates on a PCollection of key-value pairs (tuples).</li><li>Groups by common key, returning a key-value pair where the value is a group of values.</li></ul></li><li><p><strong>Example</strong>: Finding zip codes associated with a city.</p><ul><li><strong>Key-Value Pair Creation</strong>: Create pairs and group by key.</li><li><strong>Data Skew Issue</strong>:<ul><li><strong>Small Scale</strong>: Manageable with 1 million items.</li><li><strong>Large Scale</strong>: 1 billion items can cause performance issues.</li><li><strong>High Cardinality</strong>: Performance concerns in queries with billions of records.</li></ul></li></ul></li><li><p><strong>Performance Concern</strong>:</p><ul><li>Grouping by key causes unbalanced workload among workers.</li><li><strong>Example</strong>:<ul><li>1 million X values on one worker.</li><li>1,000 Y values on another worker.</li></ul></li><li><strong>Inefficiency</strong>: Idle worker waiting for the other to complete.</li></ul></li><li><p><strong>Dataflow Optimization</strong>:</p><ul><li>Design applications to divide work into aggregation steps.</li><li>Push grouping towards the end of the processing pipeline.</li></ul></li><li><p><strong>CoGroupByKey</strong>: Similar to GroupByKey but groups results across multiple PCollections by key.</p></li><li><p><strong>Combine</strong>: Used to aggregate values in PCollections.</p><ul><li><strong>Combine Variants</strong>:<ul><li><strong>Combine.globally()</strong>: Reduces a PCollection to a single value.</li><li><strong>Combine.perKey()</strong>: Similar to GroupByKey but combines values using a specified function.</li></ul></li><li><strong>Prebuilt Combine Functions</strong>: For common operations like sum, min, and max.</li><li><strong>Custom Combine Function</strong>: Create a subclass of CombineFn for complex operations.<ul><li><strong>Four Operations</strong>:<ol><li><strong>Create Accumulator</strong>: New local accumulator.</li><li><strong>Add Input</strong>: Add input to the accumulator.</li><li><strong>Merge Accumulators</strong>: Merge multiple accumulators.</li><li><strong>Extract Output</strong>: Produce final result from the accumulator.</li></ol></li></ul></li><li><strong>Efficiency</strong>: Combine is faster than GroupByKey due to parallelization.</li><li><strong>Custom Combine Class</strong>: For operations with commutative and associative properties.</li></ul></li><li><p><strong>Flatten</strong>: Merges multiple PCollection objects into a single PCollection, similar to SQL UNION.</p></li><li><p><strong>Partition</strong>: Splits a single PCollection into smaller collections.</p><ul><li><strong>Use Case</strong>: Different processing for different quartiles.</li></ul></li></ul><h3 id=side-inputs-and-windows-of-data>Side Inputs and Windows of Data<a hidden class=anchor aria-hidden=true href=#side-inputs-and-windows-of-data>#</a></h3><ul><li><strong>Side Inputs</strong>: Additional inputs to a ParDo transform.<ul><li><strong>Use Case</strong>: Inject additional data at runtime.</li><li><strong>Example</strong>:<ul><li>Compute average word length.</li><li>Use as side input to determine if a word is longer or shorter than average.</li></ul></li></ul></li><li><strong>Dataflow Windows</strong>:<ul><li><strong>Global Window</strong>: Default, not useful for unbounded PCollections (streaming data).</li><li><strong>Time-based Windows</strong>:<ul><li>Useful for processing streaming data.</li><li><strong>Example</strong>: Sliding Windows - <code>beam.WindowInto(beam.window.SlidingWindows(60, 30))</code>.<ul><li>Capture 60 seconds of data, start new window every 30 seconds.</li></ul></li><li><strong>Fixed Windows</strong>: Example - group sales by day.</li></ul></li></ul></li></ul><h3 id=creating-and-re-using-pipeline-templates>Creating and Re-using Pipeline Templates<a hidden class=anchor aria-hidden=true href=#creating-and-re-using-pipeline-templates>#</a></h3><ul><li><strong>Dataflow Templates</strong>: Enable non-developers to execute dataflow jobs.<ul><li><strong>Workflow</strong>:<ul><li>Developer creates pipeline with Dataflow SDK (Java/Python).</li><li>Separate development from execution activities.</li></ul></li><li><strong>Benefits</strong>:<ul><li>Simplifies scheduling batch jobs.</li><li>Allows deployment from environments like App Engine, Cloud Functions.</li></ul></li></ul></li><li><strong>Custom Templates</strong>:<ul><li><strong>Value Providers</strong>: Parse command-line or optional arguments.</li><li><strong>Runtime Parameters</strong>: Convert compile-time parameters for user customization.</li><li><strong>Metadata File</strong>: Describes template parameters and functions for downstream users.</li></ul></li></ul><h2 id=manage-data-pipelines-with-cloud-data-fusion-and-cloud-composer>Manage Data Pipelines with Cloud Data Fusion and Cloud Composer<a hidden class=anchor aria-hidden=true href=#manage-data-pipelines-with-cloud-data-fusion-and-cloud-composer>#</a></h2><ul><li><strong>Cloud Data Fusion</strong> provides a <strong>graphical user interface</strong> and <strong>APIs</strong> to build, deploy, and manage data integration pipelines efficiently.</li><li><strong>Users</strong>: Developers, Data Scientists, Business Analysts<ul><li><strong>Developers</strong>: Cleanse, match, remove duplicates, blend, transform, partition, transfer, standardize, automate, and monitor data.</li><li><strong>Data Scientists</strong>: Visually build integration pipelines, test, debug, and deploy applications.</li><li><strong>Business Analysts</strong>: Run at scale, operationalize pipelines, inspect rich integration metadata.</li></ul></li><li><strong>Benefits</strong>:<ul><li><strong>Integration</strong>: Connects with a variety of systems (legacy, modern, relational databases, file systems, cloud services, NoSQL, etc.).</li><li><strong>Productivity</strong>: Centralizes data from various sources (e.g., BigQuery, Cloud Spanner).</li><li><strong>Reduced Complexity</strong>: Visual interface for building pipelines, code-free transformations, reusable templates.</li><li><strong>Flexibility</strong>: Supports on-prem and cloud environments, interoperable with open-source software (CDAP).</li><li><strong>Extensibility</strong>: Template pipelines, create conditional triggers, manage and create plugins, custom compute profiles.</li></ul></li></ul><h3 id=components-of-cloud-data-fusion>Components of Cloud Data Fusion<a hidden class=anchor aria-hidden=true href=#components-of-cloud-data-fusion>#</a></h3><ul><li><strong>User Interface Components</strong>:<ul><li><strong>Wrangler UI</strong>: Explore datasets visually, build pipelines with no code.</li><li><strong>Data Pipeline UI</strong>: Draw pipelines on a canvas.</li><li><strong>Control Center</strong>: Manage applications, artifacts, and datasets.</li><li><strong>Pipeline Section</strong>: Developer studio, preview, export, schedule jobs, connector, function palette, navigation.</li><li><strong>Integration Metadata</strong>: Search, add tags and properties, see data lineage.</li><li><strong>Hub</strong>: Available plugins, sample use cases, prebuilt pipelines.</li><li><strong>Administration</strong>: Management (services, metrics) and configuration (namespace, compute profiles, preferences, system artifacts, REST client).</li></ul></li></ul><h3 id=building-a-pipeline>Building a Pipeline<a hidden class=anchor aria-hidden=true href=#building-a-pipeline>#</a></h3><ul><li><strong>Pipeline Representation</strong>: Visual series of stages in a <strong>Directed Acyclic Graph (DAG)</strong>.<ul><li><strong>Nodes</strong>: Different types (e.g., data from cloud storage, parse CSV, join data, sync data).</li><li><strong>Canvas</strong>: Area for creating and chaining nodes.</li><li><strong>Mini Map</strong>: Navigate large pipelines.</li><li><strong>Pipeline Actions Tool Bar</strong>: Save, run, and manage pipelines.</li><li><strong>Templates and Plugins</strong>: Use pre-existing resources to avoid starting from scratch.</li><li><strong>Preview Mode</strong>: Test and debug pipelines before deployment.</li><li><strong>Monitoring</strong>: Track health, data throughput, processing time, and anomalies.</li><li><strong>Tags Feature</strong>: Organize pipelines for quick access.</li><li><strong>Concurrency</strong>: Set maximum number of concurrent runs to optimize processing.</li></ul></li></ul><h3 id=exploring-data-using-wrangler>Exploring Data Using Wrangler<a hidden class=anchor aria-hidden=true href=#exploring-data-using-wrangler>#</a></h3><ul><li><strong>Wrangler UI</strong>: For exploring and analyzing new datasets visually.<ul><li><strong>Connections</strong>: Add and manage connections to data sources (e.g., Google Cloud Storage, BigQuery).</li><li><strong>Data Exploration</strong>: Inspect rows and columns, view sample insights.</li><li><strong>Data Transformation</strong>: Create calculated fields, drop columns, filter rows using directives to form a transformation recipe.</li><li><strong>Pipeline Creation</strong>: Convert transformations into pipelines for regular execution.</li></ul></li></ul><h3 id=example-pipeline>Example Pipeline<a hidden class=anchor aria-hidden=true href=#example-pipeline>#</a></h3><ul><li><strong>Twitter Data Ingestion</strong>:<ul><li>Ingest data from Twitter and Google Cloud.</li><li>Parse tweets.</li><li>Load into various data sinks.</li></ul></li><li><strong>Health Monitoring</strong>:<ul><li>View start time, duration, and summary of pipeline runs.</li><li>Data throughput at each node.</li><li>Inputs, outputs, and errors per node.</li></ul></li><li><strong>Streaming Data Pipelines</strong>: Future modules will cover streaming data pipelines.</li></ul><h3 id=key-metrics-and-features>Key Metrics and Features<a hidden class=anchor aria-hidden=true href=#key-metrics-and-features>#</a></h3><ul><li><strong>Metrics</strong>: Records out per second, average processing time, max processing time.</li><li><strong>Automation</strong>: Set pipelines to run automatically at intervals.</li><li><strong>Field Lineage Tracking</strong>: Track transformations applied to data fields.</li></ul><h4 id=example-campaign-field-lineage>Example: Campaign Field Lineage<a hidden class=anchor aria-hidden=true href=#example-campaign-field-lineage>#</a></h4><ul><li>Track every transformation before and after the field.</li><li>View the lineage of operations between datasets.</li><li>Identify time of last change and involved input fields.</li></ul><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><ul><li><strong>Cloud Data Fusion</strong>: Efficient, flexible, and extensible tool for building and managing data pipelines.</li><li><strong>User Interfaces</strong>: Wrangler UI for visual exploration, Data Pipeline UI for pipeline creation.</li><li><strong>Building Pipelines</strong>: Use DAGs, nodes, canvas, templates, and plugins for streamlined pipeline creation.</li><li><strong>Wrangler for Exploration</strong>: Visual data exploration and transformation using directives.</li><li><strong>Monitoring and Automation</strong>: Track metrics, automate runs, and manage concurrency for optimized processing.</li><li><strong>Field Lineage Tracking</strong>: Detailed tracking of data transformations for comprehensive data management.</li></ul><h2 id=orchestrating-work-between-google-cloud-services-with-cloud-composer>Orchestrating Work Between Google Cloud Services with Cloud Composer<a hidden class=anchor aria-hidden=true href=#orchestrating-work-between-google-cloud-services-with-cloud-composer>#</a></h2><h3 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h3><ul><li><strong>Orchestration</strong>: Managing multiple Google Cloud services (e.g., data fusion pipelines, ML models) in a specified order.</li><li><strong>Cloud Composer</strong>: Serverless environment running <strong>Apache Airflow</strong> for workflow orchestration.</li></ul><h3 id=apache-airflow-environment>Apache Airflow Environment<a hidden class=anchor aria-hidden=true href=#apache-airflow-environment>#</a></h3><ul><li><strong>Launching Cloud Composer</strong>: Can be done via command line or Google Cloud web UI.</li><li><strong>Environment Variables</strong>: Edited at Apache Airflow instance level, not Cloud Composer level.</li><li><strong>Components</strong>:<ul><li><strong>Airflow Web Server</strong>: Access via Cloud Composer to monitor and interact with workflows.</li><li><strong>DAGs Folder</strong>: Cloud storage bucket for storing Python DAG files.</li></ul></li></ul><h3 id=dags-and-operators>DAGs and Operators<a hidden class=anchor aria-hidden=true href=#dags-and-operators>#</a></h3><ul><li><strong>DAGs (Directed Acyclic Graphs)</strong>:<ul><li>Represent workflows in Airflow.</li><li>Consist of tasks invoking predefined operators.</li></ul></li><li><strong>Operators</strong>:<ul><li><strong>BigQuery Operators</strong>: Used for data querying and related tasks.</li><li><strong>Vertex AI Operators</strong>: Used for retraining and deploying ML models.</li><li><strong>Common Operators</strong>: Atomic tasks, often one operator per task.</li></ul></li></ul><h4 id=sample-dag-file>Sample DAG File<a hidden class=anchor aria-hidden=true href=#sample-dag-file>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>airflow</span> <span class=kn>import</span> <span class=n>DAG</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>airflow.operators.dummy</span> <span class=kn>import</span> <span class=n>DummyOperator</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>airflow.operators.python</span> <span class=kn>import</span> <span class=n>PythonOperator</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>process_data</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1>## Your data processing code here</span>
</span></span><span class=line><span class=cl>    <span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>default_args</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;start_date&#39;</span><span class=p>:</span> <span class=n>datetime</span><span class=p>(</span><span class=mi>2023</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dag</span> <span class=o>=</span> <span class=n>DAG</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;example_dag&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>default_args</span><span class=o>=</span><span class=n>default_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>schedule_interval</span><span class=o>=</span><span class=s1>&#39;@daily&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>start</span> <span class=o>=</span> <span class=n>DummyOperator</span><span class=p>(</span><span class=n>task_id</span><span class=o>=</span><span class=s1>&#39;start&#39;</span><span class=p>,</span> <span class=n>dag</span><span class=o>=</span><span class=n>dag</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>process</span> <span class=o>=</span> <span class=n>PythonOperator</span><span class=p>(</span><span class=n>task_id</span><span class=o>=</span><span class=s1>&#39;process&#39;</span><span class=p>,</span> <span class=n>python_callable</span><span class=o>=</span><span class=n>process_data</span><span class=p>,</span> <span class=n>dag</span><span class=o>=</span><span class=n>dag</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>end</span> <span class=o>=</span> <span class=n>DummyOperator</span><span class=p>(</span><span class=n>task_id</span><span class=o>=</span><span class=s1>&#39;end&#39;</span><span class=p>,</span> <span class=n>dag</span><span class=o>=</span><span class=n>dag</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>start</span> <span class=o>&gt;&gt;</span> <span class=n>process</span> <span class=o>&gt;&gt;</span> <span class=n>end</span>
</span></span></code></pre></div><h3 id=workflow-scheduling>Workflow Scheduling<a hidden class=anchor aria-hidden=true href=#workflow-scheduling>#</a></h3><h4 id=periodic-scheduling>Periodic Scheduling<a hidden class=anchor aria-hidden=true href=#periodic-scheduling>#</a></h4><ul><li><strong>Description</strong>: Set schedule (e.g., daily at 6 a.m.).</li></ul><h4 id=event-driven-scheduling>Event-Driven Scheduling<a hidden class=anchor aria-hidden=true href=#event-driven-scheduling>#</a></h4><ul><li><strong>Description</strong>: Triggered by events (e.g., new CSV files in cloud storage).</li></ul><h4 id=event-driven-example>Event-Driven Example<a hidden class=anchor aria-hidden=true href=#event-driven-example>#</a></h4><ul><li><strong>Cloud Function</strong>: Watches for new files in a Cloud Storage bucket and triggers a DAG.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=p>{</span> <span class=nx>google</span> <span class=p>}</span> <span class=o>=</span> <span class=nx>require</span><span class=p>(</span><span class=s2>&#34;googleapis&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>dataflow</span> <span class=o>=</span> <span class=nx>google</span><span class=p>.</span><span class=nx>dataflow</span><span class=p>(</span><span class=s2>&#34;v1b3&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>PROJECT_ID</span> <span class=o>=</span> <span class=s2>&#34;your-project-id&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>LOCATION</span> <span class=o>=</span> <span class=s2>&#34;your-location&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>TEMPLATE_PATH</span> <span class=o>=</span> <span class=s2>&#34;gs://your-template-path&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>BUCKET_NAME</span> <span class=o>=</span> <span class=s2>&#34;your-bucket-name&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>exports</span><span class=p>.</span><span class=nx>triggerDAG</span> <span class=o>=</span> <span class=kr>async</span> <span class=p>(</span><span class=nx>event</span><span class=p>,</span> <span class=nx>context</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>file</span> <span class=o>=</span> <span class=nx>event</span><span class=p>.</span><span class=nx>name</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>metadata</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>projectId</span><span class=o>:</span> <span class=nx>PROJECT_ID</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>location</span><span class=o>:</span> <span class=nx>LOCATION</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>templatePath</span><span class=o>:</span> <span class=nx>TEMPLATE_PATH</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>gcsPath</span><span class=o>:</span> <span class=sb>`gs://</span><span class=si>${</span><span class=nx>BUCKET_NAME</span><span class=si>}</span><span class=sb>/</span><span class=si>${</span><span class=nx>file</span><span class=si>}</span><span class=sb>`</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>request</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>projectId</span><span class=o>:</span> <span class=nx>metadata</span><span class=p>.</span><span class=nx>projectId</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>location</span><span class=o>:</span> <span class=nx>metadata</span><span class=p>.</span><span class=nx>location</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>gcsPath</span><span class=o>:</span> <span class=nx>metadata</span><span class=p>.</span><span class=nx>templatePath</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>requestBody</span><span class=o>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>parameters</span><span class=o>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>inputFile</span><span class=o>:</span> <span class=nx>metadata</span><span class=p>.</span><span class=nx>gcsPath</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>try</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>dataflow</span><span class=p>.</span><span class=nx>projects</span><span class=p>.</span><span class=nx>locations</span><span class=p>.</span><span class=nx>templates</span><span class=p>.</span><span class=nx>launch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=nx>request</span>
</span></span><span class=line><span class=cl>    <span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=sb>`Job launched successfully: </span><span class=si>${</span><span class=nx>response</span><span class=p>.</span><span class=nx>data</span><span class=p>.</span><span class=nx>job</span><span class=p>.</span><span class=nx>id</span><span class=si>}</span><span class=sb>`</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> <span class=k>catch</span> <span class=p>(</span><span class=nx>err</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>console</span><span class=p>.</span><span class=nx>error</span><span class=p>(</span><span class=sb>`Error launching job: </span><span class=si>${</span><span class=nx>err</span><span class=si>}</span><span class=sb>`</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><h3 id=monitoring-and-logging-1>Monitoring and Logging<a hidden class=anchor aria-hidden=true href=#monitoring-and-logging-1>#</a></h3><h4 id=monitoring-dag-runs>Monitoring DAG Runs<a hidden class=anchor aria-hidden=true href=#monitoring-dag-runs>#</a></h4><ul><li><strong>Check status</strong>: Monitor DAGs for success, running, or failure states and troubleshoot accordingly.</li><li><strong>Logs</strong>: Available for each task and overall workflow.</li></ul><h4 id=google-cloud-logs>Google Cloud Logs<a hidden class=anchor aria-hidden=true href=#google-cloud-logs>#</a></h4><ul><li><strong>Usage</strong>: Diagnose errors with Cloud Functions and other services.</li></ul><h4 id=troubleshooting-example>Troubleshooting Example<a hidden class=anchor aria-hidden=true href=#troubleshooting-example>#</a></h4><ul><li><strong>Error Detection</strong>: Use logs to identify and correct issues (e.g., missing output bucket).</li><li><strong>Case Sensitivity</strong>: Ensure correct naming in Cloud Functions.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://raghu-vijaykumar.github.io/blog/tags/data-engineering/>Data-Engineering</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/batch-pipelines/>Batch-Pipelines</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/google-cloud/>Google-Cloud</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/course-summary/>Course-Summary</a></li></ul><nav class=paginav><a class=next href=https://raghu-vijaykumar.github.io/blog/posts/architecture/how-to-ha-and-dr/><span class=title>Next »</span><br><span>Ensuring Resiliency: High Availability and Disaster Recovery Strategies</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://raghu-vijaykumar.github.io/blog/>Raghu Vijaykumar</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>