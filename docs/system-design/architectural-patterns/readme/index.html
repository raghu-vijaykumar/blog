<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Architectural Patterns | Raghu Vijaykumar</title>
<meta name=keywords content="system-design,software-architecture,patterns"><meta name=description content="Architectural Building Blocks API Gateway Overview An API Gateway is an API management service that acts as an intermediary between clients and backend services. It uses the API composition pattern to aggregate multiple backend services into a single API endpoint that clients interact with. This abstraction provides several key benefits:
Benefits of an API Gateway Seamless Internal Changes:
Facilitates internal system changes without impacting external API consumers. For example, it enables the splitting of a frontend service into different services for various devices without altering the external API."><meta name=author content="Me"><link rel=canonical href=https://raghu-vijaykumar.github.io/blog/docs/system-design/architectural-patterns/readme/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.e7c811b1152f0ea0017b0724f2c040700cf8bf84343fd4fd5eca45af82337db9.css integrity="sha256-58gRsRUvDqABewck8sBAcAz4v4Q0P9T9XspFr4Izfbk=" rel="preload stylesheet" as=style><link rel=icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://raghu-vijaykumar.github.io/blog/docs/system-design/architectural-patterns/readme/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Architectural Patterns"><meta property="og:description" content="Architectural Building Blocks API Gateway Overview An API Gateway is an API management service that acts as an intermediary between clients and backend services. It uses the API composition pattern to aggregate multiple backend services into a single API endpoint that clients interact with. This abstraction provides several key benefits:
Benefits of an API Gateway Seamless Internal Changes:
Facilitates internal system changes without impacting external API consumers. For example, it enables the splitting of a frontend service into different services for various devices without altering the external API."><meta property="og:type" content="article"><meta property="og:url" content="https://raghu-vijaykumar.github.io/blog/docs/system-design/architectural-patterns/readme/"><meta property="og:image" content="https://raghu-vijaykumar.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="docs"><meta property="article:published_time" content="2024-08-24T05:54:49+00:00"><meta property="article:modified_time" content="2024-08-24T05:54:49+00:00"><meta property="og:site_name" content="Raghu Vijaykumar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raghu-vijaykumar.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Architectural Patterns"><meta name=twitter:description content="Architectural Building Blocks API Gateway Overview An API Gateway is an API management service that acts as an intermediary between clients and backend services. It uses the API composition pattern to aggregate multiple backend services into a single API endpoint that clients interact with. This abstraction provides several key benefits:
Benefits of an API Gateway Seamless Internal Changes:
Facilitates internal system changes without impacting external API consumers. For example, it enables the splitting of a frontend service into different services for various devices without altering the external API."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Docs","item":"https://raghu-vijaykumar.github.io/blog/docs/"},{"@type":"ListItem","position":2,"name":"Architectural Patterns","item":"https://raghu-vijaykumar.github.io/blog/docs/system-design/architectural-patterns/readme/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Architectural Patterns","name":"Architectural Patterns","description":"Architectural Building Blocks API Gateway Overview An API Gateway is an API management service that acts as an intermediary between clients and backend services. It uses the API composition pattern to aggregate multiple backend services into a single API endpoint that clients interact with. This abstraction provides several key benefits:\nBenefits of an API Gateway Seamless Internal Changes:\nFacilitates internal system changes without impacting external API consumers. For example, it enables the splitting of a frontend service into different services for various devices without altering the external API.","keywords":["system-design","software-architecture","patterns"],"articleBody":"Architectural Building Blocks API Gateway Overview An API Gateway is an API management service that acts as an intermediary between clients and backend services. It uses the API composition pattern to aggregate multiple backend services into a single API endpoint that clients interact with. This abstraction provides several key benefits:\nBenefits of an API Gateway Seamless Internal Changes:\nFacilitates internal system changes without impacting external API consumers. For example, it enables the splitting of a frontend service into different services for various devices without altering the external API. Consolidated Security:\nCentralizes security, authentication, and authorization. The API Gateway can handle SSL termination, enforce rate limits, and control access, ensuring secure interactions and protecting against malicious requests. Improved Performance:\nRequest Routing: Routes client requests to the appropriate backend services and aggregates responses into a single response, reducing the number of client-side calls. Caching: Stores responses to common requests, improving response times by serving cached data instead of querying backend services repeatedly. Enhanced Monitoring and Alerting:\nProvides real-time visibility into system traffic patterns and load. Enables monitoring and alerting for traffic anomalies, helping in system observability and proactive issue management. Protocol Translation:\nHandles protocol and format translation between clients and backend services. Supports integration with various external systems by converting different protocols and data formats as needed. Best Practices and Anti-Patterns Avoid Business Logic:\nThe API Gateway should not include business logic. Its primary functions should be API composition and request routing. Adding business logic can lead to a monolithic architecture and undermine the benefits of service decomposition. Single Point of Failure:\nAn API Gateway can become a single point of failure. To address this, deploy multiple instances behind a load balancer and ensure robust release management to prevent service disruptions. Performance Overhead:\nWhile an API Gateway introduces some performance overhead, the overall benefits generally outweigh this. Avoid bypassing or over-optimizing the API Gateway, as this can reintroduce tight coupling and complicate client interactions. API Gateway Solutions \u0026 Cloud Technologies Open Source API Gateways Netflix Zuul Description: Zuul is a free and open-source application gateway written in Java. Capabilities: Provides dynamic routing, monitoring, resiliency, security, and more. Cloud-Based API Gateways Amazon API Gateway Description: A fully managed service for creating, publishing, maintaining, monitoring, and securing APIs at any scale. Supports: RESTful APIs and WebSocket APIs (bi-directional communication between client and server). Google Cloud Platform API Gateway Description: Enables secure access to services through a well-defined REST API consistent across all services. Microsoft Azure API Management Description: Helps organizations publish APIs to external, partner, and internal developers to unlock the potential of their data and services. Load Balancer A load balancer is a fundamental building block in software architecture, especially for large-scale systems. Its primary role is to distribute incoming network traffic across multiple servers, ensuring no single server is overwhelmed. This distribution helps achieve high availability and horizontal scalability by running multiple instances of an application on different servers.\nMotivation for Using Load Balancers Without a load balancer, a client application would need to know the addresses and number of server instances directly. This tight coupling makes it challenging to modify the system’s internal structure without affecting the client application. Load balancers provide an abstraction layer, making the entire system appear as a single server with immense computing power and memory.\nQuality Attributes Provided by Load Balancers High Scalability:\nAllows for scaling the system horizontally by adding or removing servers based on demand. In cloud environments, this can be automated with policies that react to metrics like request rate and bandwidth usage. High Availability:\nLoad balancers can monitor server health and route traffic only to healthy servers, ensuring continuous availability even if some servers fail. Performance:\nWhile load balancers may introduce minimal latency, they enable higher throughput by distributing requests across multiple servers. Maintainability:\nFacilitates rolling updates and maintenance by allowing servers to be taken offline for upgrades without disrupting the overall system. Types of Load Balancers DNS Load Balancing: Uses DNS to map a domain name to multiple IP addresses. The list of addresses is rotated, balancing the load. However, this method lacks health checks and only supports simple round-robin strategies, making it less reliable and secure. Hardware Load Balancers:\nDedicated devices optimized for load balancing tasks. They offer features like health checks, intelligent traffic distribution, and can secure the system by hiding internal server details. Software Load Balancers:\nPrograms running on general-purpose hardware. They provide similar features to hardware load balancers but can be more flexible and cost-effective. Global Server Load Balancer (GSLB):\nCombines DNS and load balancer functionalities, intelligently routing users based on location, server load, response time, and more. GSLBs are essential for multi-data center deployments and disaster recovery scenarios. Load Balancing Solutions \u0026 Cloud Technologies Open Source Software Load Balancing Solutions HAProxy HAProxy is a free and open-source, reliable, high-performance TCP/HTTP load balancer. It is particularly well-suited for high-traffic websites and powers many of the world’s most visited ones. HAProxy is considered the de-facto standard open-source load balancer and is included with most mainstream Linux distributions. It supports most Unix-style operating systems.\nNGINX NGINX is a free, open-source, high-performance HTTP server and reverse proxy (load balancer). Known for its performance, stability, rich feature set, and simple configuration, NGINX is a popular choice for many applications.\nCloud-Based Load Balancing Solutions AWS - Elastic Load Balancing (ELB) Amazon ELB is a highly scalable load balancing solution designed for use with AWS services. It operates in four modes:\nApplication (Layer 7) Load Balancer: Ideal for advanced load balancing of HTTP and HTTPS traffic. Network (Layer 4) Load Balancer: Ideal for load balancing TCP and UDP traffic. Gateway Load Balancer: Ideal for deploying, scaling, and managing third-party virtual appliances. Classic Load Balancer (Layer 4 and 7): Ideal for routing traffic to EC2 instances. GCP - Cloud Load Balancing Google Cloud Platform Load Balancer is a scalable and robust load-balancing solution. It allows you to put your resources behind a single IP address that is either externally accessible or internal to your Virtual Private Cloud (VPC) network. Available load balancer types include:\nExternal HTTP(S) Load Balancer: Externally facing HTTP(s) (Layer 7) load balancer. Internal HTTP(S) Load Balancer: Internal Layer 7 load balancer. External TCP/UDP Network Load Balancer: Externally facing TCP/UDP (Layer 4) load balancer. Internal TCP/UDP Load Balancer: Internally facing TCP/UDP (Layer 4) load balancer. Microsoft Azure Load Balancer Microsoft Azure provides three types of load balancers:\nStandard Load Balancer: Public and internal Layer 4 load balancer. Gateway Load Balancer: High performance and high availability load balancer for third-party Network Virtual Appliances. Basic Load Balancer: Ideal for small-scale applications. GSLB Solutions Amazon Route 53 Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service.\nAWS Global Accelerator AWS Global Accelerator is a networking service that improves the availability, performance, and security of public applications.\nGoogle Cloud Platform Load Balancer \u0026 Cloud DNS Google Cloud Platform offers reliable, resilient, low-latency DNS services from its worldwide network, with comprehensive domain registration, management, and serving capabilities.\nAzure Traffic Manager Azure Traffic Manager provides DNS-based load balancing.\nMessage Brokers: The Building Block for Asynchronous Architectures A message broker is a software component that uses the queue data structure to store and manage messages between senders and receivers. Unlike load balancers, which manage external traffic and are visible to clients, message brokers operate internally within a system and are not exposed externally.\nSynchronous Communication Drawbacks In synchronous communication, both the sender and receiver must be active and maintain an open connection for the transaction to complete. This can lead to several issues:\nConnection Dependence: Both services must be healthy and running simultaneously. Long Processing Times: Services that take a long time to process requests can cause delays and hold up the entire system. Traffic Handling: Synchronous communication lacks the ability to handle sudden increases in traffic effectively. Example: In a ticket reservation system, a frontend service must wait for the backend service to complete several operations before providing a response to the user. This creates a delay and can be problematic if the backend service crashes or if there’s a sudden spike in requests.\nBenefits of Message Brokers Decoupling and Asynchronous Processing Message brokers allow services to communicate without requiring them to be available simultaneously. For instance:\nAsynchronous Responses: In the ticket reservation system, the user receives an immediate acknowledgment and the system processes the ticket reservation and payment in the background. Service Decoupling: Services can be broken down into smaller components, each handling a part of the transaction, and communicate through the message broker. Buffering and Handling Traffic Spikes Message brokers can queue messages, helping absorb traffic spikes. For example:\nOrder Fulfillment: In an online store, orders can be queued during high traffic periods and processed sequentially when the load decreases. Publish-Subscribe Pattern Message brokers support the publish-subscribe pattern where:\nMultiple Subscribers: Different services can subscribe to the same channel to receive notifications or updates. Flexible Integration: New services can be added without altering existing systems, such as adding analytics or notification services. Quality Attributes of Message Brokers Fault Tolerance: Message brokers enhance fault tolerance by allowing services to communicate even if some are temporarily unavailable. Message Reliability: They prevent message loss, contributing to higher system availability. Scalability: They help the system scale to handle high traffic by buffering messages. Performance Considerations While message brokers provide superior availability and scalability, they introduce some latency due to the indirection involved. However, this performance penalty is generally minimal for most systems.\nMessage Brokers Solutions \u0026 Cloud Technologies Open Source Message Brokers Apache Kafka: The most popular open-source message broker today. Apache Kafka is a distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\nRabbitMQ: A widely deployed open-source message broker used globally by both small startups and large enterprises.\nCloud-Based Message Brokers Amazon Simple Queue Service (SQS): A fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\nGCP Pub/Sub and Cloud Tasks: Publisher/Subscriber and message queue solutions offered by Google Cloud Platform.\nMicrosoft Azure:\nService Bus: A fully managed enterprise message broker with message queues and publish-subscribe topics. Event Hubs: A fully managed real-time data ingestion service that allows streaming millions of events per second from any source. Integrates seamlessly with Apache Kafka clients without code changes. Ideal for Big Data. Event Grid: A reliable, serverless event delivery system at a massive scale. Uses the publish-subscribe model and is dynamically scalable. It offers a low-cost pay-as-you-go model and guarantees “at least once delivery of an event.” Content Delivery Networks (CDNs) Problem Addressed by CDNs Even with distributed web hosting and technologies like Global Server Load Balancing, significant latency remains due to the physical distance between users and hosting servers, as well as the multiple network hops between routers.\nExample of Latency Without CDN User Location: Brazil Server Location: East Coast, USA Initial Latency: 200 milliseconds TCP Connection Latency: 600 milliseconds (3-way handshake) HTTP Request Latency: 400 milliseconds Asset Loading Latency: 2,000 milliseconds Total Latency: Over 3 seconds Introduction to CDNs Definition: A Content Delivery Network (CDN) is a globally distributed network of servers designed to speed up content delivery to end users. Purpose: Reduces latency by caching content on edge servers closer to users. Usage: Delivers webpage content, assets (images, text, CSS, JavaScript), and video streams. Benefits of CDNs Faster Page Loads: Reduces total latency to under one second by serving cached content from edge servers. Improved Availability: Content is distributed, reducing the impact of server issues. Enhanced Security: Protects against DDoS attacks by distributing traffic across multiple servers. CDN Caching Strategies Pull Strategy How It Works: CDN caches content on first request, subsequent requests are served from the cache. Advantages: Lower maintenance, CDN manages cache updates. Drawbacks: Initial latency for uncached assets, potential traffic spikes when assets expire simultaneously. Push Strategy How It Works: Content is manually or automatically uploaded to the CDN. Updates require re-publishing. Advantages: Reduces traffic to the origin server, maintains high availability even if the origin server is down. Drawbacks: Requires active management to update content, risk of serving outdated content if not updated. CDN Solutions \u0026 Cloud Technologies Cloudflare Description: Offers ultra-fast static and dynamic content delivery over a global edge network. Benefits: Reduces bandwidth costs and provides built-in unmetered DDoS protection. Fastly Description: Deliver@Edge is a modern, efficient, and highly configurable CDN. Benefits: Provides control over content caching to deliver user-requested content quickly. Akamai Description: Offers a variety of services including API Acceleration, Global Traffic Management, Image \u0026 Video Management, and Media Delivery. Amazon CloudFront Description: A high-performance CDN service built for security and developer convenience. Use Cases: Delivers fast, secure websites, accelerates dynamic content and APIs, supports live streaming, and video-on-demand. Google Cloud Platform CDN Description: Provides fast, reliable web and video content delivery with global scale and reach. Microsoft Azure Content Delivery Network Description: Offers global coverage, full integration with Azure services, and a simple setup. Scalability Patterns Load Balancing in Scalable System Architectures Load balancing is a software architecture pattern used to distribute incoming requests across multiple servers, allowing systems to scale efficiently and maintain performance under high traffic conditions. Single cloud servers are insufficient for handling high volumes of requests, leading to crashes or performance issues. Upgrading servers only postpones the problem. A dispatcher routes incoming requests to available worker servers, enabling load distribution and scalability.\nUse Cases HTTP Requests: Distributes front-end requests (web/mobile) to back-end servers. Microservices Architecture: Manages service instances, enabling independent scaling for each service. Implementation Methods Cloud Load Balancing Services: Managed services that route requests and can scale automatically. Avoid becoming a single point of failure. Message Brokers: Used for asynchronous, one-directional communication between services. Useful for internal load balancing of message queues. Routing Algorithms Round Robin: Sequentially distributes requests, suitable for stateless applications. Example: Most of the stateless API’s Sticky Sessions: Routes requests from the same client to the same server, ideal for stateful applications. Example: Banking, Financial transactions, Multipart file upload. Least Connection: Directs requests to servers with the fewest active connections, suitable for long-term connections. Example: SQL, LDAP. Auto Scaling Integration Auto Scaling: Automatically adjusts the number of servers based on metrics like CPU usage and traffic. Works in conjunction with load balancing to optimize resource use and cost. Example of Auto Scaling Integration Scenario: A cloud-based e-commerce platform experiences variable traffic, with peaks during sales events and lower traffic during off-peak hours.\nAuto Scaling Integration Steps:\nMonitoring and Metrics Collection:\nServer instances in the cloud environment run monitoring agents to collect metrics such as CPU utilization, memory usage, and network traffic. Defining Auto Scaling Policies:\nPolicies are established based on the collected metrics. For example: Scale Up Policy: Add more server instances if the average CPU utilization exceeds 70% for 5 consecutive minutes. Scale Down Policy: Remove server instances if the average CPU utilization drops below 30% for 10 consecutive minutes. Load Balancer Coordination:\nThe load balancer is configured to recognize the dynamic pool of server instances. It automatically adjusts the routing of incoming requests based on the current set of available instances. Implementation Example:\nCloud Load Balancer: Utilizes a cloud provider’s load balancing service to distribute incoming traffic across a pool of identical web server instances. Auto Scaling Group: The web server instances are managed as an auto-scaling group within the cloud provider’s infrastructure. Elasticity in Action: During a flash sale, the traffic spikes, causing the average CPU utilization to rise above 70%. The auto-scaling policy triggers, launching additional server instances to handle the increased load. The load balancer detects the new instances and routes traffic to them, balancing the load. After the sale, as traffic decreases, the average CPU utilization falls below 30%. The auto-scaling policy triggers the removal of excess instances, reducing costs. Benefits:\nDynamic Scalability: Automatically adapts to traffic changes, ensuring optimal performance. Cost Efficiency: Reduces infrastructure costs by scaling down during low-traffic periods. Resilience: Prevents server overload and potential crashes by distributing the load and adding capacity when necessary. This example illustrates how auto scaling, combined with a load balancer, efficiently manages variable workloads in a cloud environment, providing both scalability and cost-effectiveness.\nReferences\nGoogle Compute Engine Autoscaling Groups GKE Cluster Autoscaling Amazon EC2 Autoscaling Considerations Session Management: Choosing the right routing algorithm depends on whether the application is stateless or stateful. Scalability and Cost Efficiency: Combining load balancing with auto scaling helps in dynamically resizing the backend infrastructure, ensuring optimal resource utilization and cost savings. This documentation outlines the essential aspects of implementing and managing load balancing in scalable system architectures, highlighting methods, algorithms, and practical considerations for effective deployment.\nPipes and Filters Architecture Pattern Overview Analogy: Data flows like water through a series of pipes and filters. Components: Data Source: Origin of the data, e.g., backend services, sensors. Filters: Isolated software components that process data. Pipes: Mechanisms like distributed queues or message brokers that connect filters. Data Sink: Final destination for processed data, e.g., databases, external services. Key Concepts Filters: Perform single operations, unaware of the rest of the pipeline. Pipes: Can store data temporarily or use message systems for notifications. Benefits Decoupling: Allows different processing tasks to use different programming languages and technologies. Hardware Optimization: Each task can run on the most suitable hardware (e.g., specialized hardware for machine learning). Scalability: Each filter can be scaled independently based on workload needs. Real-World Use Cases Digital Advertising: Processing streams of user activity data. Internet of Things (IoT): Data processing from end devices. Media Processing: Video and audio processing pipelines, including chunking, thumbnail creation, resolution resizing, adaptive streaming, and captioning. Example: Video Sharing Service Video Processing Pipeline: Chunking: Split video into smaller chunks. Thumbnail Extraction: Select frames as thumbnails. Resolution and Bitrate Adjustment: Resize chunks for adaptive streaming. Encoding: Encode chunks into different formats. Audio Processing Pipeline: Transcription: Convert speech to text. Captioning and Translation: Provide captions and translate into different languages. Content Moderation: Detect copyrighted or inappropriate content. Considerations Complexity: Maintaining this architecture can be complex, especially with granular filters. Stateless Filters: Each filter should be independent and stateless. Transaction Handling: Not suitable for scenarios requiring a single transaction across the entire pipeline. The pipes and filters pattern is valuable for scenarios needing flexible, scalable, and decoupled processing pipelines but may not be ideal for transactional data processing.\nScatter-Gather Architecture Pattern Overview Components: Sender/Requester: Initiates the request. Workers: Respond to the request; can be internal or external services. Dispatcher: Distributes the request to all workers and collects responses. Aggregator: Combines responses from workers into a single response. Key Concepts Parallel Processing: Requests are sent to all workers simultaneously, allowing for parallel processing. Workers can be diverse, performing different functions or accessing different data. Use Cases Search Services: Users send a query, and internal workers search through various data subsets. Results are aggregated and returned as a ranked list. Hospitality Services: A request for hotel availability is sent to multiple hotels. Responses are collected and sorted based on criteria like price or rating. Considerations Timeouts: Set an upper limit for waiting for worker responses to avoid delays due to unresponsive workers. Decoupling: Use a message broker to decouple the dispatcher and workers, facilitating asynchronous communication. Long-running Tasks: For tasks requiring extensive processing, separate the dispatcher and aggregator. Use unique identifiers for tracking and retrieving results. Example Workflow Immediate Response Use Case:\nDispatcher sends a request to workers. Workers process and return responses quickly. Aggregator compiles the results and sends them to the user. Long-running Task Use Case:\nDispatcher assigns a unique ID to the request and sends it to workers. Workers return partial results with the same ID. Aggregator stores and compiles results, accessible via the unique ID. Benefits Scalability: Supports high scalability by enabling parallel processing across numerous workers. Flexibility: Can integrate diverse internal and external services. Resilience: Can continue processing even if some workers are unavailable or slow. The scatter-gather pattern is versatile and widely used in many production systems, providing efficient parallel processing and aggregation of results.\nExecution Orchestrator Pattern Overview Purpose: Manages a sequence of operations across multiple services in microservices architecture. Analogy: Like a conductor in an orchestra, the orchestrator directs services without performing the business logic itself. Key Concepts Execution Orchestrator: A centralized service that: Calls different services in the right order. Handles exceptions and retries. Maintains the state of the flow until completion. Microservices: Individual services responsible for specific business logic, typically stateless and independently scalable. Use Case Example: Video on Demand Service User Registration Flow: User fills out a registration form (username, password, payment). Orchestrator Service handles: User Service: Validates username and password. Payment Service: Authorizes credit card via a third-party API. Location Service: Registers user location for content access. Recommendation Service: Sets up user preferences. Email Service: Sends a confirmation email with details. Considerations Scalability: Orchestrator and microservices can be scaled independently. Orchestrator can be deployed across multiple instances for reliability. Failure and Recovery: Orchestrator handles errors, retries, and manages the flow’s state. Maintains a database to persist the state for recovery in case of failures. Avoiding Monolithic Tendencies: Keep the orchestrator focused on coordination, not business logic. Benefits Decoupling: Microservices operate independently and are unaware of the orchestration. Efficiency: Supports parallel and sequential operations. Flexibility: Easy to modify the flow by updating the orchestrator. This pattern is particularly useful in complex systems requiring coordination of multiple independent services, offering a scalable and maintainable solution for executing business logic workflows.\nChoreography Pattern Overview Purpose: Helps scale complex flows of business transactions in microservices architecture. Comparison: Unlike the orchestration pattern, choreography uses asynchronous events without a central orchestrator. Key Concepts Microservices: Decoupled services that communicate through a message broker. Message Broker: A distributed message queue that stores and distributes events. Advantages Loose Coupling: Services operate independently and are not tightly coupled through a central orchestrator. Scalability: Easy to add or remove services and scale operations. Cost Efficiency: Services can be implemented as functions that only run when triggered, saving resources. Example: Job Search Service User Registration: User submits a form with their details and resume. Candidate Service: Stores data and emits an event. Email Confirmation: Triggered by the event, an email confirmation is sent to the user. Skills Parsing: Skills Parser Service: Extracts and stores skills data, then emits an event. Job Search: Job Search Service: Searches for job matches and emits results as an event. Job Notifications: Candidate Service: Updates user records. Email Service: Sends job notifications based on user preferences. Considerations Debugging Challenges: Troubleshooting issues can be difficult due to lack of a central coordinator. Harder to trace the flow of events and identify issues. Testing Complexity: Requires complex integration tests to catch issues before production. Becomes more challenging as the number of services grows. Performance Patterns MapReduce Pattern Overview Origin: Introduced by Jeff Dean and S.J. Ghemawat from Google in 2004. Purpose: Simplifies processing of large data sets by distributing computation across many machines. Key Concepts Map Function: Transforms input key-value pairs into intermediate key-value pairs. Reduce Function: Aggregates and processes intermediate pairs to produce final output. Example: Word Count in Text Files Input: Key-value pairs (filename, content). Map Function: Emits (word, 1) for each word in the content. Shuffle and Sort: Groups intermediate pairs by word. Reduce Function: Sums the counts for each word. Architecture TODO: Add backup master and snapshot storage in the image\nMaster Node: Orchestrates the computation, schedules tasks, and handles failures. Worker Nodes: Execute map and reduce tasks in parallel. Data Distribution: Input data is split into chunks for parallel processing. Fault Tolerance Worker Failures: Master reassigns tasks and notifies reduce workers of new data locations. Master Failures: Can either restart the process or use a backup master to continue. Cloud Integration Scalability: Cloud environments provide access to many machines, enabling large-scale data processing. Cost Efficiency: MapReduce’s batch processing nature means paying only for resources used during the job, not for maintaining idle machines. Saga Pattern Introduction Context: Microservices architecture, where each service has its own database. Problem: Ensuring data consistency across multiple databases without a central database, losing traditional ACID transactions. Solution: Saga Pattern Definition: Manages data consistency in distributed transactions by breaking them into a series of local transactions. If an operation fails, compensating transactions are executed to roll back. Implementation Methods Execution Orchestrator Pattern:\nA central orchestrator service manages the transaction flow, calling services sequentially. Decides whether to continue or rollback based on service responses. Choreography Pattern:\nNo central orchestrator; services communicate through a message broker. Each service listens for events and triggers subsequent steps or compensations as needed. Example Scenario: Ticket Reservation System Services Involved: Order, Security, Billing, Reservation, Email, (Orchestration if using orchestrator pattern).\nProcess:\nOrder Service: Registers an order and sets it to pending. Security Service: Validates the user (not a bot or blacklisted). Billing Service: Authorizes the pending transaction on the user’s credit card. Reservation Service: Reserves the ticket for the user. Email Service: Sends a confirmation email. Failure Handling:\nIf any service returns a failure, compensating operations are triggered: Cancel pending transactions. Remove records from databases. Compensating operations ensure consistency by undoing previous steps if necessary. Conclusion Purpose: Maintains data consistency in a microservices environment by handling distributed transactions. Flexibility: Can be implemented using either the execution orchestrator pattern or the choreography pattern. Resilience: Provides mechanisms to complete transactions or roll back in case of failures. The saga pattern is crucial for ensuring reliable operations and consistency in complex systems with distributed architectures.\nTransactional Outbox Pattern Overview Problem: In event-driven architectures, ensuring that a database update and an event publication occur together reliably can be challenging. Specifically, there’s a risk of losing events or data if a system crash occurs between these operations. Solution: Transactional Outbox Pattern Concept: Involves adding an Outbox Table to the database to store messages intended for the message broker. Updates to both the business logic table (e.g., users) and the Outbox Table are performed within a single database transaction. Implementation Steps Database Update and Message Logging:\nInstead of sending an event directly to the message broker, the service logs the message to the Outbox Table along with updating the primary business logic table. Ensures atomicity: Both tables are updated together, or neither is, preventing partial updates. Message Sender/Relay Service:\nA separate service monitors the Outbox Table for new messages. Upon finding a new message, it sends it to the message broker and marks it as sent (or deletes it). Addressing Potential Issues Duplicate Events:\nCause: A crash between sending a message and marking it as sent can result in duplicate events. Solution: Implement “at least once” delivery semantics. Each message gets a unique ID. Consumers track processed message IDs to discard duplicates. Lack of Transaction Support:\nScenario: Some databases, especially NoSQL ones, may not support multi-collection transactions. Solution: Embed the Outbox message directly within the same document (or object) in the database. The sender service queries for documents with messages, sends them, and then clears the messages. Ordering of Events:\nProblem: Ensuring the order of related events, such as a user signup followed by a cancellation. Solution: Assign each message a sequential ID. The sender service can then sort and send messages in the correct order based on these IDs. Conclusion Benefits: The transactional outbox pattern provides a robust solution for ensuring data consistency and reliable event publication in distributed systems. Considerations: Addressing potential issues like duplicate events, lack of transaction support, and event ordering is crucial for effective implementation. The transactional outbox pattern is an essential tool for maintaining consistency in microservices architectures, particularly in systems that rely heavily on event-driven communication.\nMaterialized View Pattern Overview Purpose: To optimize performance and cost efficiency in data-intensive applications by pre-computing and storing query results. Problem Statement Performance: Complex queries, especially those involving multiple tables or databases, can be slow. Cost: Repeatedly running the same complex queries can be resource-intensive, leading to high costs in a cloud environment. Solution: Materialized View Concept: Create a read-only table (materialized view) that stores the pre-computed results of a specific query. This allows for quick data retrieval without recalculating the query each time. Implementation Data Storage:\nStore materialized views as separate tables in the same database or in a read-optimized separate database. In cases of frequent data updates, the materialized view can be regenerated immediately or on a fixed schedule. Example Use Case:\nScenario: An online education platform with tables for courses and reviews. Need: Display top courses based on average ratings for a specific topic. Solution: Create a materialized view storing pre-computed course ratings, which can be filtered quickly based on the topic. Benefits:\nPerformance: Significantly reduces query time by avoiding complex aggregations and joins. Cost Efficiency: Saves resources by reducing the need for repeated complex query execution. Considerations Storage Space: Materialized views require additional storage, increasing costs. The trade-off between performance and space must be evaluated. Update Frequency: Same Database: If the database supports materialized views, updates can be automatic and efficient. External Storage: Requires manual or programmatic updates, which can add complexity. Conclusion The materialized view pattern is a powerful tool for optimizing the performance of data-intensive applications. By pre-computing and storing query results, this pattern improves user experience and reduces operational costs, especially in environments where resource usage incurs significant expenses. CQRS (Command and Query Responsibility Segregation) Overview Purpose: To separate the command (write) and query (read) responsibilities in a system into distinct services and databases, optimizing each for its specific workload. Key Concepts Command Operations: Actions that mutate data, such as insertions, updates, and deletions. Query Operations: Actions that read and return data without altering it. Benefits of CQRS Separation of Concerns: Command service handles business logic, validations, and data mutations. Query service focuses solely on data retrieval and presentation. Independent Scalability: The number of instances for each service can be scaled independently based on demand. Optimized Data Models: Command database can be optimized for write operations. Query database can be optimized for read operations, often using different database technologies. Example Use Case: Online Store Command Side: Stores user reviews in a relational database. Ensures business logic, such as validating purchases and review content. Query Side: Stores reviews and average ratings in a NoSQL database, optimized for quick retrieval. Contains pre-aggregated data to minimize real-time computations. Synchronization Event Publishing: On data mutation, the command service publishes events to keep the query database updated. Can use message brokers or functions-as-a-service for event handling and data synchronization. Eventual Consistency: The system guarantees eventual consistency between command and query databases but not strict consistency. Considerations Complexity: Requires managing multiple services, databases, and synchronization mechanisms. Adds overhead but provides significant performance benefits. Eventual Consistency: Suitable for scenarios where eventual consistency is acceptable, not for strict consistency requirements. Combining CQRS and Materialized View Patterns in Microservices Overview Problem: In a microservices architecture, data is often split across multiple services and databases, making it challenging to aggregate data efficiently for queries. Solution: Use a combination of CQRS and materialized view patterns to optimize data retrieval and maintain synchronization across services. Key Concepts Microservices Split:\nInitially, all data might reside in a single database. Splitting into microservices involves dividing this data into separate databases, each handled by a specific service. This division prevents traditional join operations across different microservices’ databases. Performance Challenges:\nAggregating data across multiple services requires API calls, database queries, and programmatic joins, leading to significant performance overhead. Solution: Combining Patterns CQRS (Command and Query Responsibility Segregation):\nCreate a new microservice with a read-optimized database specifically for querying aggregated data. This microservice only handles queries and does not perform data mutations. Materialized View:\nA materialized view is created to pre-aggregate and store relevant data from multiple microservices. The view includes data necessary for user queries, combining information from different microservices. Synchronization Methods Message Broker:\nEach microservice publishes events to a message broker when data changes. The query microservice listens to these events and updates the materialized view accordingly. Cloud Function:\nA function as a service monitors tables in different services’ databases. On detecting changes, the function updates the materialized view in the query database. Real-World Example: Online Education Platform Setup:\nCourses Microservice: Manages course data (name, description, price). Reviews Microservice: Handles reviews and ratings. Course Search Service: New service with a materialized view that includes course details and reviews. Data Flow:\nChanges in the course details or reviews trigger events. The Course Search Service updates its materialized view based on these events, ensuring quick access to aggregated data. Benefits Efficient Data Aggregation: Combines data from multiple microservices into a single, query-optimized view. Performance Optimization: Reduces the need for complex, runtime data joins and minimizes latency. Scalability and Maintainability: Isolates query logic, making it easier to maintain and scale based on query load. Conclusion The combination of CQRS and materialized views effectively addresses the challenges of data aggregation in microservices architectures. This approach optimizes data retrieval, maintains synchronization, and allows for scalable and maintainable systems. Event Sourcing Pattern Overview Event sourcing is an architecture pattern where the state of an application is derived from a sequence of events rather than storing the current state directly.\nTraditional Data Handling CRUD Operations: Applications typically use Create, Read, Update, and Delete operations to manage data, focusing on the current state. Current State: The database stores the current state of entities, and any modifications overwrite previous states. Need for Event Sourcing Limited by Current State: In some cases, knowing only the current state isn’t sufficient. Historical data or the sequence of changes may be crucial. Examples: Banking: Clients need to see transaction histories, not just current balances. E-commerce: Merchants might need to understand inventory changes, not just the current stock level. Event Sourcing Explained Events Over State: Instead of the current state, the system stores events that describe changes or facts about entities. Immutability: Events are immutable; once logged, they cannot be changed. Replaying Events: The current state is derived by replaying all events related to an entity. Benefits of Event Sourcing Complete History: Retains a full history of changes, useful for auditing and troubleshooting. Performance: Improved write performance, as events are appended rather than updating a state. Storing Events Database: Each event can be stored as a record, allowing for complex queries and analytics. Message Broker: Events can be published for consumption by other services, ensuring order and scalability. Challenges and Optimizations Replaying All Events: Reconstructing state by replaying all events can be inefficient. Snapshots: Periodically save the current state to reduce the number of events replayed. CQRS (Command Query Responsibility Segregation): Separate systems for handling commands (writes) and queries (reads). This allows for efficient read operations using a read-optimized database. Combining Event Sourcing and CQRS Eventual Consistency: Combining these patterns often results in eventual consistency, which may be acceptable depending on the use case. Benefits: Auditing and History: Complete record of all changes. Efficient Writes and Reads: Separate systems optimize performance. Big Data Processing and Lambda Architecture Overview In big data processing, two main strategies exist: batch processing and real-time processing. Batch processing allows for deep insights into historical data and the fusion of data from various sources, but it has a higher delay between data collection and availability for querying. Real-time processing provides immediate visibility and response to incoming data but is limited to recent information without deep historical context.\nThe Challenge Deciding between batch and real-time processing can be difficult, as many systems require the benefits of both strategies. For instance, systems that aggregate logs and performance metrics, or those used in ride-sharing services, need both real-time insights for immediate actions and historical analysis for detecting patterns and optimizing performance.\nLambda Architecture The Lambda Architecture addresses this challenge by combining batch and real-time processing, offering the best of both worlds. It consists of three layers:\nBatch Layer:\nManages the master dataset as a system of records, storing immutable data. Pre-computes batch views for in-depth analysis and data corrections. Operates on the entire dataset for perfect accuracy. Speed Layer:\nHandles real-time data processing to provide low-latency views. Processes data as it arrives and updates real-time views. Focuses on recent data without complex corrections or historical context. Serving Layer:\nMerges data from both batch and real-time views. Responds to ad-hoc queries, providing a comprehensive view combining historical and real-time data. Practical Application A practical example of Lambda Architecture is in the ad tech industry, where advertisers and content producers interact through an ad-serving system. The system must process three types of events: ad views, ad clicks, and user purchases. These events are processed by both the batch and speed layers to provide:\nReal-time data, such as the number of users currently viewing ads. Combined data for queries like the total ads shown in the last 24 hours, merging batch and speed layer data. In-depth analysis, like determining the return on investment for advertisers, which requires historical data fusion and analytics. Conclusion Lambda Architecture effectively handles scenarios requiring both real-time and batch processing capabilities. It allows for comprehensive data analysis and immediate responsiveness, making it a versatile solution for modern big data systems.\nSoftware Extensibility Patterns Extensibility Patterns: Sidecar Overview The Sidecar pattern is an extensibility pattern used to extend the functionality of a service without embedding additional logic directly into the service. This approach allows for modular and scalable systems.\nProblem to Solve Additional Functionality Needs: Services often require extra capabilities like logging, monitoring, or configuration management, beyond their core business logic. Challenges: Library Reuse: In a multi-language environment, reusing libraries across services is impractical and can lead to inconsistencies. Separate Services: Deploying shared functionalities as separate services can be excessive and complex. Sidecar Pattern Analogy: Like a sidecar on a motorcycle, this pattern adds extra functionality as a separate process or container alongside the main service. Benefits: Isolation: Provides separation between the core service and the sidecar, reducing potential conflicts. Shared Resources: Both the main service and the sidecar share the same host, allowing fast and reliable communication. Language Independence: Sidecars can be implemented in any language and reused across different services. Simplified Updates: Updates to sidecar functionalities can be rolled out across all services simultaneously. Ambassador Sidecar Function: A specific type of sidecar that acts as a proxy for handling network requests. Advantages: Complexity Offloading: Manages network communication complexities, including retries, authentication, and routing. Simplified Core Service: Keeps the core service focused on business logic, while the ambassador handles network concerns. Distributed Tracing: Enables instrumentation and tracing across services, aiding in troubleshooting and isolating issues. Anti-Corruption Adapter Pattern Introduction The Anti-Corruption Adapter Pattern is a crucial software architecture pattern used to manage interactions between systems with different technologies, protocols, or data models. It prevents the corruption of a new system by the legacy system during integration or migration processes.\nScenarios and Solutions Migration from Monolith to Microservices:\nProblem: During migration from a monolithic system to microservices, new services may need to interact with old technologies, APIs, or data models. This can corrupt the clean design of new services. Solution: Implement an Anti-Corruption Adapter (ACA) service that acts as a mediator. The ACA translates communications, allowing new microservices to interact with the monolithic application using modern technologies, while the monolith continues to operate as before. Coexistence with Legacy Systems:\nProblem: Sometimes, parts of the legacy system cannot be fully migrated or replaced due to various constraints, such as high costs or critical dependencies. Solution: The ACA enables the new system to leverage legacy components without inheriting outdated logic or technologies. This allows the legacy system to remain as-is, while the new system evolves independently. Benefits Isolation: The ACA isolates new and old systems, preventing legacy logic from contaminating new architectures. Seamless Integration: It allows for smooth interaction between systems with different technologies or data models. Gradual Migration: Facilitates gradual migration from old to new systems without disrupting business operations. Challenges Development and Maintenance: The ACA itself is a service that requires development, testing, and maintenance like any other component. Performance Overhead: The translation process can introduce latency and may require scaling to avoid becoming a bottleneck. Cost: In a cloud environment, the ACA can incur additional costs, particularly if run continuously. Deploying it as a Function-as-a-Service (FaaS) can help mitigate these costs if usage is infrequent. Conclusion The Anti-Corruption Adapter Pattern is valuable for maintaining the integrity and cleanliness of new systems while interacting with legacy components. It is particularly useful in scenarios involving system migration or the need for long-term coexistence with legacy systems. However, it comes with trade-offs, including potential latency and additional costs.\nThis pattern helps balance the evolution of technology stacks while minimizing disruption and preserving system integrity.\nBackends for Frontends (BFF) Pattern Introduction The Backends for Frontends (BFF) pattern addresses the challenges of supporting different frontend applications (e.g., web, mobile, desktop) with a single monolithic backend. This pattern involves creating separate backend services, each tailored to the specific needs and features of a particular frontend.\nProblem Statement In a typical e-commerce system with a microservices architecture:\nThe frontend code running in browsers interacts with a backend that serves static and dynamic content. Over time, as the system grows and more frontend types (e.g., mobile, desktop) are introduced, the backend becomes complex, supporting diverse features and device-specific needs. This complexity leads to a monolithic backend that struggles to provide optimal experiences for different devices. Solution: BFF Pattern The BFF pattern proposes creating distinct backend services for each frontend type:\nFrontend-Specific Backends: Each backend service is dedicated to a particular frontend, containing only the relevant functionality. This results in smaller, more manageable codebases and services that can be optimized for specific devices (e.g., mobile vs. desktop). Full Stack Teams: Teams can work as full stack developers, managing both the frontend and the corresponding backend, streamlining the development and deployment process. Benefits Optimized User Experience: Each backend is tailored to the unique capabilities and needs of its corresponding frontend, providing a better user experience. Reduced Complexity: Smaller, frontend-specific backends are easier to manage and evolve. Independent Development: Teams can work independently without depending on a separate backend team, reducing coordination overhead. Challenges Shared Functionality:\nDuplication Risk: There may be shared logic or business rules needed across multiple backends (e.g., authentication, checkout process). Solutions: Shared Libraries: Suitable for small, stable pieces of logic but can lead to tight coupling and maintenance issues. Separate Services: Creating dedicated services for shared functionality with clear APIs and ownership, ensuring consistency without duplication. Granularity Decision:\nDetermining the appropriate level of granularity depends on the uniqueness of the experiences across different platforms. For example, separate backends for Android and iOS are justified if their user experiences are significantly different. Cloud Deployment Considerations:\nIn a cloud environment, smaller and less powerful virtual machines can replace the original monolithic backend. The choice of hardware can be optimized for the specific demands of each frontend (e.g., CPU or memory requirements). Load balancing can be used to route requests to the appropriate backend, using URL paths, parameters, or HTTP headers like the user agent. Conclusion The BFF pattern helps manage the complexity and scalability of systems supporting multiple frontend types by creating dedicated backends. This approach improves user experience, reduces development friction, and allows for independent and efficient development. However, it requires careful management of shared functionality and thoughtful decisions regarding service granularity.\nReliability \u0026 Error Handling Patterns Throttling or Rate Limiting Pattern Introduction The Throttling or Rate Limiting pattern is designed to enhance system reliability by controlling the rate at which resources are consumed. It helps prevent overconsumption of system resources, whether due to malicious activity, legitimate high traffic, or interactions with external services.\nProblem Statement Two main issues this pattern addresses:\nOverconsumption of Resources: High request rates can lead to system overload, potentially causing slowdowns or outages. Unexpected traffic spikes can trigger costly auto-scaling, resulting in financial strain. Overspending on External Services: Systems interacting with external APIs or cloud services can accidentally consume more resources than budgeted, leading to unexpectedly high costs. Types of Throttling Server-Side Throttling:\nLimits the number of requests to protect the system’s backend from overconsumption. Common in scenarios where the system serves multiple clients through an API. Client-Side Throttling:\nPrevents a client from exceeding a predetermined budget when calling external services. Used when a system consumes external APIs or cloud services to avoid overspending. Strategies for Handling Exceeding Limits Dropping Requests:\nRequests exceeding the rate limit are dropped. An error response (e.g., HTTP 429 “Too Many Requests”) can inform the client. Suitable for non-critical services like fetching real-time data (e.g., stock prices). Queuing Requests:\nRequests are queued and processed later when capacity allows. Useful for critical operations like executing trades, where delaying is preferable to dropping. Service Degradation:\nAdjust service quality instead of dropping or delaying requests, such as reducing video quality in streaming services. Can also set an upper limit on resource usage, like the number of media items accessed per day. Considerations for Implementation Global vs. Customer-Based Throttling:\nGlobal Throttling: A single limit applies to all clients, ensuring overall system stability but risking unfair resource allocation. Customer-Based Throttling: Individual limits for each client, ensuring fair resource distribution but complicating total request rate management. External vs. Service-Based Throttling:\nExternal Throttling: Limits based on the overall number of API calls from clients, straightforward but can lead to internal service overload. Service-Based Throttling: Specific limits for internal services, requiring complex tracking but better protecting individual system components. Conclusion Throttling is crucial for maintaining system reliability and controlling costs. The choice between different throttling strategies—global vs. customer-based, external vs. service-based—depends on the specific use case and system requirements. Understanding these considerations helps in implementing an effective throttling strategy that balances performance, cost, and resource allocation.\nRetry Pattern Introduction The Retry Pattern is a reliability architecture pattern used to handle errors by retrying operations that have failed. This pattern helps recover from transient issues in a system, such as network failures or temporary unavailability of services.\nProblem Statement In cloud environments, errors can occur at any time due to software, hardware, or network issues. When a client calls a service, which in turn may call another service, these errors can cause delays, timeouts, or outright failures. The Retry Pattern aims to handle such situations by retrying failed operations.\nKey Considerations Error Categorization:\nUser Errors: Errors caused by invalid user actions (e.g., HTTP 403 Unauthorized). These should not be retried; instead, return the error to the user. System Errors: Internal errors that may be transient and recoverable (e.g., HTTP 503 Service Unavailable). These are candidates for retries. Choosing Errors to Retry:\nOnly retry errors that are likely to be temporary and recoverable, such as timeouts or service unavailability. Delay and Backoff Strategies:\nFixed Delay: A constant delay between retries (e.g., 100 ms). Incremental Delay: Increasing the delay gradually after each retry (e.g., 100 ms, 200 ms, 300 ms). Exponential Backoff: Exponentially increasing delay (e.g., 100 ms, 200 ms, 400 ms, 800 ms) to reduce load on recovering services. Randomization (Jitter):\nAdding randomness to the delay helps spread out retry attempts, reducing the chance of overloading the system simultaneously. Retry Limits and Time Boxing:\nSet limits on the number of retries or a maximum time period for retries to prevent indefinite retry attempts. Idempotency:\nEnsure that retrying an operation does not cause unintended side effects, such as double billing in a payment system. Only idempotent operations should be retried safely. Implementation Strategies:\nShared Library: Implement retry logic in a reusable library or module. Ambassador Pattern: Separate the retry logic from the main application code by running it as a separate process on the same server. Conclusion The Retry Pattern is a simple yet effective method for handling transient errors in a system. However, careful implementation is crucial to avoid pitfalls like retry storms, and to ensure retries are applied only in appropriate situations. Properly applying the Retry Pattern can enhance system reliability and provide a smoother experience for users.\nCircuit Breaker Pattern Introduction The Circuit Breaker Pattern is a software architecture pattern used to handle long-lasting errors and prevent cascading failures. It contrasts with the Retry Pattern, which is used for short, recoverable issues. The Circuit Breaker Pattern prevents requests from being sent to a failing service, thus saving resources and improving system stability.\nReal-Life Example Consider an online dating service that fetches profile images from an image service. If the image service is down for a significant time, retrying requests is futile. Instead, using a circuit breaker can prevent further attempts and conserve resources.\nKey Concepts Circuit States:\nClosed: All requests are allowed through, and the system tracks success and failure rates. Open: No requests are allowed through; failures are assumed to continue. The system immediately returns errors. Half-Open: A limited number of requests are allowed to test if the service has recovered. Operation\nThe circuit starts in a Closed state, allowing requests and tracking failures. If failures exceed a certain threshold, the circuit moves to an Open state, blocking requests. After a timeout, it transitions to a Half-Open state to test the service’s health. Depending on the results, it either closes the circuit or returns to the open state. Implementation Considerations Handling Requests in Open State:\nDrop Requests: Simply ignore them, with proper logging for analysis. Log and Replay: Record the requests for later processing, useful in critical systems like e-commerce. Response Strategy in Open State:\nFail Silently: Return empty responses or placeholders (e.g., a placeholder image in a dating app). Best Effort: Provide cached or old data if available. Separate Circuit Breakers for Each Service:\nEach external service should have its own circuit breaker to prevent one failing service from affecting others. Asynchronous Health Checks:\nInstead of real requests in the Half-Open state, use small, asynchronous health checks to determine service recovery. This approach conserves resources and reduces impact on the recovering service. Implementation Location:\nAs with the Retry Pattern, the Circuit Breaker can be implemented as a shared library or through an Ambassador Sidecar, especially useful for services in different programming languages. Conclusion The Circuit Breaker Pattern is crucial for managing long-lasting errors, preventing resource waste, and improving system resilience. It involves careful consideration of how to handle failed requests, the state management of the circuit, and the method of implementation. Properly implementing this pattern can significantly enhance the stability and reliability of a distributed system.\nDead Letter Queue Pattern Overview The Dead Letter Queue (DLQ) pattern is designed to handle message delivery failures in an event-driven architecture. It helps manage errors in publishing and consuming messages through a message broker or distributed messaging system.\nEvent-Driven Architecture In an event-driven system, three key components are involved:\nEvent Publishers: Produce messages or events. Message Broker: Manages channels, topics, or queues for message distribution. Consumers: Read and process incoming messages. While this architecture offers benefits like decoupling and scalability, it also introduces potential points of failure.\nReal-Life Example Consider an online store with:\nOrder Service: Publishes order events. Inventory, Payment, Fulfillment Services: Subscribe to these events and process them based on the product type (physical vs. digital). Potential issues include:\nOrder service publishing to a non-existent or full queue. Consumers facing issues reading or processing messages, which could clog the queue and delay other messages. Dead Letter Queue (DLQ) Purpose:\nA special queue for messages that cannot be delivered or processed successfully. Helps in isolating problematic messages and preventing them from affecting the main queue. Message Entry into DLQ:\nProgrammatic Publishing: Consumers or publishers manually move messages to the DLQ if they encounter issues. Automated Configuration: Message brokers can be configured to automatically move messages to the DLQ due to delivery failures or other issues. Configuration Support:\nMost open-source or cloud-based message brokers support DLQ functionality. Processing Messages in DLQ:\nMonitoring and Alerting: Regular monitoring ensures that messages in the DLQ are addressed and not forgotten. Error Details: Attach headers or additional information to messages to understand and fix issues. Handling: Automatic Republishing: Once issues are resolved, messages can be moved back to the original queue for normal processing. Manual Processing: Rare cases or special scenarios might require manual intervention. Deployment Patterns Rolling Deployment Overview: The Rolling Deployment pattern is used for upgrading production servers without significant downtime. It involves gradually replacing application instances with a new version while maintaining service availability.\nHow It Works:\nLoad Balancing: Stop sending traffic to one server at a time using a load balancer. Upgrade: Replace the old application instance with the new version on the server. Testing: Optionally run tests on the upgraded server. Reintegration: Add the updated server back into the load balancer’s rotation. Repeat: Continue the process for all servers until all are running the latest version. Benefits:\nNo Downtime: The system remains operational throughout the upgrade. Gradual Release: New versions are released gradually, reducing risk compared to a “big bang” approach. Cost-Effective: No need for additional hardware or infrastructure. Downsides:\nCascading Failures: New versions might cause failures that could impact old servers still in operation. Compatibility Issues: Running two versions side by side may cause issues if the new version is not fully compatible with the old one. Blue Green Deployment Overview: The Blue Green Deployment pattern is used to release a new version of software by maintaining two separate environments—Blue and Green. This approach aims to minimize risks and ensure a smooth transition between versions.\nHow It Works:\nBlue Environment: The old version of the application continues running on this set of servers. Green Environment: A new set of servers is provisioned, and the new version of the application is deployed here. Verification: After deployment, the new instances in the Green environment are tested to ensure they work as expected. Traffic Shift: Traffic is gradually redirected from the Blue environment to the Green environment using a load balancer. Rollback: If issues are detected, traffic can be switched back to the Blue environment. If all is well, the Blue environment can be decommissioned or kept for the next release cycle. Benefits:\nNo Downtime: The Blue environment remains operational during the transition, ensuring continuous service availability. Safe Rollbacks: If problems arise, traffic can be easily shifted back to the old version, minimizing risk. Consistent User Experience: Customers experience only one version of the software at a time, reducing compatibility issues. Downsides:\nIncreased Costs: Running both Blue and Green environments simultaneously means needing twice the server capacity during the release. Resource Usage: Additional servers are required, which can lead to higher operational costs. Canary Release and A/B Testing Canary Release: The Canary Release pattern blends elements from both rolling and blue-green deployment strategies to offer a balanced approach to deploying new software versions.\nHow It Works:\nInitial Deployment: Deploy the new version of the software to a small subset of existing servers (the Canary servers). Traffic Management: Redirect either all or a subset of traffic (e.g., internal users or beta testers) to these Canary servers. Monitoring: Observe the performance and functionality of the Canary servers compared to the rest of the servers running the old version. Rollout Decision: If the Canary version performs well, gradually update the rest of the servers using a rolling release approach. If issues arise, traffic can be shifted back to the old servers. Benefits:\nReduced Risk: Issues affect only a small subset of users, making it easier to manage and rollback if needed. Informed Decisions: Provides confidence in the new version before a full-scale deployment. Selective Traffic: Directs traffic from internal users or beta testers to mitigate potential impacts. Challenges:\nMonitoring Complexity: Requires clear success criteria and effective automation to monitor performance. Resource Allocation: Needs careful management to ensure that monitoring and rollback processes are efficient. A/B Testing: A/B Testing is similar to Canary Release but focuses on testing new features rather than full software versions.\nHow It Works:\nExperimental Deployment: Deploy a new feature or version on a small subset of servers. User Segmentation: Redirect a portion of real user traffic to these experimental servers. Data Collection: Gather data on user interactions and performance metrics. Evaluation: Analyze the results to determine if the new feature should be rolled out to all users or if changes are needed. Benefits:\nReal User Feedback: Provides genuine insights into how users interact with the new feature. Informed Decisions: Helps make data-driven decisions about future features or changes. Testing in Production: Allows testing of features under real-world conditions. Challenges:\nUser Awareness: Users are typically unaware they are part of an experiment, which can complicate feedback collection. Reversion Process: Requires careful handling to revert back to the original version after testing. Chaos Engineering Overview: Chaos Engineering is a production testing technique used to improve the resilience and reliability of distributed systems by deliberately injecting controlled failures into a live environment. This approach helps identify and address potential weaknesses before they lead to catastrophic issues during unexpected real-world events.\nWhy Chaos Engineering?\nInevitability of Failures: In distributed systems, failures are inevitable due to infrastructure issues, network problems, or third-party outages. Limitations of Traditional Testing: Traditional testing methods (unit tests, integration tests) may not capture all possible failure scenarios, especially those that occur in production environments. Key Concepts:\nControlled Failures: Introduce failures such as server terminations, latency, resource exhaustion, or loss of access to simulate potential issues. Monitoring and Analysis: Observe how the system responds to these failures and analyze the performance. Improvement: Identify and fix any issues found during testing to enhance system resilience. Common Failures to Inject:\nServer Termination: Randomly terminate servers to test system recovery and redundancy. Latency Injection: Introduce delays between services or between a service and its database. Resource Exhaustion: Fill up disk space or other resources to observe how the system handles resource limitations. Traffic Restrictions: Disable traffic to specific zones or regions to test failover mechanisms. Process of Chaos Engineering:\nBaseline Measurement: Establish a performance baseline before injecting failures. Hypothesis Formation: Define expected behavior and system responses. Failure Injection: Introduce the failure and monitor its impact. Documentation: Record findings and observations during the test. Restoration: Restore the system to its original state post-testing. Continuous Improvement: Address identified issues and continuously test to ensure ongoing resilience. Considerations:\nMinimize Blast Radius: Ensure failures are contained and do not excessively impact the system. Error Budget: Avoid promising 100% availability; maintain flexibility for unexpected and deliberate failures. ","wordCount":"9685","inLanguage":"en","image":"https://raghu-vijaykumar.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-08-24T05:54:49Z","dateModified":"2024-08-24T05:54:49Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://raghu-vijaykumar.github.io/blog/docs/system-design/architectural-patterns/readme/"},"publisher":{"@type":"Organization","name":"Raghu Vijaykumar","logo":{"@type":"ImageObject","url":"https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://raghu-vijaykumar.github.io/blog/ accesskey=h title="Raghu Vijaykumar (Alt + H)"><img src=https://raghu-vijaykumar.github.io/apple-touch-icon.png alt aria-label=logo height=35>Raghu Vijaykumar</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://raghu-vijaykumar.github.io/blog/about/ title=about><span>about</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/docs/ title=docs><span>docs</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/sponsor/ title=sponsor><span>sponsor</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://raghu-vijaykumar.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://raghu-vijaykumar.github.io/blog/docs/>Docs</a></div><h1 class="post-title entry-hint-parent">Architectural Patterns</h1><div class=post-meta><span title='2024-08-24 05:54:49.971070091 +0000 UTC'>August 24, 2024</span>&nbsp;·&nbsp;46 min&nbsp;·&nbsp;9685 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#architectural-building-blocks>Architectural Building Blocks</a><ul><li><a href=#api-gateway>API Gateway</a></li><li><a href=#api-gateway-solutions--cloud-technologies>API Gateway Solutions & Cloud Technologies</a></li><li><a href=#load-balancer>Load Balancer</a></li><li><a href=#message-brokers-the-building-block-for-asynchronous-architectures>Message Brokers: The Building Block for Asynchronous Architectures</a></li><li><a href=#content-delivery-networks-cdns>Content Delivery Networks (CDNs)</a></li></ul></li><li><a href=#scalability-patterns>Scalability Patterns</a><ul><li><a href=#load-balancing-in-scalable-system-architectures>Load Balancing in Scalable System Architectures</a></li><li><a href=#pipes-and-filters-architecture-pattern>Pipes and Filters Architecture Pattern</a></li><li><a href=#scatter-gather-architecture-pattern>Scatter-Gather Architecture Pattern</a></li><li><a href=#execution-orchestrator-pattern>Execution Orchestrator Pattern</a></li><li><a href=#choreography-pattern>Choreography Pattern</a></li></ul></li><li><a href=#performance-patterns>Performance Patterns</a><ul><li><a href=#mapreduce-pattern>MapReduce Pattern</a></li><li><a href=#saga-pattern>Saga Pattern</a></li><li><a href=#transactional-outbox-pattern>Transactional Outbox Pattern</a></li><li><a href=#materialized-view-pattern>Materialized View Pattern</a></li><li><a href=#cqrs-command-and-query-responsibility-segregation>CQRS (Command and Query Responsibility Segregation)</a></li><li><a href=#combining-cqrs-and-materialized-view-patterns-in-microservices>Combining CQRS and Materialized View Patterns in Microservices</a></li><li><a href=#event-sourcing-pattern>Event Sourcing Pattern</a></li><li><a href=#big-data-processing-and-lambda-architecture>Big Data Processing and Lambda Architecture</a></li></ul></li><li><a href=#software-extensibility-patterns>Software Extensibility Patterns</a><ul><li><a href=#extensibility-patterns-sidecar>Extensibility Patterns: Sidecar</a></li><li><a href=#anti-corruption-adapter-pattern>Anti-Corruption Adapter Pattern</a></li><li><a href=#backends-for-frontends-bff-pattern>Backends for Frontends (BFF) Pattern</a></li></ul></li><li><a href=#reliability--error-handling-patterns>Reliability & Error Handling Patterns</a><ul><li><a href=#throttling-or-rate-limiting-pattern>Throttling or Rate Limiting Pattern</a></li><li><a href=#retry-pattern>Retry Pattern</a></li><li><a href=#circuit-breaker-pattern>Circuit Breaker Pattern</a></li><li><a href=#dead-letter-queue-pattern>Dead Letter Queue Pattern</a></li></ul></li><li><a href=#deployment-patterns>Deployment Patterns</a><ul><li><a href=#rolling-deployment>Rolling Deployment</a></li><li><a href=#blue-green-deployment>Blue Green Deployment</a></li><li><a href=#canary-release-and-ab-testing>Canary Release and A/B Testing</a></li><li><a href=#chaos-engineering>Chaos Engineering</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=architectural-building-blocks>Architectural Building Blocks<a hidden class=anchor aria-hidden=true href=#architectural-building-blocks>#</a></h2><h3 id=api-gateway>API Gateway<a hidden class=anchor aria-hidden=true href=#api-gateway>#</a></h3><h4 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h4><p>An API Gateway is an API management service that acts as an intermediary between clients and backend services. It uses the API composition pattern to aggregate multiple backend services into a single API endpoint that clients interact with. This abstraction provides several key benefits:</p><p><img loading=lazy src=./../images/api-gateway.png alt="API Gateway"></p><h4 id=benefits-of-an-api-gateway>Benefits of an API Gateway<a hidden class=anchor aria-hidden=true href=#benefits-of-an-api-gateway>#</a></h4><ol><li><p><strong>Seamless Internal Changes</strong>:</p><ul><li>Facilitates internal system changes without impacting external API consumers. For example, it enables the splitting of a frontend service into different services for various devices without altering the external API.</li></ul></li><li><p><strong>Consolidated Security</strong>:</p><ul><li>Centralizes security, authentication, and authorization. The API Gateway can handle SSL termination, enforce rate limits, and control access, ensuring secure interactions and protecting against malicious requests.</li></ul></li><li><p><strong>Improved Performance</strong>:</p><ul><li><strong>Request Routing</strong>: Routes client requests to the appropriate backend services and aggregates responses into a single response, reducing the number of client-side calls.</li><li><strong>Caching</strong>: Stores responses to common requests, improving response times by serving cached data instead of querying backend services repeatedly.</li></ul></li><li><p><strong>Enhanced Monitoring and Alerting</strong>:</p><ul><li>Provides real-time visibility into system traffic patterns and load. Enables monitoring and alerting for traffic anomalies, helping in system observability and proactive issue management.</li></ul></li><li><p><strong>Protocol Translation</strong>:</p><ul><li>Handles protocol and format translation between clients and backend services. Supports integration with various external systems by converting different protocols and data formats as needed.</li></ul></li></ol><h4 id=best-practices-and-anti-patterns>Best Practices and Anti-Patterns<a hidden class=anchor aria-hidden=true href=#best-practices-and-anti-patterns>#</a></h4><ol><li><p><strong>Avoid Business Logic</strong>:</p><ul><li>The API Gateway should not include business logic. Its primary functions should be API composition and request routing. Adding business logic can lead to a monolithic architecture and undermine the benefits of service decomposition.</li></ul></li><li><p><strong>Single Point of Failure</strong>:</p><ul><li>An API Gateway can become a single point of failure. To address this, deploy multiple instances behind a load balancer and ensure robust release management to prevent service disruptions.</li></ul></li><li><p><strong>Performance Overhead</strong>:</p><ul><li>While an API Gateway introduces some performance overhead, the overall benefits generally outweigh this. Avoid bypassing or over-optimizing the API Gateway, as this can reintroduce tight coupling and complicate client interactions.</li></ul></li></ol><h3 id=api-gateway-solutions--cloud-technologies>API Gateway Solutions & Cloud Technologies<a hidden class=anchor aria-hidden=true href=#api-gateway-solutions--cloud-technologies>#</a></h3><h4 id=open-source-api-gateways>Open Source API Gateways<a hidden class=anchor aria-hidden=true href=#open-source-api-gateways>#</a></h4><h5 id=netflix-zuul>Netflix Zuul<a hidden class=anchor aria-hidden=true href=#netflix-zuul>#</a></h5><ul><li><strong>Description</strong>: Zuul is a free and open-source application gateway written in Java.</li><li><strong>Capabilities</strong>: Provides dynamic routing, monitoring, resiliency, security, and more.</li></ul><h4 id=cloud-based-api-gateways>Cloud-Based API Gateways<a hidden class=anchor aria-hidden=true href=#cloud-based-api-gateways>#</a></h4><h5 id=amazon-api-gateway>Amazon API Gateway<a hidden class=anchor aria-hidden=true href=#amazon-api-gateway>#</a></h5><ul><li><strong>Description</strong>: A fully managed service for creating, publishing, maintaining, monitoring, and securing APIs at any scale.</li><li><strong>Supports</strong>: RESTful APIs and WebSocket APIs (bi-directional communication between client and server).</li></ul><h5 id=google-cloud-platform-api-gateway>Google Cloud Platform API Gateway<a hidden class=anchor aria-hidden=true href=#google-cloud-platform-api-gateway>#</a></h5><ul><li><strong>Description</strong>: Enables secure access to services through a well-defined REST API consistent across all services.</li></ul><h5 id=microsoft-azure-api-management>Microsoft Azure API Management<a hidden class=anchor aria-hidden=true href=#microsoft-azure-api-management>#</a></h5><ul><li><strong>Description</strong>: Helps organizations publish APIs to external, partner, and internal developers to unlock the potential of their data and services.</li></ul><h3 id=load-balancer>Load Balancer<a hidden class=anchor aria-hidden=true href=#load-balancer>#</a></h3><p>A <strong>load balancer</strong> is a fundamental building block in software architecture, especially for large-scale systems. Its primary role is to distribute incoming network traffic across multiple servers, ensuring no single server is overwhelmed. This distribution helps achieve high availability and horizontal scalability by running multiple instances of an application on different servers.</p><p><img loading=lazy src=./../images/load-balancer.png alt="Load balancer"></p><h4 id=motivation-for-using-load-balancers>Motivation for Using Load Balancers<a hidden class=anchor aria-hidden=true href=#motivation-for-using-load-balancers>#</a></h4><p>Without a load balancer, a client application would need to know the addresses and number of server instances directly. This tight coupling makes it challenging to modify the system&rsquo;s internal structure without affecting the client application. Load balancers provide an abstraction layer, making the entire system appear as a single server with immense computing power and memory.</p><h4 id=quality-attributes-provided-by-load-balancers>Quality Attributes Provided by Load Balancers<a hidden class=anchor aria-hidden=true href=#quality-attributes-provided-by-load-balancers>#</a></h4><ol><li><p><strong>High Scalability</strong>:</p><ul><li>Allows for scaling the system horizontally by adding or removing servers based on demand. In cloud environments, this can be automated with policies that react to metrics like request rate and bandwidth usage.</li></ul></li><li><p><strong>High Availability</strong>:</p><ul><li>Load balancers can monitor server health and route traffic only to healthy servers, ensuring continuous availability even if some servers fail.</li></ul></li><li><p><strong>Performance</strong>:</p><ul><li>While load balancers may introduce minimal latency, they enable higher throughput by distributing requests across multiple servers.</li></ul></li><li><p><strong>Maintainability</strong>:</p><ul><li>Facilitates rolling updates and maintenance by allowing servers to be taken offline for upgrades without disrupting the overall system.</li></ul></li></ol><h5 id=types-of-load-balancers>Types of Load Balancers<a hidden class=anchor aria-hidden=true href=#types-of-load-balancers>#</a></h5><ol><li><strong>DNS Load Balancing</strong>:</li></ol><p><img loading=lazy src=./../images/dns-loadbalancing.png alt="DNS Load Balancing"></p><ul><li>Uses DNS to map a domain name to multiple IP addresses. The list of addresses is rotated, balancing the load. However, this method lacks health checks and only supports simple round-robin strategies, making it less reliable and secure.</li></ul><ol start=2><li><p><strong>Hardware Load Balancers</strong>:</p><ul><li>Dedicated devices optimized for load balancing tasks. They offer features like health checks, intelligent traffic distribution, and can secure the system by hiding internal server details.</li></ul></li><li><p><strong>Software Load Balancers</strong>:</p><ul><li>Programs running on general-purpose hardware. They provide similar features to hardware load balancers but can be more flexible and cost-effective.</li></ul></li><li><p><strong>Global Server Load Balancer (GSLB)</strong>:</p></li></ol><p><img loading=lazy src=./../images/gs-load-balancing.png alt=GSLB></p><ul><li>Combines DNS and load balancer functionalities, intelligently routing users based on location, server load, response time, and more. GSLBs are essential for multi-data center deployments and disaster recovery scenarios.</li></ul><h4 id=load-balancing-solutions--cloud-technologies>Load Balancing Solutions & Cloud Technologies<a hidden class=anchor aria-hidden=true href=#load-balancing-solutions--cloud-technologies>#</a></h4><h5 id=open-source-software-load-balancing-solutions>Open Source Software Load Balancing Solutions<a hidden class=anchor aria-hidden=true href=#open-source-software-load-balancing-solutions>#</a></h5><h6 id=haproxy>HAProxy<a hidden class=anchor aria-hidden=true href=#haproxy>#</a></h6><p>HAProxy is a free and open-source, reliable, high-performance TCP/HTTP load balancer. It is particularly well-suited for high-traffic websites and powers many of the world&rsquo;s most visited ones. HAProxy is considered the de-facto standard open-source load balancer and is included with most mainstream Linux distributions. It supports most Unix-style operating systems.</p><h6 id=nginx>NGINX<a hidden class=anchor aria-hidden=true href=#nginx>#</a></h6><p>NGINX is a free, open-source, high-performance HTTP server and reverse proxy (load balancer). Known for its performance, stability, rich feature set, and simple configuration, NGINX is a popular choice for many applications.</p><h5 id=cloud-based-load-balancing-solutions>Cloud-Based Load Balancing Solutions<a hidden class=anchor aria-hidden=true href=#cloud-based-load-balancing-solutions>#</a></h5><h6 id=aws---elastic-load-balancing-elb>AWS - Elastic Load Balancing (ELB)<a hidden class=anchor aria-hidden=true href=#aws---elastic-load-balancing-elb>#</a></h6><p>Amazon ELB is a highly scalable load balancing solution designed for use with AWS services. It operates in four modes:</p><ul><li><strong>Application (Layer 7) Load Balancer</strong>: Ideal for advanced load balancing of HTTP and HTTPS traffic.</li><li><strong>Network (Layer 4) Load Balancer</strong>: Ideal for load balancing TCP and UDP traffic.</li><li><strong>Gateway Load Balancer</strong>: Ideal for deploying, scaling, and managing third-party virtual appliances.</li><li><strong>Classic Load Balancer (Layer 4 and 7)</strong>: Ideal for routing traffic to EC2 instances.</li></ul><h6 id=gcp---cloud-load-balancing>GCP - Cloud Load Balancing<a hidden class=anchor aria-hidden=true href=#gcp---cloud-load-balancing>#</a></h6><p>Google Cloud Platform Load Balancer is a scalable and robust load-balancing solution. It allows you to put your resources behind a single IP address that is either externally accessible or internal to your Virtual Private Cloud (VPC) network. Available load balancer types include:</p><ul><li><strong>External HTTP(S) Load Balancer</strong>: Externally facing HTTP(s) (Layer 7) load balancer.</li><li><strong>Internal HTTP(S) Load Balancer</strong>: Internal Layer 7 load balancer.</li><li><strong>External TCP/UDP Network Load Balancer</strong>: Externally facing TCP/UDP (Layer 4) load balancer.</li><li><strong>Internal TCP/UDP Load Balancer</strong>: Internally facing TCP/UDP (Layer 4) load balancer.</li></ul><h6 id=microsoft-azure-load-balancer>Microsoft Azure Load Balancer<a hidden class=anchor aria-hidden=true href=#microsoft-azure-load-balancer>#</a></h6><p>Microsoft Azure provides three types of load balancers:</p><ul><li><strong>Standard Load Balancer</strong>: Public and internal Layer 4 load balancer.</li><li><strong>Gateway Load Balancer</strong>: High performance and high availability load balancer for third-party Network Virtual Appliances.</li><li><strong>Basic Load Balancer</strong>: Ideal for small-scale applications.</li></ul><h5 id=gslb-solutions>GSLB Solutions<a hidden class=anchor aria-hidden=true href=#gslb-solutions>#</a></h5><h6 id=amazon-route-53>Amazon Route 53<a hidden class=anchor aria-hidden=true href=#amazon-route-53>#</a></h6><p>Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service.</p><h6 id=aws-global-accelerator>AWS Global Accelerator<a hidden class=anchor aria-hidden=true href=#aws-global-accelerator>#</a></h6><p>AWS Global Accelerator is a networking service that improves the availability, performance, and security of public applications.</p><h6 id=google-cloud-platform-load-balancer--cloud-dns>Google Cloud Platform Load Balancer & Cloud DNS<a hidden class=anchor aria-hidden=true href=#google-cloud-platform-load-balancer--cloud-dns>#</a></h6><p>Google Cloud Platform offers reliable, resilient, low-latency DNS services from its worldwide network, with comprehensive domain registration, management, and serving capabilities.</p><h6 id=azure-traffic-manager>Azure Traffic Manager<a hidden class=anchor aria-hidden=true href=#azure-traffic-manager>#</a></h6><p>Azure Traffic Manager provides DNS-based load balancing.</p><h3 id=message-brokers-the-building-block-for-asynchronous-architectures>Message Brokers: The Building Block for Asynchronous Architectures<a hidden class=anchor aria-hidden=true href=#message-brokers-the-building-block-for-asynchronous-architectures>#</a></h3><p>A message broker is a software component that uses the queue data structure to store and manage messages between senders and receivers. Unlike load balancers, which manage external traffic and are visible to clients, message brokers operate internally within a system and are not exposed externally.</p><h4 id=synchronous-communication-drawbacks>Synchronous Communication Drawbacks<a hidden class=anchor aria-hidden=true href=#synchronous-communication-drawbacks>#</a></h4><p>In synchronous communication, both the sender and receiver must be active and maintain an open connection for the transaction to complete. This can lead to several issues:</p><ul><li><strong>Connection Dependence</strong>: Both services must be healthy and running simultaneously.</li><li><strong>Long Processing Times</strong>: Services that take a long time to process requests can cause delays and hold up the entire system.</li><li><strong>Traffic Handling</strong>: Synchronous communication lacks the ability to handle sudden increases in traffic effectively.</li></ul><p><strong>Example</strong>: In a ticket reservation system, a frontend service must wait for the backend service to complete several operations before providing a response to the user. This creates a delay and can be problematic if the backend service crashes or if there’s a sudden spike in requests.</p><h4 id=benefits-of-message-brokers>Benefits of Message Brokers<a hidden class=anchor aria-hidden=true href=#benefits-of-message-brokers>#</a></h4><h5 id=decoupling-and-asynchronous-processing>Decoupling and Asynchronous Processing<a hidden class=anchor aria-hidden=true href=#decoupling-and-asynchronous-processing>#</a></h5><p>Message brokers allow services to communicate without requiring them to be available simultaneously. For instance:</p><ul><li><strong>Asynchronous Responses</strong>: In the ticket reservation system, the user receives an immediate acknowledgment and the system processes the ticket reservation and payment in the background.</li><li><strong>Service Decoupling</strong>: Services can be broken down into smaller components, each handling a part of the transaction, and communicate through the message broker.</li></ul><h5 id=buffering-and-handling-traffic-spikes>Buffering and Handling Traffic Spikes<a hidden class=anchor aria-hidden=true href=#buffering-and-handling-traffic-spikes>#</a></h5><p>Message brokers can queue messages, helping absorb traffic spikes. For example:</p><ul><li><strong>Order Fulfillment</strong>: In an online store, orders can be queued during high traffic periods and processed sequentially when the load decreases.</li></ul><h5 id=publish-subscribe-pattern>Publish-Subscribe Pattern<a hidden class=anchor aria-hidden=true href=#publish-subscribe-pattern>#</a></h5><p>Message brokers support the publish-subscribe pattern where:</p><ul><li><strong>Multiple Subscribers</strong>: Different services can subscribe to the same channel to receive notifications or updates.</li><li><strong>Flexible Integration</strong>: New services can be added without altering existing systems, such as adding analytics or notification services.</li></ul><h4 id=quality-attributes-of-message-brokers>Quality Attributes of Message Brokers<a hidden class=anchor aria-hidden=true href=#quality-attributes-of-message-brokers>#</a></h4><ul><li><strong>Fault Tolerance</strong>: Message brokers enhance fault tolerance by allowing services to communicate even if some are temporarily unavailable.</li><li><strong>Message Reliability</strong>: They prevent message loss, contributing to higher system availability.</li><li><strong>Scalability</strong>: They help the system scale to handle high traffic by buffering messages.</li></ul><h4 id=performance-considerations>Performance Considerations<a hidden class=anchor aria-hidden=true href=#performance-considerations>#</a></h4><p>While message brokers provide superior availability and scalability, they introduce some latency due to the indirection involved. However, this performance penalty is generally minimal for most systems.</p><h4 id=message-brokers-solutions--cloud-technologies>Message Brokers Solutions & Cloud Technologies<a hidden class=anchor aria-hidden=true href=#message-brokers-solutions--cloud-technologies>#</a></h4><h4 id=open-source-message-brokers>Open Source Message Brokers<a hidden class=anchor aria-hidden=true href=#open-source-message-brokers>#</a></h4><ul><li><p><strong>Apache Kafka</strong>: The most popular open-source message broker today. Apache Kafka is a distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.</p></li><li><p><strong>RabbitMQ</strong>: A widely deployed open-source message broker used globally by both small startups and large enterprises.</p></li></ul><h4 id=cloud-based-message-brokers>Cloud-Based Message Brokers<a hidden class=anchor aria-hidden=true href=#cloud-based-message-brokers>#</a></h4><ul><li><p><strong>Amazon Simple Queue Service (SQS)</strong>: A fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p></li><li><p><strong>GCP Pub/Sub and Cloud Tasks</strong>: Publisher/Subscriber and message queue solutions offered by Google Cloud Platform.</p></li><li><p><strong>Microsoft Azure</strong>:</p><ul><li><strong>Service Bus</strong>: A fully managed enterprise message broker with message queues and publish-subscribe topics.</li><li><strong>Event Hubs</strong>: A fully managed real-time data ingestion service that allows streaming millions of events per second from any source. Integrates seamlessly with Apache Kafka clients without code changes. Ideal for Big Data.</li><li><strong>Event Grid</strong>: A reliable, serverless event delivery system at a massive scale. Uses the publish-subscribe model and is dynamically scalable. It offers a low-cost pay-as-you-go model and guarantees &ldquo;at least once delivery of an event.&rdquo;</li></ul></li></ul><h3 id=content-delivery-networks-cdns>Content Delivery Networks (CDNs)<a hidden class=anchor aria-hidden=true href=#content-delivery-networks-cdns>#</a></h3><h4 id=problem-addressed-by-cdns>Problem Addressed by CDNs<a hidden class=anchor aria-hidden=true href=#problem-addressed-by-cdns>#</a></h4><p>Even with distributed web hosting and technologies like Global Server Load Balancing, significant latency remains due to the physical distance between users and hosting servers, as well as the multiple network hops between routers.</p><h4 id=example-of-latency-without-cdn>Example of Latency Without CDN<a hidden class=anchor aria-hidden=true href=#example-of-latency-without-cdn>#</a></h4><ul><li><strong>User Location</strong>: Brazil</li><li><strong>Server Location</strong>: East Coast, USA</li><li><strong>Initial Latency</strong>: 200 milliseconds</li><li><strong>TCP Connection Latency</strong>: 600 milliseconds (3-way handshake)</li><li><strong>HTTP Request Latency</strong>: 400 milliseconds</li><li><strong>Asset Loading Latency</strong>: 2,000 milliseconds</li><li><strong>Total Latency</strong>: Over 3 seconds</li></ul><h4 id=introduction-to-cdns>Introduction to CDNs<a hidden class=anchor aria-hidden=true href=#introduction-to-cdns>#</a></h4><ul><li><strong>Definition</strong>: A Content Delivery Network (CDN) is a globally distributed network of servers designed to speed up content delivery to end users.</li><li><strong>Purpose</strong>: Reduces latency by caching content on edge servers closer to users.</li><li><strong>Usage</strong>: Delivers webpage content, assets (images, text, CSS, JavaScript), and video streams.</li></ul><h4 id=benefits-of-cdns>Benefits of CDNs<a hidden class=anchor aria-hidden=true href=#benefits-of-cdns>#</a></h4><ol><li><strong>Faster Page Loads</strong>: Reduces total latency to under one second by serving cached content from edge servers.</li><li><strong>Improved Availability</strong>: Content is distributed, reducing the impact of server issues.</li><li><strong>Enhanced Security</strong>: Protects against DDoS attacks by distributing traffic across multiple servers.</li></ol><h4 id=cdn-caching-strategies>CDN Caching Strategies<a hidden class=anchor aria-hidden=true href=#cdn-caching-strategies>#</a></h4><h5 id=pull-strategy>Pull Strategy<a hidden class=anchor aria-hidden=true href=#pull-strategy>#</a></h5><p><img loading=lazy src=./../images/cdn-pull-strategy.png alt="Pull Strategy"></p><ul><li><strong>How It Works</strong>: CDN caches content on first request, subsequent requests are served from the cache.</li><li><strong>Advantages</strong>: Lower maintenance, CDN manages cache updates.</li><li><strong>Drawbacks</strong>: Initial latency for uncached assets, potential traffic spikes when assets expire simultaneously.</li></ul><h5 id=push-strategy>Push Strategy<a hidden class=anchor aria-hidden=true href=#push-strategy>#</a></h5><p><img loading=lazy src=./../images/cdn-push-strategy.png alt="Push Strategy"></p><ul><li><strong>How It Works</strong>: Content is manually or automatically uploaded to the CDN. Updates require re-publishing.</li><li><strong>Advantages</strong>: Reduces traffic to the origin server, maintains high availability even if the origin server is down.</li><li><strong>Drawbacks</strong>: Requires active management to update content, risk of serving outdated content if not updated.</li></ul><h4 id=cdn-solutions--cloud-technologies>CDN Solutions & Cloud Technologies<a hidden class=anchor aria-hidden=true href=#cdn-solutions--cloud-technologies>#</a></h4><h4 id=cloudflare>Cloudflare<a hidden class=anchor aria-hidden=true href=#cloudflare>#</a></h4><ul><li><strong>Description</strong>: Offers ultra-fast static and dynamic content delivery over a global edge network.</li><li><strong>Benefits</strong>: Reduces bandwidth costs and provides built-in unmetered DDoS protection.</li></ul><h4 id=fastly>Fastly<a hidden class=anchor aria-hidden=true href=#fastly>#</a></h4><ul><li><strong>Description</strong>: Deliver@Edge is a modern, efficient, and highly configurable CDN.</li><li><strong>Benefits</strong>: Provides control over content caching to deliver user-requested content quickly.</li></ul><h4 id=akamai>Akamai<a hidden class=anchor aria-hidden=true href=#akamai>#</a></h4><ul><li><strong>Description</strong>: Offers a variety of services including API Acceleration, Global Traffic Management, Image & Video Management, and Media Delivery.</li></ul><h4 id=amazon-cloudfront>Amazon CloudFront<a hidden class=anchor aria-hidden=true href=#amazon-cloudfront>#</a></h4><ul><li><strong>Description</strong>: A high-performance CDN service built for security and developer convenience.</li><li><strong>Use Cases</strong>: Delivers fast, secure websites, accelerates dynamic content and APIs, supports live streaming, and video-on-demand.</li></ul><h4 id=google-cloud-platform-cdn>Google Cloud Platform CDN<a hidden class=anchor aria-hidden=true href=#google-cloud-platform-cdn>#</a></h4><ul><li><strong>Description</strong>: Provides fast, reliable web and video content delivery with global scale and reach.</li></ul><h4 id=microsoft-azure-content-delivery-network>Microsoft Azure Content Delivery Network<a hidden class=anchor aria-hidden=true href=#microsoft-azure-content-delivery-network>#</a></h4><ul><li><strong>Description</strong>: Offers global coverage, full integration with Azure services, and a simple setup.</li></ul><h2 id=scalability-patterns>Scalability Patterns<a hidden class=anchor aria-hidden=true href=#scalability-patterns>#</a></h2><h3 id=load-balancing-in-scalable-system-architectures>Load Balancing in Scalable System Architectures<a hidden class=anchor aria-hidden=true href=#load-balancing-in-scalable-system-architectures>#</a></h3><p>Load balancing is a software architecture pattern used to distribute incoming requests across multiple servers, allowing systems to scale efficiently and maintain performance under high traffic conditions. Single cloud servers are insufficient for handling high volumes of requests, leading to crashes or performance issues. Upgrading servers only postpones the problem. A dispatcher routes incoming requests to available worker servers, enabling load distribution and scalability.</p><h4 id=use-cases>Use Cases<a hidden class=anchor aria-hidden=true href=#use-cases>#</a></h4><ul><li><strong>HTTP Requests:</strong> Distributes front-end requests (web/mobile) to back-end servers.</li><li><strong>Microservices Architecture:</strong> Manages service instances, enabling independent scaling for each service.</li></ul><h4 id=implementation-methods>Implementation Methods<a hidden class=anchor aria-hidden=true href=#implementation-methods>#</a></h4><ul><li><strong>Cloud Load Balancing Services:</strong><ul><li>Managed services that route requests and can scale automatically.</li><li>Avoid becoming a single point of failure.
<img loading=lazy src=./../images/load-balancing.png alt=load-balancing></li></ul></li><li><strong>Message Brokers:</strong><ul><li>Used for asynchronous, one-directional communication between services.</li><li>Useful for internal load balancing of message queues.
<img loading=lazy src=./../images/msg-broker-as-internal-load-balancer.png alt=msg-broker></li></ul></li></ul><h4 id=routing-algorithms>Routing Algorithms<a hidden class=anchor aria-hidden=true href=#routing-algorithms>#</a></h4><ul><li><strong>Round Robin:</strong> Sequentially distributes requests, suitable for stateless applications. Example: Most of the stateless API&rsquo;s</li><li><strong>Sticky Sessions:</strong> Routes requests from the same client to the same server, ideal for stateful applications. Example: Banking, Financial transactions, Multipart file upload.</li><li><strong>Least Connection:</strong> Directs requests to servers with the fewest active connections, suitable for long-term connections. Example: SQL, LDAP.</li></ul><h4 id=auto-scaling-integration>Auto Scaling Integration<a hidden class=anchor aria-hidden=true href=#auto-scaling-integration>#</a></h4><ul><li><strong>Auto Scaling:</strong> Automatically adjusts the number of servers based on metrics like CPU usage and traffic.</li><li>Works in conjunction with load balancing to optimize resource use and cost.</li></ul><h5 id=example-of-auto-scaling-integration>Example of Auto Scaling Integration<a hidden class=anchor aria-hidden=true href=#example-of-auto-scaling-integration>#</a></h5><p><strong>Scenario:</strong>
A cloud-based e-commerce platform experiences variable traffic, with peaks during sales events and lower traffic during off-peak hours.</p><p><strong>Auto Scaling Integration Steps:</strong></p><p><img loading=lazy src=./../images/auto-scaling-group.png alt="Auto Scaling"></p><ol><li><p><strong>Monitoring and Metrics Collection:</strong></p><ul><li>Server instances in the cloud environment run monitoring agents to collect metrics such as CPU utilization, memory usage, and network traffic.</li></ul></li><li><p><strong>Defining Auto Scaling Policies:</strong></p><ul><li>Policies are established based on the collected metrics. For example:<ul><li><strong>Scale Up Policy:</strong> Add more server instances if the average CPU utilization exceeds 70% for 5 consecutive minutes.</li><li><strong>Scale Down Policy:</strong> Remove server instances if the average CPU utilization drops below 30% for 10 consecutive minutes.</li></ul></li></ul></li><li><p><strong>Load Balancer Coordination:</strong></p><ul><li>The load balancer is configured to recognize the dynamic pool of server instances. It automatically adjusts the routing of incoming requests based on the current set of available instances.</li></ul></li><li><p><strong>Implementation Example:</strong></p><ul><li><strong>Cloud Load Balancer:</strong> Utilizes a cloud provider&rsquo;s load balancing service to distribute incoming traffic across a pool of identical web server instances.</li><li><strong>Auto Scaling Group:</strong> The web server instances are managed as an auto-scaling group within the cloud provider&rsquo;s infrastructure.</li><li><strong>Elasticity in Action:</strong><ul><li>During a flash sale, the traffic spikes, causing the average CPU utilization to rise above 70%. The auto-scaling policy triggers, launching additional server instances to handle the increased load.</li><li>The load balancer detects the new instances and routes traffic to them, balancing the load.</li><li>After the sale, as traffic decreases, the average CPU utilization falls below 30%. The auto-scaling policy triggers the removal of excess instances, reducing costs.</li></ul></li></ul></li></ol><p><strong>Benefits:</strong></p><ul><li><strong>Dynamic Scalability:</strong> Automatically adapts to traffic changes, ensuring optimal performance.</li><li><strong>Cost Efficiency:</strong> Reduces infrastructure costs by scaling down during low-traffic periods.</li><li><strong>Resilience:</strong> Prevents server overload and potential crashes by distributing the load and adding capacity when necessary.</li></ul><p>This example illustrates how auto scaling, combined with a load balancer, efficiently manages variable workloads in a cloud environment, providing both scalability and cost-effectiveness.</p><p>References</p><ul><li><a href=https://cloud.google.com/compute/docs/autoscaler>Google Compute Engine Autoscaling Groups</a></li><li><a href=https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler>GKE Cluster Autoscaling</a></li><li><a href=https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html>Amazon EC2 Autoscaling</a></li></ul><h4 id=considerations>Considerations<a hidden class=anchor aria-hidden=true href=#considerations>#</a></h4><ul><li><strong>Session Management:</strong> Choosing the right routing algorithm depends on whether the application is stateless or stateful.</li><li><strong>Scalability and Cost Efficiency:</strong> Combining load balancing with auto scaling helps in dynamically resizing the backend infrastructure, ensuring optimal resource utilization and cost savings.</li></ul><p>This documentation outlines the essential aspects of implementing and managing load balancing in scalable system architectures, highlighting methods, algorithms, and practical considerations for effective deployment.</p><h3 id=pipes-and-filters-architecture-pattern>Pipes and Filters Architecture Pattern<a hidden class=anchor aria-hidden=true href=#pipes-and-filters-architecture-pattern>#</a></h3><p><img loading=lazy src=./../images/pipes-and-filter-approach.png alt="pipes and filter"></p><h4 id=overview-1>Overview<a hidden class=anchor aria-hidden=true href=#overview-1>#</a></h4><ul><li><strong>Analogy:</strong> Data flows like water through a series of pipes and filters.</li><li><strong>Components:</strong><ul><li><strong>Data Source:</strong> Origin of the data, e.g., backend services, sensors.</li><li><strong>Filters:</strong> Isolated software components that process data.</li><li><strong>Pipes:</strong> Mechanisms like distributed queues or message brokers that connect filters.</li><li><strong>Data Sink:</strong> Final destination for processed data, e.g., databases, external services.</li></ul></li></ul><h4 id=key-concepts>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts>#</a></h4><ul><li><strong>Filters:</strong> Perform single operations, unaware of the rest of the pipeline.</li><li><strong>Pipes:</strong> Can store data temporarily or use message systems for notifications.</li></ul><h4 id=benefits>Benefits<a hidden class=anchor aria-hidden=true href=#benefits>#</a></h4><ol><li><strong>Decoupling:</strong><ul><li>Allows different processing tasks to use different programming languages and technologies.</li></ul></li><li><strong>Hardware Optimization:</strong><ul><li>Each task can run on the most suitable hardware (e.g., specialized hardware for machine learning).</li></ul></li><li><strong>Scalability:</strong><ul><li>Each filter can be scaled independently based on workload needs.</li></ul></li></ol><h4 id=real-world-use-cases>Real-World Use Cases<a hidden class=anchor aria-hidden=true href=#real-world-use-cases>#</a></h4><ul><li><strong>Digital Advertising:</strong> Processing streams of user activity data.</li><li><strong>Internet of Things (IoT):</strong> Data processing from end devices.</li><li><strong>Media Processing:</strong><ul><li>Video and audio processing pipelines, including chunking, thumbnail creation, resolution resizing, adaptive streaming, and captioning.</li></ul></li></ul><h4 id=example-video-sharing-service>Example: Video Sharing Service<a hidden class=anchor aria-hidden=true href=#example-video-sharing-service>#</a></h4><ol><li><strong>Video Processing Pipeline:</strong></li></ol><p><img loading=lazy src=./../images/video-sharing-service-architecture.png alt=video-sharing-arch></p><ul><li><strong>Chunking:</strong> Split video into smaller chunks.</li><li><strong>Thumbnail Extraction:</strong> Select frames as thumbnails.</li><li><strong>Resolution and Bitrate Adjustment:</strong> Resize chunks for adaptive streaming.</li><li><strong>Encoding:</strong> Encode chunks into different formats.</li></ul><ol start=2><li><strong>Audio Processing Pipeline:</strong><ul><li><strong>Transcription:</strong> Convert speech to text.</li><li><strong>Captioning and Translation:</strong> Provide captions and translate into different languages.</li><li><strong>Content Moderation:</strong> Detect copyrighted or inappropriate content.</li></ul></li></ol><h4 id=considerations-1>Considerations<a hidden class=anchor aria-hidden=true href=#considerations-1>#</a></h4><ol><li><strong>Complexity:</strong><ul><li>Maintaining this architecture can be complex, especially with granular filters.</li></ul></li><li><strong>Stateless Filters:</strong><ul><li>Each filter should be independent and stateless.</li></ul></li><li><strong>Transaction Handling:</strong><ul><li>Not suitable for scenarios requiring a single transaction across the entire pipeline.</li></ul></li></ol><p>The pipes and filters pattern is valuable for scenarios needing flexible, scalable, and decoupled processing pipelines but may not be ideal for transactional data processing.</p><h3 id=scatter-gather-architecture-pattern>Scatter-Gather Architecture Pattern<a hidden class=anchor aria-hidden=true href=#scatter-gather-architecture-pattern>#</a></h3><p><img loading=lazy src=./../images/scatter-gatherer-pattern.png alt=scatter-gatherer-pattern></p><h4 id=overview-2>Overview<a hidden class=anchor aria-hidden=true href=#overview-2>#</a></h4><ul><li><strong>Components:</strong><ul><li><strong>Sender/Requester:</strong> Initiates the request.</li><li><strong>Workers:</strong> Respond to the request; can be internal or external services.</li><li><strong>Dispatcher:</strong> Distributes the request to all workers and collects responses.</li><li><strong>Aggregator:</strong> Combines responses from workers into a single response.</li></ul></li></ul><h4 id=key-concepts-1>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-1>#</a></h4><ul><li><strong>Parallel Processing:</strong><ul><li>Requests are sent to all workers simultaneously, allowing for parallel processing.</li><li>Workers can be diverse, performing different functions or accessing different data.</li></ul></li></ul><h4 id=use-cases-1>Use Cases<a hidden class=anchor aria-hidden=true href=#use-cases-1>#</a></h4><ol><li><strong>Search Services:</strong></li></ol><p><img loading=lazy src=./../images/serach-service.png alt="search service"></p><ul><li>Users send a query, and internal workers search through various data subsets.</li><li>Results are aggregated and returned as a ranked list.</li></ul><ol start=2><li><strong>Hospitality Services:</strong></li></ol><p><img loading=lazy src=./../images/hospitality-service.png alt="Hospitatlity service"></p><ul><li>A request for hotel availability is sent to multiple hotels.</li><li>Responses are collected and sorted based on criteria like price or rating.</li></ul><h4 id=considerations-2>Considerations<a hidden class=anchor aria-hidden=true href=#considerations-2>#</a></h4><ol><li><strong>Timeouts:</strong><ul><li>Set an upper limit for waiting for worker responses to avoid delays due to unresponsive workers.</li></ul></li><li><strong>Decoupling:</strong><ul><li>Use a message broker to decouple the dispatcher and workers, facilitating asynchronous communication.</li></ul></li><li><strong>Long-running Tasks:</strong><ul><li>For tasks requiring extensive processing, separate the dispatcher and aggregator.</li><li>Use unique identifiers for tracking and retrieving results.</li></ul></li></ol><h4 id=example-workflow>Example Workflow<a hidden class=anchor aria-hidden=true href=#example-workflow>#</a></h4><ol><li><p><strong>Immediate Response Use Case:</strong></p><ul><li>Dispatcher sends a request to workers.</li><li>Workers process and return responses quickly.</li><li>Aggregator compiles the results and sends them to the user.</li></ul></li><li><p><strong>Long-running Task Use Case:</strong></p><ul><li>Dispatcher assigns a unique ID to the request and sends it to workers.</li><li>Workers return partial results with the same ID.</li><li>Aggregator stores and compiles results, accessible via the unique ID.</li></ul></li></ol><h5 id=benefits-1>Benefits<a hidden class=anchor aria-hidden=true href=#benefits-1>#</a></h5><ul><li><strong>Scalability:</strong><ul><li>Supports high scalability by enabling parallel processing across numerous workers.</li></ul></li><li><strong>Flexibility:</strong><ul><li>Can integrate diverse internal and external services.</li></ul></li><li><strong>Resilience:</strong><ul><li>Can continue processing even if some workers are unavailable or slow.</li></ul></li></ul><p>The scatter-gather pattern is versatile and widely used in many production systems, providing efficient parallel processing and aggregation of results.</p><h3 id=execution-orchestrator-pattern>Execution Orchestrator Pattern<a hidden class=anchor aria-hidden=true href=#execution-orchestrator-pattern>#</a></h3><p><img loading=lazy src=./../images/executor-orchestrator-pattern.png alt=executor-orchestrator></p><h4 id=overview-3>Overview<a hidden class=anchor aria-hidden=true href=#overview-3>#</a></h4><ul><li><strong>Purpose:</strong> Manages a sequence of operations across multiple services in microservices architecture.</li><li><strong>Analogy:</strong> Like a conductor in an orchestra, the orchestrator directs services without performing the business logic itself.</li></ul><h4 id=key-concepts-2>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-2>#</a></h4><ul><li><strong>Execution Orchestrator:</strong> A centralized service that:<ul><li>Calls different services in the right order.</li><li>Handles exceptions and retries.</li><li>Maintains the state of the flow until completion.</li></ul></li><li><strong>Microservices:</strong> Individual services responsible for specific business logic, typically stateless and independently scalable.</li></ul><h4 id=use-case-example-video-on-demand-service>Use Case Example: Video on Demand Service<a hidden class=anchor aria-hidden=true href=#use-case-example-video-on-demand-service>#</a></h4><p><img loading=lazy src=./../images/video-on-demand-user-onboarding.png alt="Video on Demand Service"></p><ul><li><strong>User Registration Flow:</strong><ol><li>User fills out a registration form (username, password, payment).</li><li><strong>Orchestrator Service</strong> handles:<ul><li><strong>User Service:</strong> Validates username and password.</li><li><strong>Payment Service:</strong> Authorizes credit card via a third-party API.</li><li><strong>Location Service:</strong> Registers user location for content access.</li><li><strong>Recommendation Service:</strong> Sets up user preferences.</li><li><strong>Email Service:</strong> Sends a confirmation email with details.</li></ul></li></ol></li></ul><h4 id=considerations-3>Considerations<a hidden class=anchor aria-hidden=true href=#considerations-3>#</a></h4><ol><li><strong>Scalability:</strong><ul><li>Orchestrator and microservices can be scaled independently.</li><li>Orchestrator can be deployed across multiple instances for reliability.</li></ul></li><li><strong>Failure and Recovery:</strong><ul><li>Orchestrator handles errors, retries, and manages the flow&rsquo;s state.</li><li>Maintains a database to persist the state for recovery in case of failures.</li></ul></li><li><strong>Avoiding Monolithic Tendencies:</strong><ul><li>Keep the orchestrator focused on coordination, not business logic.</li></ul></li></ol><h4 id=benefits-2>Benefits<a hidden class=anchor aria-hidden=true href=#benefits-2>#</a></h4><ul><li><strong>Decoupling:</strong> Microservices operate independently and are unaware of the orchestration.</li><li><strong>Efficiency:</strong> Supports parallel and sequential operations.</li><li><strong>Flexibility:</strong> Easy to modify the flow by updating the orchestrator.</li></ul><p>This pattern is particularly useful in complex systems requiring coordination of multiple independent services, offering a scalable and maintainable solution for executing business logic workflows.</p><h3 id=choreography-pattern>Choreography Pattern<a hidden class=anchor aria-hidden=true href=#choreography-pattern>#</a></h3><p><img loading=lazy src=./../images/choregraphy-pattern.png alt="Choreography Pattern"></p><h4 id=overview-4>Overview<a hidden class=anchor aria-hidden=true href=#overview-4>#</a></h4><ul><li><strong>Purpose:</strong> Helps scale complex flows of business transactions in microservices architecture.</li><li><strong>Comparison:</strong> Unlike the orchestration pattern, choreography uses asynchronous events without a central orchestrator.</li></ul><h4 id=key-concepts-3>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-3>#</a></h4><ul><li><strong>Microservices:</strong> Decoupled services that communicate through a message broker.</li><li><strong>Message Broker:</strong> A distributed message queue that stores and distributes events.</li></ul><h4 id=advantages>Advantages<a hidden class=anchor aria-hidden=true href=#advantages>#</a></h4><ul><li><strong>Loose Coupling:</strong> Services operate independently and are not tightly coupled through a central orchestrator.</li><li><strong>Scalability:</strong> Easy to add or remove services and scale operations.</li><li><strong>Cost Efficiency:</strong> Services can be implemented as functions that only run when triggered, saving resources.</li></ul><h4 id=example-job-search-service>Example: Job Search Service<a hidden class=anchor aria-hidden=true href=#example-job-search-service>#</a></h4><p><img loading=lazy src=./../images/job-search-service.png alt="Job Search Service"></p><ol><li><strong>User Registration:</strong><ul><li>User submits a form with their details and resume.</li><li><strong>Candidate Service:</strong> Stores data and emits an event.</li></ul></li><li><strong>Email Confirmation:</strong><ul><li>Triggered by the event, an email confirmation is sent to the user.</li></ul></li><li><strong>Skills Parsing:</strong><ul><li><strong>Skills Parser Service:</strong> Extracts and stores skills data, then emits an event.</li></ul></li><li><strong>Job Search:</strong><ul><li><strong>Job Search Service:</strong> Searches for job matches and emits results as an event.</li></ul></li><li><strong>Job Notifications:</strong><ul><li><strong>Candidate Service:</strong> Updates user records.</li><li><strong>Email Service:</strong> Sends job notifications based on user preferences.</li></ul></li></ol><h4 id=considerations-4>Considerations<a hidden class=anchor aria-hidden=true href=#considerations-4>#</a></h4><ol><li><strong>Debugging Challenges:</strong><ul><li>Troubleshooting issues can be difficult due to lack of a central coordinator.</li><li>Harder to trace the flow of events and identify issues.</li></ul></li><li><strong>Testing Complexity:</strong><ul><li>Requires complex integration tests to catch issues before production.</li><li>Becomes more challenging as the number of services grows.</li></ul></li></ol><h2 id=performance-patterns>Performance Patterns<a hidden class=anchor aria-hidden=true href=#performance-patterns>#</a></h2><h3 id=mapreduce-pattern>MapReduce Pattern<a hidden class=anchor aria-hidden=true href=#mapreduce-pattern>#</a></h3><p><img loading=lazy src=./../images/map-reduce-pattern.png alt=map-reduce></p><h4 id=overview-5>Overview<a hidden class=anchor aria-hidden=true href=#overview-5>#</a></h4><ul><li><strong>Origin:</strong> Introduced by Jeff Dean and S.J. Ghemawat from Google in 2004.</li><li><strong>Purpose:</strong> Simplifies processing of large data sets by distributing computation across many machines.</li></ul><h4 id=key-concepts-4>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-4>#</a></h4><ul><li><strong>Map Function:</strong> Transforms input key-value pairs into intermediate key-value pairs.</li><li><strong>Reduce Function:</strong> Aggregates and processes intermediate pairs to produce final output.</li></ul><h4 id=example-word-count-in-text-files>Example: Word Count in Text Files<a hidden class=anchor aria-hidden=true href=#example-word-count-in-text-files>#</a></h4><ol><li><strong>Input:</strong> Key-value pairs (filename, content).</li><li><strong>Map Function:</strong> Emits (word, 1) for each word in the content.</li><li><strong>Shuffle and Sort:</strong> Groups intermediate pairs by word.</li><li><strong>Reduce Function:</strong> Sums the counts for each word.</li></ol><h4 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h4><p><img loading=lazy src=./../images/map-reduce-architecture.png alt=map-reduce-architecture></p><blockquote><p>TODO: Add backup master and snapshot storage in the image</p></blockquote><ul><li><strong>Master Node:</strong> Orchestrates the computation, schedules tasks, and handles failures.</li><li><strong>Worker Nodes:</strong> Execute map and reduce tasks in parallel.</li><li><strong>Data Distribution:</strong> Input data is split into chunks for parallel processing.</li></ul><h4 id=fault-tolerance>Fault Tolerance<a hidden class=anchor aria-hidden=true href=#fault-tolerance>#</a></h4><ul><li><strong>Worker Failures:</strong> Master reassigns tasks and notifies reduce workers of new data locations.</li><li><strong>Master Failures:</strong> Can either restart the process or use a backup master to continue.</li></ul><h4 id=cloud-integration>Cloud Integration<a hidden class=anchor aria-hidden=true href=#cloud-integration>#</a></h4><ul><li><strong>Scalability:</strong> Cloud environments provide access to many machines, enabling large-scale data processing.</li><li><strong>Cost Efficiency:</strong> MapReduce&rsquo;s batch processing nature means paying only for resources used during the job, not for maintaining idle machines.</li></ul><h3 id=saga-pattern>Saga Pattern<a hidden class=anchor aria-hidden=true href=#saga-pattern>#</a></h3><h4 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h4><ul><li><strong>Context:</strong> Microservices architecture, where each service has its own database.</li><li><strong>Problem:</strong> Ensuring data consistency across multiple databases without a central database, losing traditional ACID transactions.</li></ul><h4 id=solution-saga-pattern>Solution: Saga Pattern<a hidden class=anchor aria-hidden=true href=#solution-saga-pattern>#</a></h4><p><img loading=lazy src=./../images/saga-pattern-distributed-transaction-success.png alt=distributed-transaction>
<img loading=lazy src=./../images/distributed-rollback.png alt=distributed-rollback></p><ul><li><strong>Definition:</strong> Manages data consistency in distributed transactions by breaking them into a series of local transactions. If an operation fails, compensating transactions are executed to roll back.</li></ul><h4 id=implementation-methods-1>Implementation Methods<a hidden class=anchor aria-hidden=true href=#implementation-methods-1>#</a></h4><ol><li><p><strong>Execution Orchestrator Pattern:</strong></p><ul><li>A central orchestrator service manages the transaction flow, calling services sequentially.</li><li>Decides whether to continue or rollback based on service responses.</li></ul></li><li><p><strong>Choreography Pattern:</strong></p><ul><li>No central orchestrator; services communicate through a message broker.</li><li>Each service listens for events and triggers subsequent steps or compensations as needed.</li></ul></li></ol><h4 id=example-scenario-ticket-reservation-system>Example Scenario: Ticket Reservation System<a hidden class=anchor aria-hidden=true href=#example-scenario-ticket-reservation-system>#</a></h4><p><img loading=lazy src=./../images/movie-ticketing-system.png alt="Ticket Reservation System"></p><ol><li><p><strong>Services Involved:</strong> Order, Security, Billing, Reservation, Email, (Orchestration if using orchestrator pattern).</p></li><li><p><strong>Process:</strong></p><ul><li><strong>Order Service:</strong> Registers an order and sets it to pending.</li><li><strong>Security Service:</strong> Validates the user (not a bot or blacklisted).</li><li><strong>Billing Service:</strong> Authorizes the pending transaction on the user&rsquo;s credit card.</li><li><strong>Reservation Service:</strong> Reserves the ticket for the user.</li><li><strong>Email Service:</strong> Sends a confirmation email.</li></ul></li><li><p><strong>Failure Handling:</strong></p><ul><li>If any service returns a failure, compensating operations are triggered:<ul><li>Cancel pending transactions.</li><li>Remove records from databases.</li></ul></li><li>Compensating operations ensure consistency by undoing previous steps if necessary.</li></ul></li></ol><h4 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h4><ul><li><strong>Purpose:</strong> Maintains data consistency in a microservices environment by handling distributed transactions.</li><li><strong>Flexibility:</strong> Can be implemented using either the execution orchestrator pattern or the choreography pattern.</li><li><strong>Resilience:</strong> Provides mechanisms to complete transactions or roll back in case of failures.</li></ul><p>The saga pattern is crucial for ensuring reliable operations and consistency in complex systems with distributed architectures.</p><h3 id=transactional-outbox-pattern>Transactional Outbox Pattern<a hidden class=anchor aria-hidden=true href=#transactional-outbox-pattern>#</a></h3><h4 id=overview-6>Overview<a hidden class=anchor aria-hidden=true href=#overview-6>#</a></h4><ul><li><strong>Problem:</strong> In event-driven architectures, ensuring that a database update and an event publication occur together reliably can be challenging. Specifically, there&rsquo;s a risk of losing events or data if a system crash occurs between these operations.</li></ul><p><img loading=lazy src=./../images/transactional-outbox-pattern.png alt="Transactional Outbox Pattern"></p><h4 id=solution-transactional-outbox-pattern>Solution: Transactional Outbox Pattern<a hidden class=anchor aria-hidden=true href=#solution-transactional-outbox-pattern>#</a></h4><ul><li><strong>Concept:</strong> Involves adding an <strong>Outbox Table</strong> to the database to store messages intended for the message broker. Updates to both the business logic table (e.g., users) and the Outbox Table are performed within a single database transaction.</li></ul><h4 id=implementation-steps>Implementation Steps<a hidden class=anchor aria-hidden=true href=#implementation-steps>#</a></h4><ol><li><p><strong>Database Update and Message Logging:</strong></p><ul><li>Instead of sending an event directly to the message broker, the service logs the message to the Outbox Table along with updating the primary business logic table.</li><li>Ensures atomicity: Both tables are updated together, or neither is, preventing partial updates.</li></ul></li><li><p><strong>Message Sender/Relay Service:</strong></p><ul><li>A separate service monitors the Outbox Table for new messages.</li><li>Upon finding a new message, it sends it to the message broker and marks it as sent (or deletes it).</li></ul></li></ol><h4 id=addressing-potential-issues>Addressing Potential Issues<a hidden class=anchor aria-hidden=true href=#addressing-potential-issues>#</a></h4><ol><li><p><strong>Duplicate Events:</strong></p><ul><li><strong>Cause:</strong> A crash between sending a message and marking it as sent can result in duplicate events.</li><li><strong>Solution:</strong> Implement &ldquo;at least once&rdquo; delivery semantics. Each message gets a unique ID. Consumers track processed message IDs to discard duplicates.</li></ul></li><li><p><strong>Lack of Transaction Support:</strong></p><ul><li><strong>Scenario:</strong> Some databases, especially NoSQL ones, may not support multi-collection transactions.</li><li><strong>Solution:</strong> Embed the Outbox message directly within the same document (or object) in the database. The sender service queries for documents with messages, sends them, and then clears the messages.</li></ul></li><li><p><strong>Ordering of Events:</strong></p><ul><li><strong>Problem:</strong> Ensuring the order of related events, such as a user signup followed by a cancellation.</li><li><strong>Solution:</strong> Assign each message a sequential ID. The sender service can then sort and send messages in the correct order based on these IDs.</li></ul></li></ol><h4 id=conclusion-1>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-1>#</a></h4><ul><li><strong>Benefits:</strong> The transactional outbox pattern provides a robust solution for ensuring data consistency and reliable event publication in distributed systems.</li><li><strong>Considerations:</strong> Addressing potential issues like duplicate events, lack of transaction support, and event ordering is crucial for effective implementation.</li></ul><p>The transactional outbox pattern is an essential tool for maintaining consistency in microservices architectures, particularly in systems that rely heavily on event-driven communication.</p><h3 id=materialized-view-pattern>Materialized View Pattern<a hidden class=anchor aria-hidden=true href=#materialized-view-pattern>#</a></h3><h4 id=overview-7>Overview<a hidden class=anchor aria-hidden=true href=#overview-7>#</a></h4><ul><li><strong>Purpose:</strong> To optimize performance and cost efficiency in data-intensive applications by pre-computing and storing query results.</li></ul><p><img loading=lazy src=./../images/materialized-views.png alt="Materialized View Pattern"></p><h4 id=problem-statement>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement>#</a></h4><ol><li><strong>Performance:</strong> Complex queries, especially those involving multiple tables or databases, can be slow.</li><li><strong>Cost:</strong> Repeatedly running the same complex queries can be resource-intensive, leading to high costs in a cloud environment.</li></ol><h4 id=solution-materialized-view>Solution: Materialized View<a hidden class=anchor aria-hidden=true href=#solution-materialized-view>#</a></h4><ul><li><strong>Concept:</strong> Create a read-only table (materialized view) that stores the pre-computed results of a specific query. This allows for quick data retrieval without recalculating the query each time.</li></ul><h4 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h4><ol><li><p><strong>Data Storage:</strong></p><ul><li>Store materialized views as separate tables in the same database or in a read-optimized separate database.</li><li>In cases of frequent data updates, the materialized view can be regenerated immediately or on a fixed schedule.</li></ul></li><li><p><strong>Example Use Case:</strong></p><ul><li><strong>Scenario:</strong> An online education platform with tables for courses and reviews.</li><li><strong>Need:</strong> Display top courses based on average ratings for a specific topic.</li><li><strong>Solution:</strong> Create a materialized view storing pre-computed course ratings, which can be filtered quickly based on the topic.</li></ul></li><li><p><strong>Benefits:</strong></p><ul><li><strong>Performance:</strong> Significantly reduces query time by avoiding complex aggregations and joins.</li><li><strong>Cost Efficiency:</strong> Saves resources by reducing the need for repeated complex query execution.</li></ul></li></ol><h4 id=considerations-5>Considerations<a hidden class=anchor aria-hidden=true href=#considerations-5>#</a></h4><ol><li><strong>Storage Space:</strong> Materialized views require additional storage, increasing costs. The trade-off between performance and space must be evaluated.</li><li><strong>Update Frequency:</strong><ul><li><strong>Same Database:</strong> If the database supports materialized views, updates can be automatic and efficient.</li><li><strong>External Storage:</strong> Requires manual or programmatic updates, which can add complexity.</li></ul></li></ol><h4 id=conclusion-2>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-2>#</a></h4><ul><li>The materialized view pattern is a powerful tool for optimizing the performance of data-intensive applications. By pre-computing and storing query results, this pattern improves user experience and reduces operational costs, especially in environments where resource usage incurs significant expenses.</li></ul><h3 id=cqrs-command-and-query-responsibility-segregation>CQRS (Command and Query Responsibility Segregation)<a hidden class=anchor aria-hidden=true href=#cqrs-command-and-query-responsibility-segregation>#</a></h3><h4 id=overview-8>Overview<a hidden class=anchor aria-hidden=true href=#overview-8>#</a></h4><ul><li><strong>Purpose:</strong> To separate the command (write) and query (read) responsibilities in a system into distinct services and databases, optimizing each for its specific workload.</li></ul><p><img loading=lazy src=./../images/cqrs-pattern.png alt=cqrs-pattern></p><h4 id=key-concepts-5>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-5>#</a></h4><ol><li><strong>Command Operations:</strong> Actions that mutate data, such as insertions, updates, and deletions.</li><li><strong>Query Operations:</strong> Actions that read and return data without altering it.</li></ol><h4 id=benefits-of-cqrs>Benefits of CQRS<a hidden class=anchor aria-hidden=true href=#benefits-of-cqrs>#</a></h4><ol><li><strong>Separation of Concerns:</strong><ul><li>Command service handles business logic, validations, and data mutations.</li><li>Query service focuses solely on data retrieval and presentation.</li></ul></li><li><strong>Independent Scalability:</strong><ul><li>The number of instances for each service can be scaled independently based on demand.</li></ul></li><li><strong>Optimized Data Models:</strong><ul><li>Command database can be optimized for write operations.</li><li>Query database can be optimized for read operations, often using different database technologies.</li></ul></li></ol><h4 id=example-use-case-online-store>Example Use Case: Online Store<a hidden class=anchor aria-hidden=true href=#example-use-case-online-store>#</a></h4><ul><li><strong>Command Side:</strong><ul><li>Stores user reviews in a relational database.</li><li>Ensures business logic, such as validating purchases and review content.</li></ul></li><li><strong>Query Side:</strong><ul><li>Stores reviews and average ratings in a NoSQL database, optimized for quick retrieval.</li><li>Contains pre-aggregated data to minimize real-time computations.</li></ul></li></ul><h4 id=synchronization>Synchronization<a hidden class=anchor aria-hidden=true href=#synchronization>#</a></h4><ul><li><strong>Event Publishing:</strong><ul><li>On data mutation, the command service publishes events to keep the query database updated.</li><li>Can use message brokers or functions-as-a-service for event handling and data synchronization.</li></ul></li><li><strong>Eventual Consistency:</strong><ul><li>The system guarantees eventual consistency between command and query databases but not strict consistency.</li></ul></li></ul><h4 id=considerations-6>Considerations<a hidden class=anchor aria-hidden=true href=#considerations-6>#</a></h4><ul><li><strong>Complexity:</strong><ul><li>Requires managing multiple services, databases, and synchronization mechanisms.</li><li>Adds overhead but provides significant performance benefits.</li></ul></li><li><strong>Eventual Consistency:</strong><ul><li>Suitable for scenarios where eventual consistency is acceptable, not for strict consistency requirements.</li></ul></li></ul><h3 id=combining-cqrs-and-materialized-view-patterns-in-microservices>Combining CQRS and Materialized View Patterns in Microservices<a hidden class=anchor aria-hidden=true href=#combining-cqrs-and-materialized-view-patterns-in-microservices>#</a></h3><h4 id=overview-9>Overview<a hidden class=anchor aria-hidden=true href=#overview-9>#</a></h4><ul><li><strong>Problem:</strong> In a microservices architecture, data is often split across multiple services and databases, making it challenging to aggregate data efficiently for queries.</li><li><strong>Solution:</strong> Use a combination of CQRS and materialized view patterns to optimize data retrieval and maintain synchronization across services.</li></ul><p><img loading=lazy src=./../images/cqrs-materialized-view.png alt="CQRS and Materialized View Patterns"></p><h4 id=key-concepts-6>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-6>#</a></h4><ol><li><p><strong>Microservices Split:</strong></p><ul><li>Initially, all data might reside in a single database. Splitting into microservices involves dividing this data into separate databases, each handled by a specific service.</li><li>This division prevents traditional join operations across different microservices&rsquo; databases.</li></ul></li><li><p><strong>Performance Challenges:</strong></p><ul><li>Aggregating data across multiple services requires API calls, database queries, and programmatic joins, leading to significant performance overhead.</li></ul></li></ol><h4 id=solution-combining-patterns>Solution: Combining Patterns<a hidden class=anchor aria-hidden=true href=#solution-combining-patterns>#</a></h4><ol><li><p><strong>CQRS (Command and Query Responsibility Segregation):</strong></p><ul><li>Create a new microservice with a read-optimized database specifically for querying aggregated data.</li><li>This microservice only handles queries and does not perform data mutations.</li></ul></li><li><p><strong>Materialized View:</strong></p><ul><li>A materialized view is created to pre-aggregate and store relevant data from multiple microservices.</li><li>The view includes data necessary for user queries, combining information from different microservices.</li></ul></li></ol><h4 id=synchronization-methods>Synchronization Methods<a hidden class=anchor aria-hidden=true href=#synchronization-methods>#</a></h4><ol><li><p><strong>Message Broker:</strong></p><ul><li>Each microservice publishes events to a message broker when data changes.</li><li>The query microservice listens to these events and updates the materialized view accordingly.</li></ul></li><li><p><strong>Cloud Function:</strong></p><ul><li>A function as a service monitors tables in different services&rsquo; databases.</li><li>On detecting changes, the function updates the materialized view in the query database.</li></ul></li></ol><h4 id=real-world-example-online-education-platform>Real-World Example: Online Education Platform<a hidden class=anchor aria-hidden=true href=#real-world-example-online-education-platform>#</a></h4><ul><li><p><strong>Setup:</strong></p><ul><li><em>Courses Microservice:</em> Manages course data (name, description, price).</li><li><em>Reviews Microservice:</em> Handles reviews and ratings.</li><li><em>Course Search Service:</em> New service with a materialized view that includes course details and reviews.</li></ul></li><li><p><strong>Data Flow:</strong></p><ul><li>Changes in the course details or reviews trigger events.</li><li>The Course Search Service updates its materialized view based on these events, ensuring quick access to aggregated data.</li></ul></li></ul><h4 id=benefits-3>Benefits<a hidden class=anchor aria-hidden=true href=#benefits-3>#</a></h4><ul><li><strong>Efficient Data Aggregation:</strong><ul><li>Combines data from multiple microservices into a single, query-optimized view.</li></ul></li><li><strong>Performance Optimization:</strong><ul><li>Reduces the need for complex, runtime data joins and minimizes latency.</li></ul></li><li><strong>Scalability and Maintainability:</strong><ul><li>Isolates query logic, making it easier to maintain and scale based on query load.</li></ul></li></ul><h4 id=conclusion-3>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-3>#</a></h4><ul><li>The combination of CQRS and materialized views effectively addresses the challenges of data aggregation in microservices architectures. This approach optimizes data retrieval, maintains synchronization, and allows for scalable and maintainable systems.</li></ul><h3 id=event-sourcing-pattern>Event Sourcing Pattern<a hidden class=anchor aria-hidden=true href=#event-sourcing-pattern>#</a></h3><h4 id=overview-10>Overview<a hidden class=anchor aria-hidden=true href=#overview-10>#</a></h4><p>Event sourcing is an architecture pattern where the state of an application is derived from a sequence of events rather than storing the current state directly.</p><p><img loading=lazy src=./../images/event-sourcing.png alt="event sourcing"></p><h4 id=traditional-data-handling>Traditional Data Handling<a hidden class=anchor aria-hidden=true href=#traditional-data-handling>#</a></h4><ul><li><strong>CRUD Operations:</strong> Applications typically use Create, Read, Update, and Delete operations to manage data, focusing on the current state.</li><li><strong>Current State:</strong> The database stores the current state of entities, and any modifications overwrite previous states.</li></ul><h4 id=need-for-event-sourcing>Need for Event Sourcing<a hidden class=anchor aria-hidden=true href=#need-for-event-sourcing>#</a></h4><ul><li><strong>Limited by Current State:</strong> In some cases, knowing only the current state isn&rsquo;t sufficient. Historical data or the sequence of changes may be crucial.</li><li><strong>Examples:</strong><ul><li><strong>Banking:</strong> Clients need to see transaction histories, not just current balances.</li><li><strong>E-commerce:</strong> Merchants might need to understand inventory changes, not just the current stock level.</li></ul></li></ul><h4 id=event-sourcing-explained>Event Sourcing Explained<a hidden class=anchor aria-hidden=true href=#event-sourcing-explained>#</a></h4><ul><li><strong>Events Over State:</strong> Instead of the current state, the system stores events that describe changes or facts about entities.</li><li><strong>Immutability:</strong> Events are immutable; once logged, they cannot be changed.</li><li><strong>Replaying Events:</strong> The current state is derived by replaying all events related to an entity.</li></ul><h4 id=benefits-of-event-sourcing>Benefits of Event Sourcing<a hidden class=anchor aria-hidden=true href=#benefits-of-event-sourcing>#</a></h4><ul><li><strong>Complete History:</strong> Retains a full history of changes, useful for auditing and troubleshooting.</li><li><strong>Performance:</strong> Improved write performance, as events are appended rather than updating a state.</li></ul><h4 id=storing-events>Storing Events<a hidden class=anchor aria-hidden=true href=#storing-events>#</a></h4><ol><li><strong>Database:</strong> Each event can be stored as a record, allowing for complex queries and analytics.</li><li><strong>Message Broker:</strong> Events can be published for consumption by other services, ensuring order and scalability.</li></ol><h4 id=challenges-and-optimizations>Challenges and Optimizations<a hidden class=anchor aria-hidden=true href=#challenges-and-optimizations>#</a></h4><ul><li><strong>Replaying All Events:</strong> Reconstructing state by replaying all events can be inefficient.<ul><li><strong>Snapshots:</strong> Periodically save the current state to reduce the number of events replayed.</li><li><strong>CQRS (Command Query Responsibility Segregation):</strong> Separate systems for handling commands (writes) and queries (reads). This allows for efficient read operations using a read-optimized database.</li></ul></li></ul><h4 id=combining-event-sourcing-and-cqrs>Combining Event Sourcing and CQRS<a hidden class=anchor aria-hidden=true href=#combining-event-sourcing-and-cqrs>#</a></h4><ul><li><strong>Eventual Consistency:</strong> Combining these patterns often results in eventual consistency, which may be acceptable depending on the use case.</li><li><strong>Benefits:</strong><ul><li><strong>Auditing and History:</strong> Complete record of all changes.</li><li><strong>Efficient Writes and Reads:</strong> Separate systems optimize performance.</li></ul></li></ul><h3 id=big-data-processing-and-lambda-architecture>Big Data Processing and Lambda Architecture<a hidden class=anchor aria-hidden=true href=#big-data-processing-and-lambda-architecture>#</a></h3><h4 id=overview-11>Overview<a hidden class=anchor aria-hidden=true href=#overview-11>#</a></h4><p>In big data processing, two main strategies exist: batch processing and real-time processing. Batch processing allows for deep insights into historical data and the fusion of data from various sources, but it has a higher delay between data collection and availability for querying. Real-time processing provides immediate visibility and response to incoming data but is limited to recent information without deep historical context.</p><h4 id=the-challenge>The Challenge<a hidden class=anchor aria-hidden=true href=#the-challenge>#</a></h4><p>Deciding between batch and real-time processing can be difficult, as many systems require the benefits of both strategies. For instance, systems that aggregate logs and performance metrics, or those used in ride-sharing services, need both real-time insights for immediate actions and historical analysis for detecting patterns and optimizing performance.</p><h4 id=lambda-architecture>Lambda Architecture<a hidden class=anchor aria-hidden=true href=#lambda-architecture>#</a></h4><p><img loading=lazy src=./../images/lambda-pattern.png alt="Lambda Architecture"></p><p>The Lambda Architecture addresses this challenge by combining batch and real-time processing, offering the best of both worlds. It consists of three layers:</p><ol><li><p><strong>Batch Layer</strong>:</p><ul><li>Manages the master dataset as a system of records, storing immutable data.</li><li>Pre-computes batch views for in-depth analysis and data corrections.</li><li>Operates on the entire dataset for perfect accuracy.</li></ul></li><li><p><strong>Speed Layer</strong>:</p><ul><li>Handles real-time data processing to provide low-latency views.</li><li>Processes data as it arrives and updates real-time views.</li><li>Focuses on recent data without complex corrections or historical context.</li></ul></li><li><p><strong>Serving Layer</strong>:</p><ul><li>Merges data from both batch and real-time views.</li><li>Responds to ad-hoc queries, providing a comprehensive view combining historical and real-time data.</li></ul></li></ol><h4 id=practical-application>Practical Application<a hidden class=anchor aria-hidden=true href=#practical-application>#</a></h4><p>A practical example of Lambda Architecture is in the ad tech industry, where advertisers and content producers interact through an ad-serving system. The system must process three types of events: ad views, ad clicks, and user purchases. These events are processed by both the batch and speed layers to provide:</p><ul><li>Real-time data, such as the number of users currently viewing ads.</li><li>Combined data for queries like the total ads shown in the last 24 hours, merging batch and speed layer data.</li><li>In-depth analysis, like determining the return on investment for advertisers, which requires historical data fusion and analytics.</li></ul><h4 id=conclusion-4>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-4>#</a></h4><p>Lambda Architecture effectively handles scenarios requiring both real-time and batch processing capabilities. It allows for comprehensive data analysis and immediate responsiveness, making it a versatile solution for modern big data systems.</p><h2 id=software-extensibility-patterns>Software Extensibility Patterns<a hidden class=anchor aria-hidden=true href=#software-extensibility-patterns>#</a></h2><h3 id=extensibility-patterns-sidecar>Extensibility Patterns: Sidecar<a hidden class=anchor aria-hidden=true href=#extensibility-patterns-sidecar>#</a></h3><h4 id=overview-12>Overview<a hidden class=anchor aria-hidden=true href=#overview-12>#</a></h4><p>The <strong>Sidecar</strong> pattern is an extensibility pattern used to extend the functionality of a service without embedding additional logic directly into the service. This approach allows for modular and scalable systems.</p><h4 id=problem-to-solve>Problem to Solve<a hidden class=anchor aria-hidden=true href=#problem-to-solve>#</a></h4><ul><li><strong>Additional Functionality Needs:</strong> Services often require extra capabilities like logging, monitoring, or configuration management, beyond their core business logic.</li><li><strong>Challenges:</strong><ul><li><strong>Library Reuse:</strong> In a multi-language environment, reusing libraries across services is impractical and can lead to inconsistencies.</li><li><strong>Separate Services:</strong> Deploying shared functionalities as separate services can be excessive and complex.</li></ul></li></ul><h4 id=sidecar-pattern>Sidecar Pattern<a hidden class=anchor aria-hidden=true href=#sidecar-pattern>#</a></h4><p><img loading=lazy src=./../images/sidecar-pattern.png alt="Sidecar Pattern"></p><ul><li><strong>Analogy:</strong> Like a sidecar on a motorcycle, this pattern adds extra functionality as a separate process or container alongside the main service.</li><li><strong>Benefits:</strong><ul><li><strong>Isolation:</strong> Provides separation between the core service and the sidecar, reducing potential conflicts.</li><li><strong>Shared Resources:</strong> Both the main service and the sidecar share the same host, allowing fast and reliable communication.</li><li><strong>Language Independence:</strong> Sidecars can be implemented in any language and reused across different services.</li><li><strong>Simplified Updates:</strong> Updates to sidecar functionalities can be rolled out across all services simultaneously.</li></ul></li></ul><h4 id=ambassador-sidecar>Ambassador Sidecar<a hidden class=anchor aria-hidden=true href=#ambassador-sidecar>#</a></h4><ul><li><strong>Function:</strong> A specific type of sidecar that acts as a proxy for handling network requests.</li><li><strong>Advantages:</strong><ul><li><strong>Complexity Offloading:</strong> Manages network communication complexities, including retries, authentication, and routing.</li><li><strong>Simplified Core Service:</strong> Keeps the core service focused on business logic, while the ambassador handles network concerns.</li><li><strong>Distributed Tracing:</strong> Enables instrumentation and tracing across services, aiding in troubleshooting and isolating issues.</li></ul></li></ul><h3 id=anti-corruption-adapter-pattern>Anti-Corruption Adapter Pattern<a hidden class=anchor aria-hidden=true href=#anti-corruption-adapter-pattern>#</a></h3><h4 id=introduction-1>Introduction<a hidden class=anchor aria-hidden=true href=#introduction-1>#</a></h4><p>The <strong>Anti-Corruption Adapter Pattern</strong> is a crucial software architecture pattern used to manage interactions between systems with different technologies, protocols, or data models. It prevents the corruption of a new system by the legacy system during integration or migration processes.</p><h4 id=scenarios-and-solutions>Scenarios and Solutions<a hidden class=anchor aria-hidden=true href=#scenarios-and-solutions>#</a></h4><p><img loading=lazy src=./../images/anti-corruption-adapter.png alt="Anti-Corruption Adapter Pattern"></p><ol><li><p><strong>Migration from Monolith to Microservices:</strong></p><ul><li><strong>Problem:</strong> During migration from a monolithic system to microservices, new services may need to interact with old technologies, APIs, or data models. This can corrupt the clean design of new services.</li><li><strong>Solution:</strong> Implement an Anti-Corruption Adapter (ACA) service that acts as a mediator. The ACA translates communications, allowing new microservices to interact with the monolithic application using modern technologies, while the monolith continues to operate as before.</li></ul></li><li><p><strong>Coexistence with Legacy Systems:</strong></p><ul><li><strong>Problem:</strong> Sometimes, parts of the legacy system cannot be fully migrated or replaced due to various constraints, such as high costs or critical dependencies.</li><li><strong>Solution:</strong> The ACA enables the new system to leverage legacy components without inheriting outdated logic or technologies. This allows the legacy system to remain as-is, while the new system evolves independently.</li></ul></li></ol><h4 id=benefits-4>Benefits<a hidden class=anchor aria-hidden=true href=#benefits-4>#</a></h4><ul><li><strong>Isolation:</strong> The ACA isolates new and old systems, preventing legacy logic from contaminating new architectures.</li><li><strong>Seamless Integration:</strong> It allows for smooth interaction between systems with different technologies or data models.</li><li><strong>Gradual Migration:</strong> Facilitates gradual migration from old to new systems without disrupting business operations.</li></ul><h4 id=challenges>Challenges<a hidden class=anchor aria-hidden=true href=#challenges>#</a></h4><ul><li><strong>Development and Maintenance:</strong> The ACA itself is a service that requires development, testing, and maintenance like any other component.</li><li><strong>Performance Overhead:</strong> The translation process can introduce latency and may require scaling to avoid becoming a bottleneck.</li><li><strong>Cost:</strong> In a cloud environment, the ACA can incur additional costs, particularly if run continuously. Deploying it as a Function-as-a-Service (FaaS) can help mitigate these costs if usage is infrequent.</li></ul><h4 id=conclusion-5>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-5>#</a></h4><p>The Anti-Corruption Adapter Pattern is valuable for maintaining the integrity and cleanliness of new systems while interacting with legacy components. It is particularly useful in scenarios involving system migration or the need for long-term coexistence with legacy systems. However, it comes with trade-offs, including potential latency and additional costs.</p><p>This pattern helps balance the evolution of technology stacks while minimizing disruption and preserving system integrity.</p><h3 id=backends-for-frontends-bff-pattern>Backends for Frontends (BFF) Pattern<a hidden class=anchor aria-hidden=true href=#backends-for-frontends-bff-pattern>#</a></h3><h4 id=introduction-2>Introduction<a hidden class=anchor aria-hidden=true href=#introduction-2>#</a></h4><p>The <strong>Backends for Frontends (BFF)</strong> pattern addresses the challenges of supporting different frontend applications (e.g., web, mobile, desktop) with a single monolithic backend. This pattern involves creating separate backend services, each tailored to the specific needs and features of a particular frontend.</p><h4 id=problem-statement-1>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement-1>#</a></h4><p>In a typical e-commerce system with a microservices architecture:</p><ul><li>The frontend code running in browsers interacts with a backend that serves static and dynamic content.</li><li>Over time, as the system grows and more frontend types (e.g., mobile, desktop) are introduced, the backend becomes complex, supporting diverse features and device-specific needs.</li><li>This complexity leads to a monolithic backend that struggles to provide optimal experiences for different devices.</li></ul><h4 id=solution-bff-pattern>Solution: BFF Pattern<a hidden class=anchor aria-hidden=true href=#solution-bff-pattern>#</a></h4><p><img loading=lazy src=./../images/backends-for-frontends.png alt="BFF Pattern"></p><p>The BFF pattern proposes creating distinct backend services for each frontend type:</p><ul><li><strong>Frontend-Specific Backends:</strong> Each backend service is dedicated to a particular frontend, containing only the relevant functionality. This results in smaller, more manageable codebases and services that can be optimized for specific devices (e.g., mobile vs. desktop).</li><li><strong>Full Stack Teams:</strong> Teams can work as full stack developers, managing both the frontend and the corresponding backend, streamlining the development and deployment process.</li></ul><h4 id=benefits-5>Benefits<a hidden class=anchor aria-hidden=true href=#benefits-5>#</a></h4><ul><li><strong>Optimized User Experience:</strong> Each backend is tailored to the unique capabilities and needs of its corresponding frontend, providing a better user experience.</li><li><strong>Reduced Complexity:</strong> Smaller, frontend-specific backends are easier to manage and evolve.</li><li><strong>Independent Development:</strong> Teams can work independently without depending on a separate backend team, reducing coordination overhead.</li></ul><h4 id=challenges-1>Challenges<a hidden class=anchor aria-hidden=true href=#challenges-1>#</a></h4><ol><li><p><strong>Shared Functionality:</strong></p><ul><li><strong>Duplication Risk:</strong> There may be shared logic or business rules needed across multiple backends (e.g., authentication, checkout process).</li><li><strong>Solutions:</strong><ul><li><strong>Shared Libraries:</strong> Suitable for small, stable pieces of logic but can lead to tight coupling and maintenance issues.</li><li><strong>Separate Services:</strong> Creating dedicated services for shared functionality with clear APIs and ownership, ensuring consistency without duplication.</li></ul></li></ul></li><li><p><strong>Granularity Decision:</strong></p><ul><li>Determining the appropriate level of granularity depends on the uniqueness of the experiences across different platforms. For example, separate backends for Android and iOS are justified if their user experiences are significantly different.</li></ul></li><li><p><strong>Cloud Deployment Considerations:</strong></p><ul><li>In a cloud environment, smaller and less powerful virtual machines can replace the original monolithic backend. The choice of hardware can be optimized for the specific demands of each frontend (e.g., CPU or memory requirements).</li><li>Load balancing can be used to route requests to the appropriate backend, using URL paths, parameters, or HTTP headers like the user agent.</li></ul></li></ol><h4 id=conclusion-6>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-6>#</a></h4><p>The BFF pattern helps manage the complexity and scalability of systems supporting multiple frontend types by creating dedicated backends. This approach improves user experience, reduces development friction, and allows for independent and efficient development. However, it requires careful management of shared functionality and thoughtful decisions regarding service granularity.</p><h2 id=reliability--error-handling-patterns>Reliability & Error Handling Patterns<a hidden class=anchor aria-hidden=true href=#reliability--error-handling-patterns>#</a></h2><h3 id=throttling-or-rate-limiting-pattern>Throttling or Rate Limiting Pattern<a hidden class=anchor aria-hidden=true href=#throttling-or-rate-limiting-pattern>#</a></h3><h4 id=introduction-3>Introduction<a hidden class=anchor aria-hidden=true href=#introduction-3>#</a></h4><p>The <strong>Throttling or Rate Limiting</strong> pattern is designed to enhance system reliability by controlling the rate at which resources are consumed. It helps prevent overconsumption of system resources, whether due to malicious activity, legitimate high traffic, or interactions with external services.</p><h4 id=problem-statement-2>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement-2>#</a></h4><p><img loading=lazy src=./../images/rate-limiting-pattern.png alt="Rate Limiting Pattern"></p><p>Two main issues this pattern addresses:</p><ol><li><strong>Overconsumption of Resources:</strong><ul><li>High request rates can lead to system overload, potentially causing slowdowns or outages.</li><li>Unexpected traffic spikes can trigger costly auto-scaling, resulting in financial strain.</li></ul></li><li><strong>Overspending on External Services:</strong><ul><li>Systems interacting with external APIs or cloud services can accidentally consume more resources than budgeted, leading to unexpectedly high costs.</li></ul></li></ol><h4 id=types-of-throttling>Types of Throttling<a hidden class=anchor aria-hidden=true href=#types-of-throttling>#</a></h4><ol><li><p><strong>Server-Side Throttling:</strong></p><ul><li>Limits the number of requests to protect the system&rsquo;s backend from overconsumption.</li><li>Common in scenarios where the system serves multiple clients through an API.</li></ul></li><li><p><strong>Client-Side Throttling:</strong></p><ul><li>Prevents a client from exceeding a predetermined budget when calling external services.</li><li>Used when a system consumes external APIs or cloud services to avoid overspending.</li></ul></li></ol><h4 id=strategies-for-handling-exceeding-limits>Strategies for Handling Exceeding Limits<a hidden class=anchor aria-hidden=true href=#strategies-for-handling-exceeding-limits>#</a></h4><ol><li><p><strong>Dropping Requests:</strong></p><ul><li>Requests exceeding the rate limit are dropped. An error response (e.g., HTTP 429 &ldquo;Too Many Requests&rdquo;) can inform the client.</li><li>Suitable for non-critical services like fetching real-time data (e.g., stock prices).</li></ul></li><li><p><strong>Queuing Requests:</strong></p><ul><li>Requests are queued and processed later when capacity allows.</li><li>Useful for critical operations like executing trades, where delaying is preferable to dropping.</li></ul></li><li><p><strong>Service Degradation:</strong></p><ul><li>Adjust service quality instead of dropping or delaying requests, such as reducing video quality in streaming services.</li><li>Can also set an upper limit on resource usage, like the number of media items accessed per day.</li></ul></li></ol><h4 id=considerations-for-implementation>Considerations for Implementation<a hidden class=anchor aria-hidden=true href=#considerations-for-implementation>#</a></h4><ol><li><p><strong>Global vs. Customer-Based Throttling:</strong></p><ul><li><strong>Global Throttling:</strong> A single limit applies to all clients, ensuring overall system stability but risking unfair resource allocation.</li><li><strong>Customer-Based Throttling:</strong> Individual limits for each client, ensuring fair resource distribution but complicating total request rate management.</li></ul></li><li><p><strong>External vs. Service-Based Throttling:</strong></p><ul><li><strong>External Throttling:</strong> Limits based on the overall number of API calls from clients, straightforward but can lead to internal service overload.</li><li><strong>Service-Based Throttling:</strong> Specific limits for internal services, requiring complex tracking but better protecting individual system components.</li></ul></li></ol><h4 id=conclusion-7>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-7>#</a></h4><p>Throttling is crucial for maintaining system reliability and controlling costs. The choice between different throttling strategies—global vs. customer-based, external vs. service-based—depends on the specific use case and system requirements. Understanding these considerations helps in implementing an effective throttling strategy that balances performance, cost, and resource allocation.</p><h3 id=retry-pattern>Retry Pattern<a hidden class=anchor aria-hidden=true href=#retry-pattern>#</a></h3><h4 id=introduction-4>Introduction<a hidden class=anchor aria-hidden=true href=#introduction-4>#</a></h4><p>The <strong>Retry Pattern</strong> is a reliability architecture pattern used to handle errors by retrying operations that have failed. This pattern helps recover from transient issues in a system, such as network failures or temporary unavailability of services.</p><h4 id=problem-statement-3>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement-3>#</a></h4><p>In cloud environments, errors can occur at any time due to software, hardware, or network issues. When a client calls a service, which in turn may call another service, these errors can cause delays, timeouts, or outright failures. The Retry Pattern aims to handle such situations by retrying failed operations.</p><h4 id=key-considerations>Key Considerations<a hidden class=anchor aria-hidden=true href=#key-considerations>#</a></h4><p><img loading=lazy src=./../images/retry-service.png alt="Retry Pattern"></p><ol><li><p><strong>Error Categorization:</strong></p><ul><li><strong>User Errors:</strong> Errors caused by invalid user actions (e.g., HTTP 403 Unauthorized). These should not be retried; instead, return the error to the user.</li><li><strong>System Errors:</strong> Internal errors that may be transient and recoverable (e.g., HTTP 503 Service Unavailable). These are candidates for retries.</li></ul></li><li><p><strong>Choosing Errors to Retry:</strong></p><ul><li>Only retry errors that are likely to be temporary and recoverable, such as timeouts or service unavailability.</li></ul></li><li><p><strong>Delay and Backoff Strategies:</strong></p><ul><li><strong>Fixed Delay:</strong> A constant delay between retries (e.g., 100 ms).</li><li><strong>Incremental Delay:</strong> Increasing the delay gradually after each retry (e.g., 100 ms, 200 ms, 300 ms).</li><li><strong>Exponential Backoff:</strong> Exponentially increasing delay (e.g., 100 ms, 200 ms, 400 ms, 800 ms) to reduce load on recovering services.</li></ul></li><li><p><strong>Randomization (Jitter):</strong></p><ul><li>Adding randomness to the delay helps spread out retry attempts, reducing the chance of overloading the system simultaneously.</li></ul></li><li><p><strong>Retry Limits and Time Boxing:</strong></p><ul><li>Set limits on the number of retries or a maximum time period for retries to prevent indefinite retry attempts.</li></ul></li><li><p><strong>Idempotency:</strong></p><ul><li>Ensure that retrying an operation does not cause unintended side effects, such as double billing in a payment system. Only idempotent operations should be retried safely.</li></ul></li><li><p><strong>Implementation Strategies:</strong></p><ul><li><strong>Shared Library:</strong> Implement retry logic in a reusable library or module.</li><li><strong>Ambassador Pattern:</strong> Separate the retry logic from the main application code by running it as a separate process on the same server.</li></ul></li></ol><h4 id=conclusion-8>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-8>#</a></h4><p>The Retry Pattern is a simple yet effective method for handling transient errors in a system. However, careful implementation is crucial to avoid pitfalls like retry storms, and to ensure retries are applied only in appropriate situations. Properly applying the Retry Pattern can enhance system reliability and provide a smoother experience for users.</p><h3 id=circuit-breaker-pattern>Circuit Breaker Pattern<a hidden class=anchor aria-hidden=true href=#circuit-breaker-pattern>#</a></h3><h4 id=introduction-5>Introduction<a hidden class=anchor aria-hidden=true href=#introduction-5>#</a></h4><p>The <strong>Circuit Breaker Pattern</strong> is a software architecture pattern used to handle long-lasting errors and prevent cascading failures. It contrasts with the Retry Pattern, which is used for short, recoverable issues. The Circuit Breaker Pattern prevents requests from being sent to a failing service, thus saving resources and improving system stability.</p><h4 id=real-life-example>Real-Life Example<a hidden class=anchor aria-hidden=true href=#real-life-example>#</a></h4><p>Consider an online dating service that fetches profile images from an image service. If the image service is down for a significant time, retrying requests is futile. Instead, using a circuit breaker can prevent further attempts and conserve resources.</p><h4 id=key-concepts-7>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts-7>#</a></h4><p><img loading=lazy src=./../images/circuit-breaker-pattern.png alt="Circuit Breaker Pattern"></p><ol><li><p><strong>Circuit States:</strong></p><ul><li><strong>Closed:</strong> All requests are allowed through, and the system tracks success and failure rates.</li><li><strong>Open:</strong> No requests are allowed through; failures are assumed to continue. The system immediately returns errors.</li><li><strong>Half-Open:</strong> A limited number of requests are allowed to test if the service has recovered.</li></ul></li><li><p><strong>Operation</strong></p><ul><li>The circuit starts in a <strong>Closed</strong> state, allowing requests and tracking failures. If failures exceed a certain threshold, the circuit moves to an <strong>Open</strong> state, blocking requests. After a timeout, it transitions to a <strong>Half-Open</strong> state to test the service&rsquo;s health. Depending on the results, it either closes the circuit or returns to the open state.</li></ul></li></ol><h4 id=implementation-considerations>Implementation Considerations<a hidden class=anchor aria-hidden=true href=#implementation-considerations>#</a></h4><ol><li><p><strong>Handling Requests in Open State:</strong></p><ul><li><strong>Drop Requests:</strong> Simply ignore them, with proper logging for analysis.</li><li><strong>Log and Replay:</strong> Record the requests for later processing, useful in critical systems like e-commerce.</li></ul></li><li><p><strong>Response Strategy in Open State:</strong></p><ul><li><strong>Fail Silently:</strong> Return empty responses or placeholders (e.g., a placeholder image in a dating app).</li><li><strong>Best Effort:</strong> Provide cached or old data if available.</li></ul></li><li><p><strong>Separate Circuit Breakers for Each Service:</strong></p><ul><li>Each external service should have its own circuit breaker to prevent one failing service from affecting others.</li></ul></li><li><p><strong>Asynchronous Health Checks:</strong></p><ul><li>Instead of real requests in the Half-Open state, use small, asynchronous health checks to determine service recovery. This approach conserves resources and reduces impact on the recovering service.</li></ul></li><li><p><strong>Implementation Location:</strong></p><ul><li>As with the Retry Pattern, the Circuit Breaker can be implemented as a shared library or through an Ambassador Sidecar, especially useful for services in different programming languages.</li></ul></li></ol><h4 id=conclusion-9>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion-9>#</a></h4><p>The Circuit Breaker Pattern is crucial for managing long-lasting errors, preventing resource waste, and improving system resilience. It involves careful consideration of how to handle failed requests, the state management of the circuit, and the method of implementation. Properly implementing this pattern can significantly enhance the stability and reliability of a distributed system.</p><h3 id=dead-letter-queue-pattern>Dead Letter Queue Pattern<a hidden class=anchor aria-hidden=true href=#dead-letter-queue-pattern>#</a></h3><h4 id=overview-13>Overview<a hidden class=anchor aria-hidden=true href=#overview-13>#</a></h4><p>The <strong>Dead Letter Queue (DLQ)</strong> pattern is designed to handle message delivery failures in an event-driven architecture. It helps manage errors in publishing and consuming messages through a message broker or distributed messaging system.</p><h4 id=event-driven-architecture>Event-Driven Architecture<a hidden class=anchor aria-hidden=true href=#event-driven-architecture>#</a></h4><p><img loading=lazy src=./../images/dead-letter-q.png alt="Dead Letter Queue Pattern"></p><p>In an event-driven system, three key components are involved:</p><ul><li><strong>Event Publishers:</strong> Produce messages or events.</li><li><strong>Message Broker:</strong> Manages channels, topics, or queues for message distribution.</li><li><strong>Consumers:</strong> Read and process incoming messages.</li></ul><p>While this architecture offers benefits like decoupling and scalability, it also introduces potential points of failure.</p><h4 id=real-life-example-1>Real-Life Example<a hidden class=anchor aria-hidden=true href=#real-life-example-1>#</a></h4><p>Consider an online store with:</p><ul><li><strong>Order Service:</strong> Publishes order events.</li><li><strong>Inventory, Payment, Fulfillment Services:</strong> Subscribe to these events and process them based on the product type (physical vs. digital).</li></ul><p>Potential issues include:</p><ul><li>Order service publishing to a non-existent or full queue.</li><li>Consumers facing issues reading or processing messages, which could clog the queue and delay other messages.</li></ul><h4 id=dead-letter-queue-dlq>Dead Letter Queue (DLQ)<a hidden class=anchor aria-hidden=true href=#dead-letter-queue-dlq>#</a></h4><ol><li><p><strong>Purpose:</strong></p><ul><li>A special queue for messages that cannot be delivered or processed successfully.</li><li>Helps in isolating problematic messages and preventing them from affecting the main queue.</li></ul></li><li><p><strong>Message Entry into DLQ:</strong></p><ul><li><strong>Programmatic Publishing:</strong> Consumers or publishers manually move messages to the DLQ if they encounter issues.</li><li><strong>Automated Configuration:</strong> Message brokers can be configured to automatically move messages to the DLQ due to delivery failures or other issues.</li></ul></li><li><p><strong>Configuration Support:</strong></p><ul><li>Most open-source or cloud-based message brokers support DLQ functionality.</li></ul></li><li><p><strong>Processing Messages in DLQ:</strong></p><ul><li><strong>Monitoring and Alerting:</strong> Regular monitoring ensures that messages in the DLQ are addressed and not forgotten.</li><li><strong>Error Details:</strong> Attach headers or additional information to messages to understand and fix issues.</li><li><strong>Handling:</strong><ul><li><strong>Automatic Republishing:</strong> Once issues are resolved, messages can be moved back to the original queue for normal processing.</li><li><strong>Manual Processing:</strong> Rare cases or special scenarios might require manual intervention.</li></ul></li></ul></li></ol><h2 id=deployment-patterns>Deployment Patterns<a hidden class=anchor aria-hidden=true href=#deployment-patterns>#</a></h2><h3 id=rolling-deployment>Rolling Deployment<a hidden class=anchor aria-hidden=true href=#rolling-deployment>#</a></h3><p><strong>Overview:</strong>
The Rolling Deployment pattern is used for upgrading production servers without significant downtime. It involves gradually replacing application instances with a new version while maintaining service availability.</p><p><img loading=lazy src=./../images/rolling-deployment.png alt="Rolling Deployment"></p><p><strong>How It Works:</strong></p><ol><li><strong>Load Balancing:</strong> Stop sending traffic to one server at a time using a load balancer.</li><li><strong>Upgrade:</strong> Replace the old application instance with the new version on the server.</li><li><strong>Testing:</strong> Optionally run tests on the upgraded server.</li><li><strong>Reintegration:</strong> Add the updated server back into the load balancer&rsquo;s rotation.</li><li><strong>Repeat:</strong> Continue the process for all servers until all are running the latest version.</li></ol><p><strong>Benefits:</strong></p><ul><li><strong>No Downtime:</strong> The system remains operational throughout the upgrade.</li><li><strong>Gradual Release:</strong> New versions are released gradually, reducing risk compared to a &ldquo;big bang&rdquo; approach.</li><li><strong>Cost-Effective:</strong> No need for additional hardware or infrastructure.</li></ul><p><strong>Downsides:</strong></p><ul><li><strong>Cascading Failures:</strong> New versions might cause failures that could impact old servers still in operation.</li><li><strong>Compatibility Issues:</strong> Running two versions side by side may cause issues if the new version is not fully compatible with the old one.</li></ul><h3 id=blue-green-deployment>Blue Green Deployment<a hidden class=anchor aria-hidden=true href=#blue-green-deployment>#</a></h3><p><strong>Overview:</strong>
The Blue Green Deployment pattern is used to release a new version of software by maintaining two separate environments—Blue and Green. This approach aims to minimize risks and ensure a smooth transition between versions.</p><p><strong>How It Works:</strong></p><p><img loading=lazy src=./../images/blue-gree-deployment.png alt="Blue Green Deployment"></p><ol><li><strong>Blue Environment:</strong> The old version of the application continues running on this set of servers.</li><li><strong>Green Environment:</strong> A new set of servers is provisioned, and the new version of the application is deployed here.</li><li><strong>Verification:</strong> After deployment, the new instances in the Green environment are tested to ensure they work as expected.</li><li><strong>Traffic Shift:</strong> Traffic is gradually redirected from the Blue environment to the Green environment using a load balancer.</li><li><strong>Rollback:</strong> If issues are detected, traffic can be switched back to the Blue environment. If all is well, the Blue environment can be decommissioned or kept for the next release cycle.</li></ol><p><strong>Benefits:</strong></p><ul><li><strong>No Downtime:</strong> The Blue environment remains operational during the transition, ensuring continuous service availability.</li><li><strong>Safe Rollbacks:</strong> If problems arise, traffic can be easily shifted back to the old version, minimizing risk.</li><li><strong>Consistent User Experience:</strong> Customers experience only one version of the software at a time, reducing compatibility issues.</li></ul><p><strong>Downsides:</strong></p><ul><li><strong>Increased Costs:</strong> Running both Blue and Green environments simultaneously means needing twice the server capacity during the release.</li><li><strong>Resource Usage:</strong> Additional servers are required, which can lead to higher operational costs.</li></ul><h3 id=canary-release-and-ab-testing>Canary Release and A/B Testing<a hidden class=anchor aria-hidden=true href=#canary-release-and-ab-testing>#</a></h3><p><strong>Canary Release:</strong>
The Canary Release pattern blends elements from both rolling and blue-green deployment strategies to offer a balanced approach to deploying new software versions.</p><p><strong>How It Works:</strong></p><p><img loading=lazy src=./../images/canary-release.png alt="Canary Release"></p><ol><li><strong>Initial Deployment:</strong> Deploy the new version of the software to a small subset of existing servers (the Canary servers).</li><li><strong>Traffic Management:</strong> Redirect either all or a subset of traffic (e.g., internal users or beta testers) to these Canary servers.</li><li><strong>Monitoring:</strong> Observe the performance and functionality of the Canary servers compared to the rest of the servers running the old version.</li><li><strong>Rollout Decision:</strong> If the Canary version performs well, gradually update the rest of the servers using a rolling release approach. If issues arise, traffic can be shifted back to the old servers.</li></ol><p><strong>Benefits:</strong></p><ul><li><strong>Reduced Risk:</strong> Issues affect only a small subset of users, making it easier to manage and rollback if needed.</li><li><strong>Informed Decisions:</strong> Provides confidence in the new version before a full-scale deployment.</li><li><strong>Selective Traffic:</strong> Directs traffic from internal users or beta testers to mitigate potential impacts.</li></ul><p><strong>Challenges:</strong></p><ul><li><strong>Monitoring Complexity:</strong> Requires clear success criteria and effective automation to monitor performance.</li><li><strong>Resource Allocation:</strong> Needs careful management to ensure that monitoring and rollback processes are efficient.</li></ul><p><strong>A/B Testing:</strong>
A/B Testing is similar to Canary Release but focuses on testing new features rather than full software versions.</p><p><img loading=lazy src=./../images/ab-testing.png alt="AB Testing"></p><p><strong>How It Works:</strong></p><ol><li><strong>Experimental Deployment:</strong> Deploy a new feature or version on a small subset of servers.</li><li><strong>User Segmentation:</strong> Redirect a portion of real user traffic to these experimental servers.</li><li><strong>Data Collection:</strong> Gather data on user interactions and performance metrics.</li><li><strong>Evaluation:</strong> Analyze the results to determine if the new feature should be rolled out to all users or if changes are needed.</li></ol><p><strong>Benefits:</strong></p><ul><li><strong>Real User Feedback:</strong> Provides genuine insights into how users interact with the new feature.</li><li><strong>Informed Decisions:</strong> Helps make data-driven decisions about future features or changes.</li><li><strong>Testing in Production:</strong> Allows testing of features under real-world conditions.</li></ul><p><strong>Challenges:</strong></p><ul><li><strong>User Awareness:</strong> Users are typically unaware they are part of an experiment, which can complicate feedback collection.</li><li><strong>Reversion Process:</strong> Requires careful handling to revert back to the original version after testing.</li></ul><h3 id=chaos-engineering>Chaos Engineering<a hidden class=anchor aria-hidden=true href=#chaos-engineering>#</a></h3><p><strong>Overview:</strong>
Chaos Engineering is a production testing technique used to improve the resilience and reliability of distributed systems by deliberately injecting controlled failures into a live environment. This approach helps identify and address potential weaknesses before they lead to catastrophic issues during unexpected real-world events.</p><p><img loading=lazy src=./../images/chaos-engineering-testing.png alt="Chaos Engineering"></p><p><strong>Why Chaos Engineering?</strong></p><ul><li><strong>Inevitability of Failures:</strong> In distributed systems, failures are inevitable due to infrastructure issues, network problems, or third-party outages.</li><li><strong>Limitations of Traditional Testing:</strong> Traditional testing methods (unit tests, integration tests) may not capture all possible failure scenarios, especially those that occur in production environments.</li></ul><p><strong>Key Concepts:</strong></p><ol><li><strong>Controlled Failures:</strong> Introduce failures such as server terminations, latency, resource exhaustion, or loss of access to simulate potential issues.</li><li><strong>Monitoring and Analysis:</strong> Observe how the system responds to these failures and analyze the performance.</li><li><strong>Improvement:</strong> Identify and fix any issues found during testing to enhance system resilience.</li></ol><p><strong>Common Failures to Inject:</strong></p><ul><li><strong>Server Termination:</strong> Randomly terminate servers to test system recovery and redundancy.</li><li><strong>Latency Injection:</strong> Introduce delays between services or between a service and its database.</li><li><strong>Resource Exhaustion:</strong> Fill up disk space or other resources to observe how the system handles resource limitations.</li><li><strong>Traffic Restrictions:</strong> Disable traffic to specific zones or regions to test failover mechanisms.</li></ul><p><strong>Process of Chaos Engineering:</strong></p><ol><li><strong>Baseline Measurement:</strong> Establish a performance baseline before injecting failures.</li><li><strong>Hypothesis Formation:</strong> Define expected behavior and system responses.</li><li><strong>Failure Injection:</strong> Introduce the failure and monitor its impact.</li><li><strong>Documentation:</strong> Record findings and observations during the test.</li><li><strong>Restoration:</strong> Restore the system to its original state post-testing.</li><li><strong>Continuous Improvement:</strong> Address identified issues and continuously test to ensure ongoing resilience.</li></ol><p><strong>Considerations:</strong></p><ul><li><strong>Minimize Blast Radius:</strong> Ensure failures are contained and do not excessively impact the system.</li><li><strong>Error Budget:</strong> Avoid promising 100% availability; maintain flexibility for unexpected and deliberate failures.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://raghu-vijaykumar.github.io/blog/tags/system-design/>System-Design</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/software-architecture/>Software-Architecture</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/patterns/>Patterns</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://raghu-vijaykumar.github.io/blog/>Raghu Vijaykumar</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>