<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Kafka | Raghu Vijaykumar</title>
<meta name=keywords content="kafka,message-broker"><meta name=description content="What is Apache Kafka? Apache Kafka is a distributed streaming platform that enables real-time processing of data streams. It provides two primary functionalities:
Creation of real-time data streams: Kafka allows for the continuous flow of data in real time, with latencies as low as milliseconds. Processing of real-time data streams: Kafka facilitates the real-time processing of incoming data, enabling immediate actions based on predefined conditions. Example Use Case Imagine a smart electricity meter generating data every minute."><meta name=author content="Me"><link rel=canonical href=https://raghu-vijaykumar.github.io/blog/docs/kafka/readme/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.e7c811b1152f0ea0017b0724f2c040700cf8bf84343fd4fd5eca45af82337db9.css integrity="sha256-58gRsRUvDqABewck8sBAcAz4v4Q0P9T9XspFr4Izfbk=" rel="preload stylesheet" as=style><link rel=icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://raghu-vijaykumar.github.io/blog/docs/kafka/readme/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Apache Kafka"><meta property="og:description" content="What is Apache Kafka? Apache Kafka is a distributed streaming platform that enables real-time processing of data streams. It provides two primary functionalities:
Creation of real-time data streams: Kafka allows for the continuous flow of data in real time, with latencies as low as milliseconds. Processing of real-time data streams: Kafka facilitates the real-time processing of incoming data, enabling immediate actions based on predefined conditions. Example Use Case Imagine a smart electricity meter generating data every minute."><meta property="og:type" content="article"><meta property="og:url" content="https://raghu-vijaykumar.github.io/blog/docs/kafka/readme/"><meta property="og:image" content="https://raghu-vijaykumar.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="docs"><meta property="article:published_time" content="2024-08-30T00:01:00+05:30"><meta property="article:modified_time" content="2024-08-30T00:01:00+05:30"><meta property="og:site_name" content="Raghu Vijaykumar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raghu-vijaykumar.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Apache Kafka"><meta name=twitter:description content="What is Apache Kafka? Apache Kafka is a distributed streaming platform that enables real-time processing of data streams. It provides two primary functionalities:
Creation of real-time data streams: Kafka allows for the continuous flow of data in real time, with latencies as low as milliseconds. Processing of real-time data streams: Kafka facilitates the real-time processing of incoming data, enabling immediate actions based on predefined conditions. Example Use Case Imagine a smart electricity meter generating data every minute."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Docs","item":"https://raghu-vijaykumar.github.io/blog/docs/"},{"@type":"ListItem","position":2,"name":"Apache Kafka","item":"https://raghu-vijaykumar.github.io/blog/docs/kafka/readme/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Kafka","name":"Apache Kafka","description":"What is Apache Kafka? Apache Kafka is a distributed streaming platform that enables real-time processing of data streams. It provides two primary functionalities:\nCreation of real-time data streams: Kafka allows for the continuous flow of data in real time, with latencies as low as milliseconds. Processing of real-time data streams: Kafka facilitates the real-time processing of incoming data, enabling immediate actions based on predefined conditions. Example Use Case Imagine a smart electricity meter generating data every minute.","keywords":["kafka","message-broker"],"articleBody":"What is Apache Kafka? Apache Kafka is a distributed streaming platform that enables real-time processing of data streams. It provides two primary functionalities:\nCreation of real-time data streams: Kafka allows for the continuous flow of data in real time, with latencies as low as milliseconds. Processing of real-time data streams: Kafka facilitates the real-time processing of incoming data, enabling immediate actions based on predefined conditions. Example Use Case Imagine a smart electricity meter generating data every minute. This data is sent to a Kafka server, which continuously receives information from multiple meters across the city, forming a continuous data stream. Applications can process this stream in real-time, such as sending alerts when electricity load exceeds a threshold, ensuring timely actions within seconds or minutes.\nHow Does Apache Kafka Work? Kafka adopts the Publish/Subscribe (Pub/Sub) messaging architecture typical of enterprise messaging systems. It consists of three core components:\nProducer: A client application that sends data records (messages). Message Broker (Kafka Server): Receives and stores messages from producers. Consumer: Client applications that read and process messages from the broker. Example Workflow The smart meter acts as a Producer, sending data to the Kafka Message Broker. The Kafka server acts as a Message Broker, storing the data. An application acts as a Consumer, processing the data and triggering actions such as sending alerts when necessary. Kafka serves as a middleman between producers and consumers, efficiently handling streams of data between them.\nOrigins of Apache Kafka Kafka was originally developed by LinkedIn to address their data integration problem. As various applications generated and consumed data across LinkedIn’s infrastructure, a need arose to simplify and streamline the process of transferring data between systems.\nSolution Kafka introduced a Pub/Sub messaging architecture that centralized the flow of data. With Kafka as a Message Broker, applications became Producers and Consumers, reducing the complexity and maintenance challenges associated with direct data pipelines between systems.\nEvolution of Apache Kafka Kafka evolved from a data integration tool into a full-fledged real-time streaming platform. Initially, Kafka consisted of:\nKafka Broker: The central server system acting as a message broker. Kafka Client API: Producer and Consumer APIs for sending and receiving messages. Over time, Kafka added three more components to expand its capabilities:\nKafka Connect: A tool to facilitate data integration, addressing the problem Kafka was initially designed to solve. Kafka Streams: A library for creating real-time stream processing applications. KSQL: A tool that transforms Kafka into a real-time database with SQL-like querying capabilities (offered commercially by Confluent Inc.). Kafka’s Role in Enterprise Applications Kafka occupies a central position in an organization’s data integration and processing ecosystem. It serves as a circulatory system for data, decoupling data producers from consumers:\nProducers: Send data to Kafka as soon as business events occur. Consumers: Retrieve and process the data from Kafka in real-time, ensuring immediate action. Decoupling Benefits Producers and consumers are independent of each other. Kafka ensures low-latency data flow between producers and consumers, often in milliseconds. Kafka’s architecture enables scalability and flexibility, allowing for the easy addition, removal, or modification of producers and consumers as business needs evolve. Apache Kafka Core Concepts This technical documentation outlines the fundamental concepts associated with Apache Kafka, a distributed streaming platform for handling real-time data streams.\nProducer Producer: An application that sends data to Kafka. The data, often called messages or records, is sent as an array of bytes. Example: A producer can send each line of a data file or database table row as a message to Kafka. Consumer Consumer: An application that receives data from Kafka. Consumers do not directly communicate with producers but retrieve messages from the Kafka server. Example: A consumer application requests messages (such as lines of text) from Kafka, processes them, and may request more messages in a loop. Broker Broker: The Kafka server acts as a broker, facilitating the exchange of messages between producers and consumers. Producers send data to the broker, and consumers request data from it. Cluster Cluster: A group of computers running Kafka instances, designed to act together for a common purpose. Kafka is a distributed system that runs on a cluster of brokers, allowing for scalability. Topic Topic: A unique name given to a data stream. Producers send messages to a specific topic, and consumers request data from that topic. Example: Topics can be used to organize data from different producers, such as creating separate topics for current load, consumed units, and input fluctuations. Partitions Partition: Kafka breaks topics into smaller, independent parts called partitions to manage large datasets. Partitions allow for distributed storage across multiple machines in a Kafka cluster. Design Decision: The number of partitions is a design decision made during topic creation, based on the expected data volume. Offset Offset: A unique sequence ID assigned to each message within a partition. Offsets are immutable and indicate the order of arrival of messages within a partition. Local Scope: Offsets are local to partitions, meaning there is no global ordering across partitions. Consumer Group Consumer Group: A group of consumers working together to share the workload of processing messages from a Kafka topic. Each consumer within a group is assigned one or more partitions to process. Scalability: The number of consumers in a group is limited by the number of partitions in the topic. Kafka ensures that only one consumer reads from a partition at a time to prevent double reading of messages. Scalability and Distribution Partitions for Scalability: Kafka partitions play a crucial role in distributing workload across the Kafka cluster. Partitions enable Kafka to scale horizontally, with producers and consumers processing data across multiple machines. Consumer Group Scalability: Multiple consumers in a group can read from different partitions simultaneously, dividing the work and improving processing efficiency. These core concepts form the foundation for working with Apache Kafka and are crucial to understanding how Kafka manages real-time data streams in a distributed system.\nKafka Connect Kafka Connect is a distributed data integration framework within Apache Kafka, designed to move large amounts of data between Kafka and external systems efficiently. It enables organizations to manage data flows between a variety of systems, without writing custom code, using reusable connectors.\nKey Concepts Kafka Connect helps solve the data integration problem, allowing data to flow from various source systems to Kafka and from Kafka to various target systems, including databases, data warehouses, and analytics platforms.\nKafka Connect Architecture Kafka Connect is composed of workers, connectors, and tasks:\nWorkers: Kafka Connect runs as a cluster of worker processes. These workers are fault-tolerant, self-managed, and use the same mechanism as Kafka consumer groups. If a worker fails or stops, other workers in the cluster will take over its connectors and tasks.\nConnectors: Connectors define how to connect Kafka to external systems. There are two types of connectors:\nSource Connectors: Pull data from an external system into Kafka. Sink Connectors: Send data from Kafka to an external system. Tasks: Tasks perform the actual data integration work. They connect to the source/target system, fetch or send data, and hand it over to workers. Tasks run in parallel, enabling scalability.\nKafka Connect Data Flow Source Connectors: These connectors pull data from external systems into Kafka. They interact with source databases, cloud storage systems, or other external data sources using source-specific APIs. Each connector creates tasks, which read and send data to Kafka.\nSink Connectors: Sink connectors move data from Kafka to external systems like databases or data lakes. Tasks consume Kafka records and write them to the target system using APIs specific to the target.\nParallelism and Tasks: Connectors split work into parallel tasks, enabling load distribution across workers. For example, a source connector might pull data from multiple tables, with each task handling a single table.\nWorkers and Reliability: Workers are responsible for running connectors and their associated tasks. The framework is highly resilient; if a worker fails, the remaining workers automatically redistribute the tasks. Kafka Connect supports dynamic scaling by adding or removing workers without downtime.\nKafka Connect Features Single Message Transformations (SMTs): Kafka Connect supports basic transformations of data records on the fly, such as adding fields, filtering, renaming fields, and routing messages to different topics. These transformations are applied within connectors before records are sent to Kafka or external systems.\nScalability: Kafka Connect scales horizontally by adding more workers. Each worker runs one or more tasks, allowing parallel processing of data.\nFault Tolerance: Kafka Connect provides built-in fault tolerance, where if a worker fails, tasks are rebalanced across the remaining workers without loss of data.\nConnector Framework The Kafka Connect Framework simplifies the process of writing connectors. Developers implement the following Java classes:\nSourceConnector/SinkConnector: These define the logic for pulling or pushing data. SourceTask/SinkTask: These implement the logic for handling data interaction with the source or target system. The framework handles the rest, including scalability, fault tolerance, error handling, and communication with Kafka. Connectors are typically distributed as JARs or ZIP files and installed in the Kafka Connect cluster.\nExample Use Case: Data Integration An example scenario where Kafka Connect is used:\nSource: A relational database (RDBMS) with several tables. Target: A data warehouse (e.g., Snowflake). Using Kafka Connect, a JDBC source connector is configured to pull data from the RDBMS into Kafka topics. On the other side, a Snowflake sink connector is configured to move data from the Kafka topics to Snowflake. Kafka Connect handles all the data flow, parallelism, and fault tolerance.\nKafka Streams Overview Kafka Streams is a library within Apache Kafka, designed to build real-time stream processing applications and microservices. These applications process data streams, continuously reading from Kafka topics and performing real-time analytics or actions. Kafka Streams allows you to scale your applications effortlessly, providing fault tolerance, and it can be deployed on various environments, from virtual machines to Kubernetes clusters.\nKey Concepts of Real-Time Stream Processing Data Streams: Streams represent unbounded, infinite sequences of data generated continuously. They can come from sources like sensors, log entries, clickstreams, transactions, or social media feeds.\nStream Processing vs. Traditional Processing:\nRequest-Response Processing: Queries are performed on stored data, returning answers to specific questions. Batch Processing: Processes data in bulk, running periodic jobs to analyze large datasets. Stream Processing: Unlike batch processing, stream processing deals with continuous data flows, ensuring real-time updates and insights as new data arrives. Kafka Streams Architecture Kafka Streams continuously reads data from one or more Kafka topics, processes it in real-time, and outputs the results. Key aspects of Kafka Streams architecture include:\nTasks and Partitions: Kafka Streams divides the work into logical tasks. Each task consumes data from one or more partitions of Kafka topics. The number of tasks is determined by the number of partitions. Threads and Scalability: Kafka Streams allows multi-threaded processing. Tasks are assigned to threads, and scaling out is done by adding more instances of the application, which will automatically distribute the tasks across the available resources. Task Re-assignment and Fault Tolerance: When a new instance is added, Kafka Streams automatically reassigns tasks to balance the load. If an instance fails, tasks are re-assigned to other running instances to ensure no data is lost, providing built-in fault tolerance. Features and Capabilities Kafka Streams provides several powerful capabilities designed for stream processing:\nStream-Table Interoperability: Kafka Streams allows interaction between streams and tables. It can convert streams to tables and vice versa, enabling more flexible data modeling. Aggregations: Kafka Streams supports continuously updating aggregates from grouped streams. Joins: Kafka Streams allows joining between streams, between tables, or between a combination of both. Stateful Processing: Kafka Streams provides fault-tolerant local state stores, which are essential for windowing and time-based operations. Windowing: Kafka Streams supports creating time windows for stream data, handling complexities such as event time, processing time, and dealing with late-arriving data. Interactive Queries: Kafka Streams provides an interactive query interface, enabling real-time querying of the state within your streams application. Fault Tolerance and Scalability: Kafka Streams offers inherent fault tolerance and scalability, allowing dynamic scaling of stream processing applications by adding more threads or instances as needed. Testing and Extensibility: Kafka Streams includes tools for unit testing applications, along with flexible DSLs and options to extend with custom processors. Kafka Streams in Action Kafka Streams allows you to scale out real-time applications while ensuring reliability:\nTask Distribution: When consuming from Kafka topics, Kafka Streams automatically creates tasks based on the number of partitions. These tasks are then assigned to threads within the application. Scaling Out: Kafka Streams makes it easy to scale out by adding more instances of the application. When more instances are deployed, tasks are automatically rebalanced and reassigned without manual intervention or downtime. Failover Handling: If an instance or thread fails, Kafka Streams reassigns the tasks to other available instances, ensuring seamless recovery and processing continuation. Conclusion Kafka Streams is a powerful, scalable, and fault-tolerant library designed for real-time stream processing. It allows developers to build stream processing applications with minimal effort, leveraging the full capabilities of Kafka, including parallelism, scalability, fault tolerance, and real-time analytics. Kafka Streams can be deployed on various environments, making it highly versatile for real-world streaming use cases.\nKafka SQL Overview Introduction to KSQL KSQL is a SQL interface for Kafka Streams, designed to enable scalable and fault-tolerant stream processing workloads without needing to write code in Java or Scala. KSQL supports two operating modes:\nInteractive Mode: Allows the user to submit KSQL commands via a command-line interface (CLI) or a web-based UI for immediate response, suitable for development environments. Headless Mode: Designed for production environments, this non-interactive mode executes predefined KSQL files on the KSQL server. KSQL Components KSQL comprises three core components:\nKSQL Engine: The main component responsible for processing KSQL statements and queries by building a Kafka Streams topology and executing them as stream tasks. REST Interface: Powers the KSQL clients and interacts with the KSQL Engine to execute commands. KSQL CLI/UI: The interface through which users submit and execute commands. These components form the KSQL server, which can operate in interactive or headless mode. Multiple KSQL servers can be deployed to form a scalable KSQL cluster.\nCapabilities of KSQL KSQL enables SQL-like operations on Kafka topics, such as:\nGrouping and aggregating over time windows Filtering and joining multiple topics Storing query results into new topics By using KSQL, Kafka topics can be treated like tables, and complex queries can be run over streaming data, turning Kafka into a real-time data warehouse.\nKafka Ecosystem Application Patterns Kafka’s ecosystem supports various application patterns, including data integration, real-time stream processing in microservice architectures, and real-time stream processing for data lakes or data warehouses.\n1. Data Integration Pattern This pattern focuses on data integration among multiple independent systems using Kafka. The components used in this pattern include:\nKafka Broker: Provides a shared infrastructure for data exchange. Kafka Client APIs: Used for implementing Kafka producers and consumers. Kafka Connect: Facilitates data integration with commercial-off-the-shelf (COTS) products. In this architecture, Kafka brokers handle data sharing between systems, while producers and consumers manage data generation and consumption.\n2. Microservice Architecture for Stream Processing This pattern involves real-time stream processing using a microservice architecture. Key Kafka components used here include:\nKafka Broker: Provides backbone infrastructure for microservices. Kafka Producers: Generate and share data streams. Kafka Streams: Handle business logic and real-time stream processing. Kafka Streams are favored over Kafka Consumers for stream processing because of their additional capabilities such as handling state, joins, and windowing.\n3. Real-Time Streaming in Data Warehouse and Data Lakes In this pattern, Kafka serves as a data ingestion layer, streaming data from various sources into a data lake. Once in the data lake, the data can be processed using batch or stream processing tools like Spark Structured Streaming.\nFor real-time querying, KSQL can be utilized to generate up-to-date summaries and reports. This extends Kafka’s use case beyond data ingestion into the realm of real-time data warehousing.\n","wordCount":"2637","inLanguage":"en","image":"https://raghu-vijaykumar.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-08-30T00:01:00+05:30","dateModified":"2024-08-30T00:01:00+05:30","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://raghu-vijaykumar.github.io/blog/docs/kafka/readme/"},"publisher":{"@type":"Organization","name":"Raghu Vijaykumar","logo":{"@type":"ImageObject","url":"https://raghu-vijaykumar.github.io/blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://raghu-vijaykumar.github.io/blog/ accesskey=h title="Raghu Vijaykumar (Alt + H)"><img src=https://raghu-vijaykumar.github.io/apple-touch-icon.png alt aria-label=logo height=35>Raghu Vijaykumar</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://raghu-vijaykumar.github.io/blog/about/ title=about><span>about</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/docs/ title=docs><span>docs</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/sponsor/ title=sponsor><span>sponsor</span></a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://raghu-vijaykumar.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://raghu-vijaykumar.github.io/blog/docs/>Docs</a></div><h1 class="post-title entry-hint-parent">Apache Kafka</h1><div class=post-meta><span title='2024-08-30 00:01:00 +0530 +0530'>August 30, 2024</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;2637 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-is-apache-kafka>What is Apache Kafka?</a><ul><li><a href=#example-use-case>Example Use Case</a></li></ul></li><li><a href=#how-does-apache-kafka-work>How Does Apache Kafka Work?</a><ul><li><a href=#example-workflow>Example Workflow</a></li></ul></li><li><a href=#origins-of-apache-kafka>Origins of Apache Kafka</a><ul><li><a href=#solution>Solution</a></li></ul></li><li><a href=#evolution-of-apache-kafka>Evolution of Apache Kafka</a></li><li><a href=#kafkas-role-in-enterprise-applications>Kafka&rsquo;s Role in Enterprise Applications</a><ul><li><a href=#decoupling-benefits>Decoupling Benefits</a></li></ul></li><li><a href=#apache-kafka-core-concepts>Apache Kafka Core Concepts</a><ul><li><a href=#producer>Producer</a></li><li><a href=#consumer>Consumer</a></li><li><a href=#broker>Broker</a></li><li><a href=#cluster>Cluster</a></li><li><a href=#topic>Topic</a></li><li><a href=#partitions>Partitions</a></li><li><a href=#offset>Offset</a></li><li><a href=#consumer-group>Consumer Group</a></li><li><a href=#scalability-and-distribution>Scalability and Distribution</a></li></ul></li><li><a href=#kafka-connect>Kafka Connect</a><ul><li><a href=#key-concepts>Key Concepts</a></li><li><a href=#kafka-connect-architecture>Kafka Connect Architecture</a></li><li><a href=#kafka-connect-data-flow>Kafka Connect Data Flow</a></li><li><a href=#kafka-connect-features>Kafka Connect Features</a></li><li><a href=#connector-framework>Connector Framework</a></li><li><a href=#example-use-case-data-integration>Example Use Case: Data Integration</a></li></ul></li><li><a href=#kafka-streams-overview>Kafka Streams Overview</a><ul><li><a href=#key-concepts-of-real-time-stream-processing>Key Concepts of Real-Time Stream Processing</a></li><li><a href=#kafka-streams-architecture>Kafka Streams Architecture</a></li><li><a href=#features-and-capabilities>Features and Capabilities</a></li><li><a href=#kafka-streams-in-action>Kafka Streams in Action</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li><li><a href=#kafka-sql-overview>Kafka SQL Overview</a><ul><li><a href=#introduction-to-ksql>Introduction to KSQL</a></li><li><a href=#ksql-components>KSQL Components</a></li><li><a href=#capabilities-of-ksql>Capabilities of KSQL</a></li></ul></li><li><a href=#kafka-ecosystem-application-patterns>Kafka Ecosystem Application Patterns</a><ul><li></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=what-is-apache-kafka>What is Apache Kafka?<a hidden class=anchor aria-hidden=true href=#what-is-apache-kafka>#</a></h2><p>Apache Kafka is a <strong>distributed streaming platform</strong> that enables real-time processing of data streams. It provides two primary functionalities:</p><ol><li><strong>Creation of real-time data streams:</strong> Kafka allows for the continuous flow of data in real time, with latencies as low as milliseconds.</li><li><strong>Processing of real-time data streams:</strong> Kafka facilitates the real-time processing of incoming data, enabling immediate actions based on predefined conditions.</li></ol><h3 id=example-use-case>Example Use Case<a hidden class=anchor aria-hidden=true href=#example-use-case>#</a></h3><p>Imagine a smart electricity meter generating data every minute. This data is sent to a Kafka server, which continuously receives information from multiple meters across the city, forming a continuous data stream. Applications can process this stream in real-time, such as sending alerts when electricity load exceeds a threshold, ensuring timely actions within seconds or minutes.</p><h2 id=how-does-apache-kafka-work>How Does Apache Kafka Work?<a hidden class=anchor aria-hidden=true href=#how-does-apache-kafka-work>#</a></h2><p>Kafka adopts the <strong>Publish/Subscribe (Pub/Sub) messaging architecture</strong> typical of enterprise messaging systems. It consists of three core components:</p><ol><li><strong>Producer:</strong> A client application that sends data records (messages).</li><li><strong>Message Broker (Kafka Server):</strong> Receives and stores messages from producers.</li><li><strong>Consumer:</strong> Client applications that read and process messages from the broker.</li></ol><h3 id=example-workflow>Example Workflow<a hidden class=anchor aria-hidden=true href=#example-workflow>#</a></h3><ul><li>The smart meter acts as a <strong>Producer</strong>, sending data to the Kafka <strong>Message Broker</strong>.</li><li>The Kafka server acts as a <strong>Message Broker</strong>, storing the data.</li><li>An application acts as a <strong>Consumer</strong>, processing the data and triggering actions such as sending alerts when necessary.</li></ul><p>Kafka serves as a <strong>middleman</strong> between producers and consumers, efficiently handling streams of data between them.</p><h2 id=origins-of-apache-kafka>Origins of Apache Kafka<a hidden class=anchor aria-hidden=true href=#origins-of-apache-kafka>#</a></h2><p>Kafka was originally developed by LinkedIn to address their <strong>data integration problem</strong>. As various applications generated and consumed data across LinkedIn’s infrastructure, a need arose to simplify and streamline the process of transferring data between systems.</p><h3 id=solution>Solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h3><p>Kafka introduced a Pub/Sub messaging architecture that centralized the flow of data. With Kafka as a <strong>Message Broker</strong>, applications became <strong>Producers</strong> and <strong>Consumers</strong>, reducing the complexity and maintenance challenges associated with direct data pipelines between systems.</p><h2 id=evolution-of-apache-kafka>Evolution of Apache Kafka<a hidden class=anchor aria-hidden=true href=#evolution-of-apache-kafka>#</a></h2><p>Kafka evolved from a <strong>data integration tool</strong> into a full-fledged <strong>real-time streaming platform</strong>. Initially, Kafka consisted of:</p><ol><li><strong>Kafka Broker:</strong> The central server system acting as a message broker.</li><li><strong>Kafka Client API:</strong> Producer and Consumer APIs for sending and receiving messages.</li></ol><p>Over time, Kafka added three more components to expand its capabilities:</p><ol><li><strong>Kafka Connect:</strong> A tool to facilitate <strong>data integration</strong>, addressing the problem Kafka was initially designed to solve.</li><li><strong>Kafka Streams:</strong> A library for creating real-time stream processing applications.</li><li><strong>KSQL:</strong> A tool that transforms Kafka into a real-time database with SQL-like querying capabilities (offered commercially by Confluent Inc.).</li></ol><h2 id=kafkas-role-in-enterprise-applications>Kafka&rsquo;s Role in Enterprise Applications<a hidden class=anchor aria-hidden=true href=#kafkas-role-in-enterprise-applications>#</a></h2><p>Kafka occupies a <strong>central position</strong> in an organization&rsquo;s data integration and processing ecosystem. It serves as a <strong>circulatory system</strong> for data, decoupling data producers from consumers:</p><ol><li><strong>Producers:</strong> Send data to Kafka as soon as business events occur.</li><li><strong>Consumers:</strong> Retrieve and process the data from Kafka in real-time, ensuring immediate action.</li></ol><h3 id=decoupling-benefits>Decoupling Benefits<a hidden class=anchor aria-hidden=true href=#decoupling-benefits>#</a></h3><ul><li>Producers and consumers are <strong>independent</strong> of each other.</li><li>Kafka ensures <strong>low-latency</strong> data flow between producers and consumers, often in milliseconds.</li><li>Kafka&rsquo;s architecture enables <strong>scalability</strong> and <strong>flexibility</strong>, allowing for the easy addition, removal, or modification of producers and consumers as business needs evolve.</li></ul><h2 id=apache-kafka-core-concepts>Apache Kafka Core Concepts<a hidden class=anchor aria-hidden=true href=#apache-kafka-core-concepts>#</a></h2><p>This technical documentation outlines the fundamental concepts associated with Apache Kafka, a distributed streaming platform for handling real-time data streams.</p><h3 id=producer>Producer<a hidden class=anchor aria-hidden=true href=#producer>#</a></h3><ul><li><strong>Producer</strong>: An application that sends data to Kafka. The data, often called messages or records, is sent as an array of bytes.</li><li><strong>Example</strong>: A producer can send each line of a data file or database table row as a message to Kafka.</li></ul><h3 id=consumer>Consumer<a hidden class=anchor aria-hidden=true href=#consumer>#</a></h3><ul><li><strong>Consumer</strong>: An application that receives data from Kafka. Consumers do not directly communicate with producers but retrieve messages from the Kafka server.</li><li><strong>Example</strong>: A consumer application requests messages (such as lines of text) from Kafka, processes them, and may request more messages in a loop.</li></ul><h3 id=broker>Broker<a hidden class=anchor aria-hidden=true href=#broker>#</a></h3><ul><li><strong>Broker</strong>: The Kafka server acts as a broker, facilitating the exchange of messages between producers and consumers. Producers send data to the broker, and consumers request data from it.</li></ul><h3 id=cluster>Cluster<a hidden class=anchor aria-hidden=true href=#cluster>#</a></h3><ul><li><strong>Cluster</strong>: A group of computers running Kafka instances, designed to act together for a common purpose. Kafka is a distributed system that runs on a cluster of brokers, allowing for scalability.</li></ul><h3 id=topic>Topic<a hidden class=anchor aria-hidden=true href=#topic>#</a></h3><ul><li><strong>Topic</strong>: A unique name given to a data stream. Producers send messages to a specific topic, and consumers request data from that topic.</li><li><strong>Example</strong>: Topics can be used to organize data from different producers, such as creating separate topics for current load, consumed units, and input fluctuations.</li></ul><h3 id=partitions>Partitions<a hidden class=anchor aria-hidden=true href=#partitions>#</a></h3><ul><li><strong>Partition</strong>: Kafka breaks topics into smaller, independent parts called partitions to manage large datasets. Partitions allow for distributed storage across multiple machines in a Kafka cluster.</li><li><strong>Design Decision</strong>: The number of partitions is a design decision made during topic creation, based on the expected data volume.</li></ul><h3 id=offset>Offset<a hidden class=anchor aria-hidden=true href=#offset>#</a></h3><ul><li><strong>Offset</strong>: A unique sequence ID assigned to each message within a partition. Offsets are immutable and indicate the order of arrival of messages within a partition.</li><li><strong>Local Scope</strong>: Offsets are local to partitions, meaning there is no global ordering across partitions.</li></ul><h3 id=consumer-group>Consumer Group<a hidden class=anchor aria-hidden=true href=#consumer-group>#</a></h3><ul><li><strong>Consumer Group</strong>: A group of consumers working together to share the workload of processing messages from a Kafka topic. Each consumer within a group is assigned one or more partitions to process.</li><li><strong>Scalability</strong>: The number of consumers in a group is limited by the number of partitions in the topic. Kafka ensures that only one consumer reads from a partition at a time to prevent double reading of messages.</li></ul><h3 id=scalability-and-distribution>Scalability and Distribution<a hidden class=anchor aria-hidden=true href=#scalability-and-distribution>#</a></h3><ul><li><strong>Partitions for Scalability</strong>: Kafka partitions play a crucial role in distributing workload across the Kafka cluster. Partitions enable Kafka to scale horizontally, with producers and consumers processing data across multiple machines.</li><li><strong>Consumer Group Scalability</strong>: Multiple consumers in a group can read from different partitions simultaneously, dividing the work and improving processing efficiency.</li></ul><p>These core concepts form the foundation for working with Apache Kafka and are crucial to understanding how Kafka manages real-time data streams in a distributed system.</p><h2 id=kafka-connect>Kafka Connect<a hidden class=anchor aria-hidden=true href=#kafka-connect>#</a></h2><p>Kafka Connect is a distributed data integration framework within Apache Kafka, designed to move large amounts of data between Kafka and external systems efficiently. It enables organizations to manage data flows between a variety of systems, without writing custom code, using reusable connectors.</p><h3 id=key-concepts>Key Concepts<a hidden class=anchor aria-hidden=true href=#key-concepts>#</a></h3><p>Kafka Connect helps solve the <strong>data integration problem</strong>, allowing data to flow from various source systems to Kafka and from Kafka to various target systems, including databases, data warehouses, and analytics platforms.</p><h3 id=kafka-connect-architecture>Kafka Connect Architecture<a hidden class=anchor aria-hidden=true href=#kafka-connect-architecture>#</a></h3><p>Kafka Connect is composed of <strong>workers</strong>, <strong>connectors</strong>, and <strong>tasks</strong>:</p><ol><li><p><strong>Workers</strong>: Kafka Connect runs as a cluster of worker processes. These workers are fault-tolerant, self-managed, and use the same mechanism as Kafka consumer groups. If a worker fails or stops, other workers in the cluster will take over its connectors and tasks.</p></li><li><p><strong>Connectors</strong>: Connectors define how to connect Kafka to external systems. There are two types of connectors:</p><ul><li><strong>Source Connectors</strong>: Pull data from an external system into Kafka.</li><li><strong>Sink Connectors</strong>: Send data from Kafka to an external system.</li></ul></li><li><p><strong>Tasks</strong>: Tasks perform the actual data integration work. They connect to the source/target system, fetch or send data, and hand it over to workers. Tasks run in parallel, enabling scalability.</p></li></ol><h3 id=kafka-connect-data-flow>Kafka Connect Data Flow<a hidden class=anchor aria-hidden=true href=#kafka-connect-data-flow>#</a></h3><ol><li><p><strong>Source Connectors</strong>: These connectors pull data from external systems into Kafka. They interact with source databases, cloud storage systems, or other external data sources using source-specific APIs. Each connector creates <strong>tasks</strong>, which read and send data to Kafka.</p></li><li><p><strong>Sink Connectors</strong>: Sink connectors move data from Kafka to external systems like databases or data lakes. Tasks consume Kafka records and write them to the target system using APIs specific to the target.</p></li><li><p><strong>Parallelism and Tasks</strong>: Connectors split work into parallel tasks, enabling load distribution across workers. For example, a source connector might pull data from multiple tables, with each task handling a single table.</p></li><li><p><strong>Workers and Reliability</strong>: Workers are responsible for running connectors and their associated tasks. The framework is highly resilient; if a worker fails, the remaining workers automatically redistribute the tasks. Kafka Connect supports dynamic scaling by adding or removing workers without downtime.</p></li></ol><h3 id=kafka-connect-features>Kafka Connect Features<a hidden class=anchor aria-hidden=true href=#kafka-connect-features>#</a></h3><ul><li><p><strong>Single Message Transformations (SMTs)</strong>: Kafka Connect supports basic transformations of data records on the fly, such as adding fields, filtering, renaming fields, and routing messages to different topics. These transformations are applied within connectors before records are sent to Kafka or external systems.</p></li><li><p><strong>Scalability</strong>: Kafka Connect scales horizontally by adding more workers. Each worker runs one or more tasks, allowing parallel processing of data.</p></li><li><p><strong>Fault Tolerance</strong>: Kafka Connect provides built-in fault tolerance, where if a worker fails, tasks are rebalanced across the remaining workers without loss of data.</p></li></ul><h3 id=connector-framework>Connector Framework<a hidden class=anchor aria-hidden=true href=#connector-framework>#</a></h3><p>The <strong>Kafka Connect Framework</strong> simplifies the process of writing connectors. Developers implement the following Java classes:</p><ul><li><strong>SourceConnector/SinkConnector</strong>: These define the logic for pulling or pushing data.</li><li><strong>SourceTask/SinkTask</strong>: These implement the logic for handling data interaction with the source or target system.</li></ul><p>The framework handles the rest, including scalability, fault tolerance, error handling, and communication with Kafka. Connectors are typically distributed as JARs or ZIP files and installed in the Kafka Connect cluster.</p><h3 id=example-use-case-data-integration>Example Use Case: Data Integration<a hidden class=anchor aria-hidden=true href=#example-use-case-data-integration>#</a></h3><p>An example scenario where Kafka Connect is used:</p><ul><li><strong>Source</strong>: A relational database (RDBMS) with several tables.</li><li><strong>Target</strong>: A data warehouse (e.g., Snowflake).</li></ul><p>Using Kafka Connect, a JDBC source connector is configured to pull data from the RDBMS into Kafka topics. On the other side, a Snowflake sink connector is configured to move data from the Kafka topics to Snowflake. Kafka Connect handles all the data flow, parallelism, and fault tolerance.</p><h2 id=kafka-streams-overview>Kafka Streams Overview<a hidden class=anchor aria-hidden=true href=#kafka-streams-overview>#</a></h2><p>Kafka Streams is a library within Apache Kafka, designed to build real-time stream processing applications and microservices. These applications process data streams, continuously reading from Kafka topics and performing real-time analytics or actions. Kafka Streams allows you to scale your applications effortlessly, providing fault tolerance, and it can be deployed on various environments, from virtual machines to Kubernetes clusters.</p><h3 id=key-concepts-of-real-time-stream-processing>Key Concepts of Real-Time Stream Processing<a hidden class=anchor aria-hidden=true href=#key-concepts-of-real-time-stream-processing>#</a></h3><ol><li><p><strong>Data Streams</strong>: Streams represent unbounded, infinite sequences of data generated continuously. They can come from sources like sensors, log entries, clickstreams, transactions, or social media feeds.</p></li><li><p><strong>Stream Processing vs. Traditional Processing</strong>:</p><ul><li><strong>Request-Response Processing</strong>: Queries are performed on stored data, returning answers to specific questions.</li><li><strong>Batch Processing</strong>: Processes data in bulk, running periodic jobs to analyze large datasets.</li><li><strong>Stream Processing</strong>: Unlike batch processing, stream processing deals with continuous data flows, ensuring real-time updates and insights as new data arrives.</li></ul></li></ol><h3 id=kafka-streams-architecture>Kafka Streams Architecture<a hidden class=anchor aria-hidden=true href=#kafka-streams-architecture>#</a></h3><p>Kafka Streams continuously reads data from one or more Kafka topics, processes it in real-time, and outputs the results. Key aspects of Kafka Streams architecture include:</p><ul><li><strong>Tasks and Partitions</strong>: Kafka Streams divides the work into logical tasks. Each task consumes data from one or more partitions of Kafka topics. The number of tasks is determined by the number of partitions.</li><li><strong>Threads and Scalability</strong>: Kafka Streams allows multi-threaded processing. Tasks are assigned to threads, and scaling out is done by adding more instances of the application, which will automatically distribute the tasks across the available resources.</li><li><strong>Task Re-assignment and Fault Tolerance</strong>: When a new instance is added, Kafka Streams automatically reassigns tasks to balance the load. If an instance fails, tasks are re-assigned to other running instances to ensure no data is lost, providing built-in fault tolerance.</li></ul><h3 id=features-and-capabilities>Features and Capabilities<a hidden class=anchor aria-hidden=true href=#features-and-capabilities>#</a></h3><p>Kafka Streams provides several powerful capabilities designed for stream processing:</p><ul><li><strong>Stream-Table Interoperability</strong>: Kafka Streams allows interaction between streams and tables. It can convert streams to tables and vice versa, enabling more flexible data modeling.</li><li><strong>Aggregations</strong>: Kafka Streams supports continuously updating aggregates from grouped streams.</li><li><strong>Joins</strong>: Kafka Streams allows joining between streams, between tables, or between a combination of both.</li><li><strong>Stateful Processing</strong>: Kafka Streams provides fault-tolerant local state stores, which are essential for windowing and time-based operations.</li><li><strong>Windowing</strong>: Kafka Streams supports creating time windows for stream data, handling complexities such as event time, processing time, and dealing with late-arriving data.</li><li><strong>Interactive Queries</strong>: Kafka Streams provides an interactive query interface, enabling real-time querying of the state within your streams application.</li><li><strong>Fault Tolerance and Scalability</strong>: Kafka Streams offers inherent fault tolerance and scalability, allowing dynamic scaling of stream processing applications by adding more threads or instances as needed.</li><li><strong>Testing and Extensibility</strong>: Kafka Streams includes tools for unit testing applications, along with flexible DSLs and options to extend with custom processors.</li></ul><h3 id=kafka-streams-in-action>Kafka Streams in Action<a hidden class=anchor aria-hidden=true href=#kafka-streams-in-action>#</a></h3><p>Kafka Streams allows you to scale out real-time applications while ensuring reliability:</p><ul><li><strong>Task Distribution</strong>: When consuming from Kafka topics, Kafka Streams automatically creates tasks based on the number of partitions. These tasks are then assigned to threads within the application.</li><li><strong>Scaling Out</strong>: Kafka Streams makes it easy to scale out by adding more instances of the application. When more instances are deployed, tasks are automatically rebalanced and reassigned without manual intervention or downtime.</li><li><strong>Failover Handling</strong>: If an instance or thread fails, Kafka Streams reassigns the tasks to other available instances, ensuring seamless recovery and processing continuation.</li></ul><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>Kafka Streams is a powerful, scalable, and fault-tolerant library designed for real-time stream processing. It allows developers to build stream processing applications with minimal effort, leveraging the full capabilities of Kafka, including parallelism, scalability, fault tolerance, and real-time analytics. Kafka Streams can be deployed on various environments, making it highly versatile for real-world streaming use cases.</p><h2 id=kafka-sql-overview>Kafka SQL Overview<a hidden class=anchor aria-hidden=true href=#kafka-sql-overview>#</a></h2><h3 id=introduction-to-ksql>Introduction to KSQL<a hidden class=anchor aria-hidden=true href=#introduction-to-ksql>#</a></h3><p>KSQL is a SQL interface for Kafka Streams, designed to enable scalable and fault-tolerant stream processing workloads without needing to write code in Java or Scala. KSQL supports two operating modes:</p><ul><li><strong>Interactive Mode</strong>: Allows the user to submit KSQL commands via a command-line interface (CLI) or a web-based UI for immediate response, suitable for development environments.</li><li><strong>Headless Mode</strong>: Designed for production environments, this non-interactive mode executes predefined KSQL files on the KSQL server.</li></ul><h3 id=ksql-components>KSQL Components<a hidden class=anchor aria-hidden=true href=#ksql-components>#</a></h3><p>KSQL comprises three core components:</p><ul><li><strong>KSQL Engine</strong>: The main component responsible for processing KSQL statements and queries by building a Kafka Streams topology and executing them as stream tasks.</li><li><strong>REST Interface</strong>: Powers the KSQL clients and interacts with the KSQL Engine to execute commands.</li><li><strong>KSQL CLI/UI</strong>: The interface through which users submit and execute commands.</li></ul><p>These components form the KSQL server, which can operate in interactive or headless mode. Multiple KSQL servers can be deployed to form a scalable KSQL cluster.</p><h3 id=capabilities-of-ksql>Capabilities of KSQL<a hidden class=anchor aria-hidden=true href=#capabilities-of-ksql>#</a></h3><p>KSQL enables SQL-like operations on Kafka topics, such as:</p><ul><li>Grouping and aggregating over time windows</li><li>Filtering and joining multiple topics</li><li>Storing query results into new topics</li></ul><p>By using KSQL, Kafka topics can be treated like tables, and complex queries can be run over streaming data, turning Kafka into a real-time data warehouse.</p><h2 id=kafka-ecosystem-application-patterns>Kafka Ecosystem Application Patterns<a hidden class=anchor aria-hidden=true href=#kafka-ecosystem-application-patterns>#</a></h2><p>Kafka&rsquo;s ecosystem supports various application patterns, including data integration, real-time stream processing in microservice architectures, and real-time stream processing for data lakes or data warehouses.</p><h4 id=1-data-integration-pattern>1. Data Integration Pattern<a hidden class=anchor aria-hidden=true href=#1-data-integration-pattern>#</a></h4><p>This pattern focuses on data integration among multiple independent systems using Kafka. The components used in this pattern include:</p><ul><li><strong>Kafka Broker</strong>: Provides a shared infrastructure for data exchange.</li><li><strong>Kafka Client APIs</strong>: Used for implementing Kafka producers and consumers.</li><li><strong>Kafka Connect</strong>: Facilitates data integration with commercial-off-the-shelf (COTS) products.</li></ul><p>In this architecture, Kafka brokers handle data sharing between systems, while producers and consumers manage data generation and consumption.</p><h4 id=2-microservice-architecture-for-stream-processing>2. Microservice Architecture for Stream Processing<a hidden class=anchor aria-hidden=true href=#2-microservice-architecture-for-stream-processing>#</a></h4><p>This pattern involves real-time stream processing using a microservice architecture. Key Kafka components used here include:</p><ul><li><strong>Kafka Broker</strong>: Provides backbone infrastructure for microservices.</li><li><strong>Kafka Producers</strong>: Generate and share data streams.</li><li><strong>Kafka Streams</strong>: Handle business logic and real-time stream processing.</li></ul><p>Kafka Streams are favored over Kafka Consumers for stream processing because of their additional capabilities such as handling state, joins, and windowing.</p><h4 id=3-real-time-streaming-in-data-warehouse-and-data-lakes>3. Real-Time Streaming in Data Warehouse and Data Lakes<a hidden class=anchor aria-hidden=true href=#3-real-time-streaming-in-data-warehouse-and-data-lakes>#</a></h4><p>In this pattern, Kafka serves as a data ingestion layer, streaming data from various sources into a data lake. Once in the data lake, the data can be processed using batch or stream processing tools like Spark Structured Streaming.</p><p>For real-time querying, KSQL can be utilized to generate up-to-date summaries and reports. This extends Kafka&rsquo;s use case beyond data ingestion into the realm of real-time data warehousing.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://raghu-vijaykumar.github.io/blog/tags/kafka/>Kafka</a></li><li><a href=https://raghu-vijaykumar.github.io/blog/tags/message-broker/>Message-Broker</a></li></ul><nav class=paginav><a class=prev href=https://raghu-vijaykumar.github.io/blog/docs/system-design/design-patterns/prototype/readme/><span class=title>« Prev</span><br><span>Prototype Pattern</span>
</a><a class=next href=https://raghu-vijaykumar.github.io/blog/docs/system-design/design-patterns/adapter/readme/><span class=title>Next »</span><br><span>Adapter Pattern</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://raghu-vijaykumar.github.io/blog/>Raghu Vijaykumar</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>